{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8266e2f-b765-44d0-bf2b-d5b777566a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q seqeval evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8222b4f9-d909-4ed6-bcc3-e9c198c4146f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=PII\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=PII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7acc420-c6da-4c9e-b3f7-e2cf7fd88101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spacy.lang.en import English\n",
    "from transformers.tokenization_utils import PreTrainedTokenizerBase\n",
    "from transformers.models.deberta_v2 import DebertaV2ForTokenClassification, DebertaV2TokenizerFast\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.trainer_utils import EvalPrediction, PredictionOutput\n",
    "from transformers.data.data_collator import DataCollatorForTokenClassification\n",
    "from sklearn.model_selection import KFold\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "from seqeval.metrics import recall_score, precision_score\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a6b54ae-7e21-460b-87e4-837dab004bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memiz6413\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"eff994fe72307679c21248b6e7859e26960b8db7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa81d1-44e4-47f5-8b59-c69faa43b1e0",
   "metadata": {},
   "source": [
    "## Config & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b135e92b-8f18-481f-a181-8cbd3e960915",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../dataset/\")\n",
    "OUTPUT_DIR = \"output2\"\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9088ae3-972e-456c-9c89-df6423941856",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_MODEL_PATH = \"Gladiator/microsoft-deberta-v3-large_ner_conll2003\"\n",
    "TRAIN_MAX_LENGTH = 1024\n",
    "EVAL_MAX_LENGTH = 3072\n",
    "STRIDE = 256\n",
    "#EVAL_MAX_LENGTH = 3072\n",
    "CONF_THRESH = 0.9\n",
    "LR = 2e-5  # 1.5e-5 ~ 3e-5 for base # 5e-6 ~ 1e-5 for large\n",
    "LR_SCHEDULER_TYPE = \"cosine_with_restarts\"\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION_STEPS = 8  # total_effective_batch_size = 32\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "AMP = True\n",
    "FREEZE_EMBEDDING = False\n",
    "FREEZE_LAYERS = 0\n",
    "# training data\n",
    "N_SPLITS = 4\n",
    "FILTER_ORIGINAL = False\n",
    "EXTRA_1 = False\n",
    "EXTRA_2 = False\n",
    "EXTRA_3 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cf06306-7564-4833-8625-b9d961b1ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    fp16=AMP,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION_STEPS,\n",
    "    report_to=\"wandb\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    eval_delay=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=10,\n",
    "    metric_for_best_model=\"f5\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2dd277-98f6-40b3-ab12-6bdc872eb140",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "069d4817-9ef6-49b4-b24b-428e1f14a7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original datapoints:  6807\n",
      "positive datapoints: 945\n",
      "negative datapoints 5862\n",
      "len(extra_data): 0\n"
     ]
    }
   ],
   "source": [
    "with DATA_DIR.joinpath(\"train.json\").open(\"r\") as f:\n",
    "    original_data = json.load(f)\n",
    "\n",
    "# downsampling of negative examples\n",
    "p=[] # positive samples (contain relevant labels)\n",
    "n=[] # negative samples (presumably contain entities that are possibly wrongly classified as entity)\n",
    "\n",
    "for d in original_data:\n",
    "    if any(np.array(d[\"labels\"]) != \"O\"):\n",
    "        p.append(d)\n",
    "    else:\n",
    "        n.append(d)\n",
    "\n",
    "print(\"original datapoints: \", len(original_data))\n",
    "print(\"positive datapoints:\", len(p))\n",
    "print(\"negative datapoints\", len(n))\n",
    "if FILTER_ORIGINAL:\n",
    "    original_data = p + n[:len(n)//3]\n",
    "\n",
    "extra_data = []  #\n",
    "\n",
    "# Moth\n",
    "if EXTRA_1:\n",
    "    with DATA_DIR.joinpath(\"pii_dataset_fixed.json\").open(\"r\") as f:\n",
    "        external_1 = json.load(f)\n",
    "    print(\"external_1 datapoints: \", len(external_1))\n",
    "    extra_data.extend(external_1)\n",
    "\n",
    "# PJMathmatician\n",
    "if EXTRA_2:\n",
    "    with DATA_DIR.joinpath(\"moredata_dataset_fixed.json\").open(\"r\") as f:\n",
    "        external_2 = json.load(f)\n",
    "    print(\"external_2 datapoints: \", len(external_2))\n",
    "    extra_data.extend(external_2)\n",
    "\n",
    "# Nicholas\n",
    "if EXTRA_3:\n",
    "    with DATA_DIR.joinpath(\"mixtral-8x7b-v1.json\").open(\"r\") as f:\n",
    "        external_3 = json.load(f)\n",
    "    print(\"external_3 datapoints: \", len(external_3))\n",
    "    extra_data.extend(external_3)\n",
    "    \n",
    "print(f\"len(extra_data): {len(extra_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44a0aa6c-023e-4746-a88f-5c25639313ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = [\n",
    "    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O'\n",
    "]\n",
    "id2label = {i: l for i, l in enumerate(all_labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "target = [l for l in all_labels if l != \"O\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a957d-ad6c-4dce-b79a-6091f1829067",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19c78c9f-dc7d-41ec-9b0f-52b68aa657aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer: PreTrainedTokenizerBase, \n",
    "        label2id: dict, \n",
    "        max_length: int,\n",
    "        stride: int,\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "        \n",
    "    def encode_for_train(self, example: dict) -> dict:\n",
    "        tokens, labels = [], []\n",
    "        for token, label, t_ws in zip(\n",
    "            example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n",
    "        ):\n",
    "            tokens.append(token)\n",
    "            labels.extend([label] * len(token))\n",
    "            if t_ws: # trainling whitespace\n",
    "                tokens.append(\" \") # add space in tokens list\n",
    "                labels.append(\"O\") # add 'O' label in labels list\n",
    "        text = \"\".join(tokens) # merge all the tokens to a text (string)\n",
    "\n",
    "        # tokenize the text with max_length and stride to retain the data\n",
    "        tokenized = tokenizer(\n",
    "            text, \n",
    "            max_length=self.max_length,\n",
    "            return_overflowing_tokens=True, \n",
    "            stride=self.stride, \n",
    "            return_offsets_mapping=True,\n",
    "            truncation=True,\n",
    "        )    \n",
    "\n",
    "        token_labels = [] # a list of list due to long sentences split up\n",
    "        for sequence_offset_mapping in tokenized.offset_mapping:\n",
    "            sequence_token_labels = []\n",
    "            for start_idx, end_idx in sequence_offset_mapping:\n",
    "                if start_idx == 0 and end_idx == 0: # CLS token\n",
    "                    sequence_token_labels.append(label2id[\"O\"]) \n",
    "                    continue\n",
    "                if text[start_idx].isspace(): # text start with a whitespace\n",
    "                    start_idx += 1\n",
    "                label = labels[start_idx]\n",
    "                sequence_token_labels.append(label2id[label])\n",
    "            token_labels.append(sequence_token_labels)\n",
    "        tokenized['labels'] = token_labels\n",
    "\n",
    "        return {**tokenized}\n",
    "\n",
    "    def encode_for_eval(self, example: dict) -> dict:\n",
    "        # rebuild text from tokens\n",
    "        text, labels, token_map = [], [], []\n",
    "\n",
    "        for idx, (t, l, ws) in enumerate(\n",
    "            zip(example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"])\n",
    "        ):\n",
    "            text.append(t)\n",
    "            labels.extend([l] * len(t))\n",
    "            token_map.extend([idx]*len(t))\n",
    "\n",
    "            if ws:\n",
    "                text.append(\" \")\n",
    "                labels.append(\"O\")\n",
    "                token_map.append(-1)\n",
    "\n",
    "        text = \"\".join(text)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # actual tokenization\n",
    "        tokenized = self.tokenizer(\n",
    "            \"\".join(text),\n",
    "            return_offsets_mapping=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "        token_labels = []\n",
    "\n",
    "        for start_idx, end_idx in tokenized.offset_mapping:\n",
    "            # CLS token\n",
    "            if start_idx == 0 and end_idx == 0:\n",
    "                token_labels.append(self.label2id[\"O\"])\n",
    "                continue\n",
    "\n",
    "            # case when token starts with whitespace\n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "\n",
    "            token_labels.append(self.label2id[labels[start_idx]])\n",
    "\n",
    "        length = len(tokenized.input_ids)\n",
    "\n",
    "        return {**tokenized, \"labels\": token_labels, \"length\": length, \"token_map\": token_map}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b5fcaf-875a-4c7c-a4d7-640f974fd258",
   "metadata": {},
   "source": [
    "### Sliding window expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5fab798-be73-4a5c-9223-d2be41c2736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_text(tokens, trailing_whitespace):\n",
    "    text = \"\"\n",
    "    for token, ws in zip(tokens, trailing_whitespace):\n",
    "        ws = \" \" if ws else \"\"\n",
    "        text += token + ws\n",
    "    return text\n",
    "\n",
    "def stride_rows(data, max_length, stride):\n",
    "    expanded = []\n",
    "    for row in data:\n",
    "        tokens = row['tokens']\n",
    "        if len(tokens) > max_length:\n",
    "            start = 0\n",
    "            while start < len(tokens):\n",
    "                remaining_tokens = len(tokens) - start\n",
    "                if remaining_tokens < max_length and start != 0:\n",
    "                    # Adjust start for the last window to ensure it has max_length tokens\n",
    "                    start = max(0, len(tokens) - max_length)\n",
    "                end = min(start + max_length, len(tokens))\n",
    "                new_row = {}\n",
    "                new_row['document'] = row['document']\n",
    "                new_row['tokens'] = tokens[start:end]\n",
    "                new_row['trailing_whitespace'] = row['trailing_whitespace'][start:end]\n",
    "                new_row['labels'] = row['labels'][start:end]\n",
    "                new_row['token_indices'] = list(range(start, end))\n",
    "                new_row['full_text'] = rebuild_text(new_row['tokens'], new_row['trailing_whitespace'])\n",
    "                expanded.append(new_row)\n",
    "                if remaining_tokens >= max_length:\n",
    "                    start += stride\n",
    "                else:\n",
    "                    # Break the loop if we've adjusted for the last window\n",
    "                    break\n",
    "        else:\n",
    "            new_row = {\n",
    "                'document': row['document'], \n",
    "                'tokens': row['tokens'], \n",
    "                'trailing_whitespace': row['trailing_whitespace'], \n",
    "                'labels': row['labels'], \n",
    "                'token_indices': list(range(len(row[\"tokens\"]))), \n",
    "                'full_text': row['full_text']\n",
    "            }\n",
    "            expanded.append(new_row)\n",
    "    return expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5ebe2-43d4-4a85-9cda-7210f0470aa2",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b7a12-ed50-4c0b-8d35-e2a026fad4d5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_LENGTH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m DebertaV2TokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(TRAINING_MODEL_PATH)\n\u001b[0;32m----> 2\u001b[0m encoder \u001b[38;5;241m=\u001b[39m CustomTokenizer(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, label2id\u001b[38;5;241m=\u001b[39mlabel2id, max_length\u001b[38;5;241m=\u001b[39m\u001b[43mMAX_LENGTH\u001b[49m, stride\u001b[38;5;241m=\u001b[39mSTRIDE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_LENGTH' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = DebertaV2TokenizerFast.from_pretrained(TRAINING_MODEL_PATH)\n",
    "train_encoder = CustomTokenizer(tokenizer=tokenizer, label2id=label2id, max_length=TRAINING_MAX_LENGTH)\n",
    "eval_encoder = CustomTokenizer(tokenizer=tokenizer, label2id=label2id, max_length=EVAL_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4218f10a-128d-49a6-a594-ef19b5435df9",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15c7679b-dec3-4c41-8758-a5c5b1f576f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsComputer:\n",
    "    def __init__(self, all_labels: list[str], beta: float = 5.0) -> None:\n",
    "        self.all_labels = all_labels\n",
    "        self.beta = beta\n",
    "        \n",
    "    def __call__(self, preds: EvalPrediction) -> dict[str, float]:\n",
    "        predictions, labels = preds\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "        # Remove ignored index (special tokens)\n",
    "        true_predictions = [\n",
    "            [self.all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [self.all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "\n",
    "        recall = recall_score(true_labels, true_predictions)\n",
    "        precision = precision_score(true_labels, true_predictions)\n",
    "        f5_score = (1 + self.beta ** 2) * recall * precision / ((self.beta ** 2) * precision + recall)\n",
    "\n",
    "        results = {\n",
    "            'recall': recall,\n",
    "            'precision': precision,\n",
    "            'f5': f5_score\n",
    "        }\n",
    "        return results\n",
    "\n",
    "# compute_metrics = MetricsComputer(all_labels=all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e470ae7a-fea0-48b5-ad4f-46be0ba5cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "    idx = 0\n",
    "    spans = []\n",
    "    span = []\n",
    "\n",
    "    for i, token in enumerate(document):\n",
    "        if token != target[idx]:\n",
    "            idx = 0\n",
    "            span = []\n",
    "            continue\n",
    "        span.append(i)\n",
    "        idx += 1\n",
    "        if idx == len(target):\n",
    "            spans.append(span)\n",
    "            span = []\n",
    "            idx = 0\n",
    "            continue\n",
    "    \n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4fd14a5-1ca7-464b-9a6a-2274abc76761",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRFScore:\n",
    "    \"\"\"A precision / recall / F score.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        tp: int = 0,\n",
    "        fp: int = 0,\n",
    "        fn: int = 0,\n",
    "    ) -> None:\n",
    "        self.tp = tp\n",
    "        self.fp = fp\n",
    "        self.fn = fn\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.tp + self.fp + self.fn\n",
    "\n",
    "    def __iadd__(self, other):  # in-place add\n",
    "        self.tp += other.tp\n",
    "        self.fp += other.fp\n",
    "        self.fn += other.fn\n",
    "        return self\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return PRFScore(\n",
    "            tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn\n",
    "        )\n",
    "\n",
    "    def score_set(self, cand: set, gold: set) -> None:\n",
    "        self.tp += len(cand.intersection(gold))\n",
    "        self.fp += len(cand - gold)\n",
    "        self.fn += len(gold - cand)\n",
    "\n",
    "    @property\n",
    "    def precision(self) -> float:\n",
    "        return self.tp / (self.tp + self.fp + 1e-100)\n",
    "\n",
    "    @property\n",
    "    def recall(self) -> float:\n",
    "        return self.tp / (self.tp + self.fn + 1e-100)\n",
    "\n",
    "    @property\n",
    "    def f1(self) -> float:\n",
    "        p = self.precision\n",
    "        r = self.recall\n",
    "        return 2 * ((p * r) / (p + r + 1e-100))\n",
    "\n",
    "    @property\n",
    "    def f5(self) -> float:\n",
    "        beta = 5\n",
    "        p = self.precision\n",
    "        r = self.recall\n",
    "\n",
    "        fbeta = (1+(beta**2))*p*r / ((beta**2)*p + r + 1e-100)\n",
    "        return fbeta\n",
    "\n",
    "    def to_dict(self) -> dict[str, float]:\n",
    "        return {\"p\": self.precision, \"r\": self.recall, \"f5\": self.f5}\n",
    "\n",
    "\n",
    "class MetricsComputerV2:\n",
    "    nlp = English()\n",
    "\n",
    "    def __init__(self, eval_ds: Dataset, label2id: dict, conf_thresh: float = 0.9) -> None:\n",
    "        self.ds = eval_ds.remove_columns(\"labels\").rename_columns({\"provided_labels\": \"labels\"})\n",
    "        self.gt_df = self.create_gt_df(self.ds)\n",
    "        self.label2id = label2id\n",
    "        self.confth = conf_thresh\n",
    "        \n",
    "    def __call__(self, eval_preds: EvalPrediction) -> dict:\n",
    "        pred_df = self.create_pred_df(eval_preds.predictions)\n",
    "        return self.compute_metrics_from_df(self.gt_df, pred_df)\n",
    "        \n",
    "    @staticmethod\n",
    "    def create_gt_df(ds: Dataset):\n",
    "        gt = []\n",
    "        for row in ds:\n",
    "            for token_idx, (token, label) in enumerate(zip(row[\"tokens\"], row[\"labels\"])):\n",
    "                if label == \"O\":\n",
    "                    continue\n",
    "                gt.append(\n",
    "                    {\"document\": row[\"document\"], \"token\": token_idx, \"label\": label, \"token_str\": token}\n",
    "                )\n",
    "        gt_df = pd.DataFrame(gt)\n",
    "        gt_df[\"row_id\"] = gt_df.index\n",
    "        \n",
    "        return gt_df\n",
    "    \n",
    "    def create_pred_df(self, prediction: np.ndarray) -> pd.DataFrame:\n",
    "        ### construct prediction df\n",
    "        o_index = self.label2id[\"O\"]\n",
    "        preds = prediction.argmax(-1)\n",
    "        preds_without_o = prediction[:,:,:o_index].argmax(-1)\n",
    "        o_preds = prediction[:,:,o_index]\n",
    "        preds_final = np.where(o_preds < self.confth, preds_without_o , preds)\n",
    "\n",
    "        triplets = set()\n",
    "        processed = []\n",
    "\n",
    "        # Iterate over document\n",
    "        for p_doc, token_map, offsets, tokens, doc in zip(\n",
    "            preds_final, self.ds[\"token_map\"], self.ds[\"offset_mapping\"], self.ds[\"tokens\"], self.ds[\"document\"]\n",
    "        ):\n",
    "            # Iterate over sequence\n",
    "            for p_token, (start_idx, end_idx) in zip(p_doc, offsets):\n",
    "                label_pred = id2label[p_token]\n",
    "\n",
    "                if start_idx + end_idx == 0:\n",
    "                    # [CLS] token i.e. BOS\n",
    "                    continue\n",
    "\n",
    "                if token_map[start_idx] == -1:\n",
    "                    start_idx += 1\n",
    "\n",
    "                # ignore \"\\n\\n\"\n",
    "                while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "                    start_idx += 1\n",
    "\n",
    "                if start_idx >= len(token_map): \n",
    "                    break\n",
    "\n",
    "                token_id = token_map[start_idx]\n",
    "                triplet = (label_pred, token_id, tokens[token_id])\n",
    "\n",
    "                # ignore \"O\", preds, phone number and  email\n",
    "                if label_pred in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"B-PHONE_NUM\") or token_id == -1:\n",
    "                    continue   \n",
    "\n",
    "                if triplet in triplets:\n",
    "                    continue\n",
    "\n",
    "                processed.append(\n",
    "                    {\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]}\n",
    "                )\n",
    "                triplets.add(triplet)\n",
    "\n",
    "        email_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
    "        phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n",
    "        emails = []\n",
    "        phone_nums = []\n",
    "\n",
    "        for _data in self.ds:\n",
    "            # email\n",
    "            for token_idx, token in enumerate(_data[\"tokens\"]):\n",
    "                if re.fullmatch(email_regex, token) is not None:\n",
    "                    emails.append(\n",
    "                        {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
    "                    )\n",
    "            # phone number\n",
    "            matches = phone_num_regex.findall(_data[\"full_text\"])\n",
    "            if not matches:\n",
    "                continue\n",
    "            for match in matches:\n",
    "                target = [t.text for t in self.nlp.tokenizer(match)]\n",
    "                matched_spans = find_span(target, _data[\"tokens\"])\n",
    "            for matched_span in matched_spans:\n",
    "                for intermediate, token_idx in enumerate(matched_span):\n",
    "                    prefix = \"I\" if intermediate else \"B\"\n",
    "                    phone_nums.append(\n",
    "                        {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": _data[\"tokens\"][token_idx]}\n",
    "                    )\n",
    "\n",
    "        pred_df = pd.DataFrame(processed + emails + phone_nums)\n",
    "        pred_df[\"row_id\"] = list(range(len(pred_df)))\n",
    "\n",
    "        return pred_df\n",
    "        \n",
    "    def compute_metrics_from_df(self, gt_df, pred_df):\n",
    "        \"\"\"\n",
    "        Compute the LB metric (lb) and other auxiliary metrics\n",
    "        \"\"\"\n",
    "\n",
    "        references = {(row.document, row.token, row.label) for row in gt_df.itertuples()}\n",
    "        predictions = {(row.document, row.token, row.label) for row in pred_df.itertuples()}\n",
    "\n",
    "        score_per_type = defaultdict(PRFScore)\n",
    "        references = set(references)\n",
    "\n",
    "        for ex in predictions:\n",
    "            pred_type = ex[-1] # (document, token, label)\n",
    "            if pred_type != 'O':\n",
    "                pred_type = pred_type[2:] # avoid B- and I- prefix\n",
    "\n",
    "            if pred_type not in score_per_type:\n",
    "                score_per_type[pred_type] = PRFScore()\n",
    "\n",
    "            if ex in references:\n",
    "                score_per_type[pred_type].tp += 1\n",
    "                references.remove(ex)\n",
    "            else:\n",
    "                score_per_type[pred_type].fp += 1\n",
    "\n",
    "        for doc, tok, ref_type in references:\n",
    "            if ref_type != 'O':\n",
    "                ref_type = ref_type[2:] # avoid B- and I- prefix\n",
    "\n",
    "            if ref_type not in score_per_type:\n",
    "                score_per_type[ref_type] = PRFScore()\n",
    "            score_per_type[ref_type].fn += 1\n",
    "\n",
    "        totals = PRFScore()\n",
    "\n",
    "        for prf in score_per_type.values():\n",
    "            totals += prf\n",
    "\n",
    "        return {\n",
    "            \"precision\": totals.precision,\n",
    "            \"recall\": totals.recall,\n",
    "            \"f5\": totals.f5,\n",
    "            **{\n",
    "                f\"{v_k}-{k}\": v_v \n",
    "                for k in set([l[2:] for l in self.label2id.keys() if l!= 'O'])\n",
    "                for v_k, v_v in score_per_type[k].to_dict().items()\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a7d8dc-6579-4678-a2ba-27d22e6aac00",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1700da69-ff6a-4fb6-9901-aaf93d589b29",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f96d12afaaf4a208ce701fea498b1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c89f2e37c24423ba349b6b3f9472673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.62G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at Gladiator/microsoft-deberta-v3-large_ner_conll2003 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([9, 1024]) in the checkpoint and torch.Size([13, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class ModelInit:\n",
    "    def __init__(\n",
    "        self, \n",
    "        checkpoint: str, \n",
    "        id2label: dict, \n",
    "        label2id: dict,\n",
    "        freeze_embedding: bool,\n",
    "        freeze_layers: int,\n",
    "    ) -> None:\n",
    "        self.model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "            checkpoint,\n",
    "            num_labels=len(id2label),\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        for param in self.model.deberta.embeddings.parameters():\n",
    "            param.requires_grad = False if freeze_embedding else True\n",
    "        for layer in self.model.deberta.encoder.layer[:freeze_layers]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.weight = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "    def __call__(self) -> DebertaV2ForTokenClassification:\n",
    "        self.model.load_state_dict(self.weight)\n",
    "        return self.model\n",
    "\n",
    "model_init = ModelInit(\n",
    "    TRAINING_MODEL_PATH, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id, \n",
    "    freeze_embedding=FREEZE_EMBEDDING, \n",
    "    freeze_layers=FREEZE_LAYERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74884fb-fc4d-4684-a79c-fdfecb64198a",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7eaea9c-c63a-4a77-932f-22ff25360923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split according to document id\n",
    "folds = [\n",
    "    (\n",
    "        np.array([i for i, d in enumerate(original_data) if int(d[\"document\"]) % N_SPLITS != s]),\n",
    "        np.array([i for i, d in enumerate(original_data) if int(d[\"document\"]) % N_SPLITS == s])\n",
    "    )\n",
    "    for s in range(N_SPLITS)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21887fb3-e3b1-497c-92ad-64def1bc9511",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec8b773-e2e8-454a-b21d-b9e5cdf9eb09",
   "metadata": {},
   "source": [
    "#### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3d40f-e75d-4b05-85b0-7e0d620ce36a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc552a2d7d81446494a32a50ded757e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/788 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf83d66f9a9e49288185e4150f5b49b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/788 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e480f922cc7d4fc6b556294cc383a0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/788 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121cd0aa609c4fce900adf24810bdf76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/788 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63032a1f6f534b23945b5d8cff2f37fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/787 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2831f5d2deb41d386cdf173bca9046f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/787 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52be7b980e344493983cd5dc264af5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/787 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc85245ccdd4c55a0d79bbfeda976d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/787 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88deeb443284e39979e8668977eeeb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/427 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31213a162254773a0b85ac1f1554b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/427 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e16732896ee476ebe5be91b28249c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/427 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6136e85c6c74aac90cfb0da301fb01b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/427 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5975c327e90843bb91320eed9739a0d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/427 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f1400d7a544ee388847786ff2d72cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/427 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be29eb85a75b4dd0a49bed28974e6031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/427 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ef1659bf9b4c0d938d7187124f3669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/426 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DebertaV2ForTokenClassification.forward` and have been ignored: token_map, length, trailing_whitespace, full_text, document, tokens, provided_labels, offset_mapping, token_indices. If token_map, length, trailing_whitespace, full_text, document, tokens, provided_labels, offset_mapping, token_indices are not expected by `DebertaV2ForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6300\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 392\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/notebooks/wandb/run-20240211_144651-2qmxoj8l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/emiz6413/PII/runs/2qmxoj8l\" target=\"_blank\">fold-0</a></strong> to <a href=\"https://wandb.ai/emiz6413/PII\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='362' max='392' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [362/392 1:09:15 < 05:46, 0.09 it/s, Epoch 1.84/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F5</th>\n",
       "      <th>P-email</th>\n",
       "      <th>R-email</th>\n",
       "      <th>F5-email</th>\n",
       "      <th>P-name Student</th>\n",
       "      <th>R-name Student</th>\n",
       "      <th>F5-name Student</th>\n",
       "      <th>P-url Personal</th>\n",
       "      <th>R-url Personal</th>\n",
       "      <th>F5-url Personal</th>\n",
       "      <th>P-phone Num</th>\n",
       "      <th>R-phone Num</th>\n",
       "      <th>F5-phone Num</th>\n",
       "      <th>P-street Address</th>\n",
       "      <th>R-street Address</th>\n",
       "      <th>F5-street Address</th>\n",
       "      <th>P-username</th>\n",
       "      <th>R-username</th>\n",
       "      <th>F5-username</th>\n",
       "      <th>P-id Num</th>\n",
       "      <th>R-id Num</th>\n",
       "      <th>F5-id Num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.003595</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.029936</td>\n",
       "      <td>0.031095</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.062840</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>0.719309</td>\n",
       "      <td>0.831076</td>\n",
       "      <td>0.826139</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.709801</td>\n",
       "      <td>0.889246</td>\n",
       "      <td>0.880682</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.311005</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.818774</td>\n",
       "      <td>0.895225</td>\n",
       "      <td>0.892021</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.826149</td>\n",
       "      <td>0.922953</td>\n",
       "      <td>0.918813</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.950857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.103586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.838535</td>\n",
       "      <td>0.880969</td>\n",
       "      <td>0.879258</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.843400</td>\n",
       "      <td>0.907705</td>\n",
       "      <td>0.905051</td>\n",
       "      <td>0.682353</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.869164</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.231683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.815480</td>\n",
       "      <td>0.938703</td>\n",
       "      <td>0.933279</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.825752</td>\n",
       "      <td>0.947030</td>\n",
       "      <td>0.941711</td>\n",
       "      <td>0.656566</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.966266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.842707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.878498</td>\n",
       "      <td>0.917320</td>\n",
       "      <td>0.915764</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.889404</td>\n",
       "      <td>0.922953</td>\n",
       "      <td>0.921616</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.959078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.867493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output2/fold_0/checkpoint-50\n",
      "Configuration saved in output2/fold_0/checkpoint-50/config.json\n",
      "Model weights saved in output2/fold_0/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in output2/fold_0/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in output2/fold_0/checkpoint-50/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForTokenClassification.forward` and have been ignored: token_map, length, trailing_whitespace, full_text, document, tokens, provided_labels, offset_mapping. If token_map, length, trailing_whitespace, full_text, document, tokens, provided_labels, offset_mapping are not expected by `DebertaV2ForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3415\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to output2/fold_0/checkpoint-100\n",
      "Configuration saved in output2/fold_0/checkpoint-100/config.json\n",
      "Model weights saved in output2/fold_0/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in output2/fold_0/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in output2/fold_0/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [output2/fold_0/checkpoint-50] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForTokenClassification.forward` and have been ignored: token_map, length, trailing_whitespace, full_text, document, tokens, provided_labels, offset_mapping. If token_map, length, trailing_whitespace, full_text, document, tokens, provided_labels, offset_mapping are not expected by `DebertaV2ForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3415\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to output2/fold_0/checkpoint-150\n",
      "Configuration saved in output2/fold_0/checkpoint-150/config.json\n",
      "Model weights saved in output2/fold_0/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in output2/fold_0/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in output2/fold_0/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [output2/fold_0/checkpoint-100] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForTokenClassification.forward` and have been ignored: token_map, length, trailing_whitespace, full_text, document, tokens, provided_labels, offset_mapping. If token_map, length, trailing_whitespace, full_text, document, tokens, provided_labels, offset_mapping are not expected by `DebertaV2ForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3415\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to output2/fold_0/checkpoint-200\n",
      "Configuration saved in output2/fold_0/checkpoint-200/config.json\n",
      "Model weights saved in output2/fold_0/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in output2/fold_0/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in output2/fold_0/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [output2/fold_0/checkpoint-150] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForTokenClassification.forward` and have been ignored: token_map, length, trailing_whitespace, full_text, document, tokens, provided_labels, offset_mapping. If token_map, length, trailing_whitespace, full_text, document, tokens, provided_labels, offset_mapping are not expected by `DebertaV2ForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3415\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to output2/fold_0/checkpoint-250\n",
      "Configuration saved in output2/fold_0/checkpoint-250/config.json\n",
      "Model weights saved in output2/fold_0/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in output2/fold_0/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in output2/fold_0/checkpoint-250/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForTokenClassification.forward` and have been ignored: token_map, length, trailing_whitespace, full_text, document, tokens, provided_labels, offset_mapping. If token_map, length, trailing_whitespace, full_text, document, tokens, provided_labels, offset_mapping are not expected by `DebertaV2ForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3415\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to output2/fold_0/checkpoint-300\n",
      "Configuration saved in output2/fold_0/checkpoint-300/config.json\n",
      "Model weights saved in output2/fold_0/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in output2/fold_0/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in output2/fold_0/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [output2/fold_0/checkpoint-200] due to args.save_total_limit\n",
      "Deleting older checkpoint [output2/fold_0/checkpoint-250] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForTokenClassification.forward` and have been ignored: token_map, length, trailing_whitespace, full_text, document, tokens, provided_labels, offset_mapping. If token_map, length, trailing_whitespace, full_text, document, tokens, provided_labels, offset_mapping are not expected by `DebertaV2ForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3415\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to output2/fold_0/checkpoint-350\n",
      "Configuration saved in output2/fold_0/checkpoint-350/config.json\n",
      "Model weights saved in output2/fold_0/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in output2/fold_0/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in output2/fold_0/checkpoint-350/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_idx, eval_idx) in enumerate(folds):\n",
    "    args.run_name = f\"fold-{fold_idx}\"\n",
    "    args.output_dir = os.path.join(OUTPUT_DIR, f\"fold_{fold_idx}\")\n",
    "    train_data = [d for d in original_data if int(d[\"document\"]) % N_SPLITS != fold_idx]\n",
    "    train_data = train_data + extra_data\n",
    "    eval_data = [d for d in original_data if int(d[\"document\"]) & N_SPLITS == fold_idx]\n",
    "    train_data = stride_rows(train_data, max_length=TRAIN_MAX_LENGTH, stride=STRIDE)\n",
    "    train_ds = Dataset.from_pandas(pd.DataFrame(train_data).rename(columns={\"labels\": \"provided_labels\"})) \n",
    "    eval_ds = Dataset.from_pandas(pd.DataFrame(eval_data).rename(columns={\"labels\": \"provided_labels\"}))\n",
    "    train_ds = train_ds.map(train_encoder, num_proc=os.cpu_count())\n",
    "    eval_ds = eval_ds.map(eval_encoder, num_proc=os.cpu_count())\n",
    "    trainer = Trainer(\n",
    "        args=args,\n",
    "        model_init=model_init,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=MetricsComputerV2(eval_ds=eval_ds, label2id=label2id),\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16),\n",
    "    )\n",
    "    trainer.train()\n",
    "    eval_res = trainer.evaluate(eval_dataset=eval_ds)\n",
    "    with open(os.path.join(args.output_dir, \"eval_result.json\"), \"w\") as f:\n",
    "        json.dump(eval_res, f)\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    wandb.finish()\n",
    "    break\n",
    "    \n",
    "# keys = set(cv_res[0].keys())\n",
    "# for key in keys:\n",
    "#     wandb.log({f\"cv_{key.split('_')[-1]}\": np.nanmean([r.get(key, np.nan) for r in cv_res])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af79fbd-6091-43d8-8db6-a8c24fe94a3b",
   "metadata": {},
   "source": [
    "#### Train with full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd36d3-579a-4965-982b-00c2e1887c50",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = concatenate_datasets([ds[\"original\"], ds[\"extra\"]])\n",
    "train_ds = train_ds.map(train_encoder, num_proc=os.cpu_count())\n",
    "args.evaluation_strategy = \"no\"\n",
    "args.run_name = f\"all_data\"\n",
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model_init=model_init,\n",
    "    train_dataset=train_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16),\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5242de1-c378-4807-a276-a1a1fed7626a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
