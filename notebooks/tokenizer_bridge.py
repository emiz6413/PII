import warnings
import spacy_alignments as alignments

from transformers import AutoTokenizer

# Implicit assumptions:
# - If the tokenized input data has white-space tokens
#   (for instance if tokens were generated by the Spacy en_core_web_lg tokenizer),
#   then we assume that white-space tokens will always be labeled as "O" (other).
#
#   Most tokenizers by default do not represent white-space tokens. In the final
#   output (convert_internal_to_external) we need to always associate "O" labels
#   with those unrepresented tokens (assuming they were represented in the original
#   input), unless those tokens are inside an entity span!


ZWSP = chr(0x200B)


def inside_out(dd: dict) -> dict:
    assert all(hasattr(v, "__hash__") for v in dd.values())
    assert len(dd) == len(set(dd.values()))
    return dict((v, k) for (k, v) in dd.items())


def isspace(c: str) -> bool:
    return c.replace(ZWSP, " ").isspace()


class TokenizerBridgeBase:
    def __call__(self, example: dict, **kwargs):
        ...

    def convert_external_to_internal(self, example: dict, rt: str = "ids") -> dict:
        ...

    def convert_internal_to_external(self, example: dict, rt: str = "str") -> dict:
        ...

    def there_and_back_again(self, example: dict, redo: bool = False, **kwargs) -> dict:
        """
        Roundtrip conversion test.
        """

        # Normally the following have already been called (in order)
        # - __call__ -> basic tokenization
        #
        # - convert_external_to_internal -> labels are internal label ids
        #                                   external_labels are external label ids
        #                                   internal_labels are same as labels
        #
        # - convert_internal_to_external -> labels are external label str
        #                                   external labels are same as labels
        #                                   internal labels are internal label str

        if redo or "input_ids" not in example:
            example.update(self.__call__(example, **kwargs))

        if redo or "external_labels" not in example:
            example.update(self.convert_external_to_internal(example, rt="ids"))

            assert all(isinstance(x, int) for x in example["labels"])
            assert all(isinstance(x, int) for x in example["external_labels"])
            assert all(isinstance(x, int) for x in example["internal_labels"])
            assert len(example["labels"]) == len(example["input_ids"])
            assert example["labels"] == example["internal_labels"]

        if redo or "internal_labels" not in example:
            example.update(self.convert_internal_to_external(example, rt="str"))

            assert all(isinstance(x, str) for x in example["labels"])
            assert all(isinstance(x, str) for x in example["external_labels"])
            assert all(isinstance(x, str) for x in example["internal_labels"])

        assert "input_ids" in example
        assert "tokens" in example
        assert "labels" in example
        assert "external_labels" in example
        assert "internal_labels" in example

        tp = type(example["labels"][0])
        assert tp in (int, str)
        assert all(isinstance(x, tp) for x in example["labels"])
        assert all(isinstance(x, tp) for x in example["external_labels"])
        assert all(isinstance(x, tp) for x in example["internal_labels"])
        assert len(example["labels"]) == len(example["tokens"])

        return dict(ok=example["labels"] == example["external_labels"])


NO_LABEL = "-"


class TokenizerBridge(TokenizerBridgeBase):
    def __init__(
        self,
        internal_tokenizer: AutoTokenizer,
        label2id: dict,
        max_length: int = None,
        o_label: str = "O",
        label_all_word_pieces: bool = False,
        **kwargs,
    ):
        assert o_label in label2id
        self.tokenizer = internal_tokenizer
        self.label2id = label2id
        self.id2label = inside_out(label2id)
        self.max_length = max_length
        self.o_label = o_label
        self.o_label_id = label2id[o_label]
        self.label_all_word_pieces = label_all_word_pieces

    def __call__(self, example: dict, **kwargs):
        """Call tokenizer on example['text'] with keyword arguments `kwargs`.

        Returns a dict with at least the keys 'input_ids', 'token_type_ids',
        and 'attention_mask'.
        """
        return self.tokenizer(example["text"], **kwargs)

    def convert_external_to_internal(self, example: dict, rt="ids") -> dict:
        """Convert external labels into internal labels.

        Input `example` should have as keys:
          - text - input text
          - tokens - token forms according to external tokenizer
          - labels - labels corresponding to **external** tokenizer
          - input_ids - input ids according to internal tokenizer

        Returns a dict with keys:
          - s2t - alignment between external tokens and internal tokens
          - labels - labels (or ids) corresponding to **internal** tokenizer
          - external_labels - original labels (or ids), according to external tokenizer
          - internal_abels - copy of labels

        If `rt` is "ids", then both labels and external_labels are returned as ids.
        If `rt` is "str", they are returned as strings.

        This function can be called to prepare a dataset for training.
        """
        if rt not in ("ids", "str"):
            raise ValueError

        input_ids = example["input_ids"]

        if "labels" not in example:  # non-training, default
            example["labels"] = [self.o_label for _ in example["tokens"]]
        else:
            assert isinstance(example["labels"][0], str)

        labels = example["labels"]

        if rt == "ids":
            o_label = self.o_label_id
            external_labels = [self.label2id[x] for x in labels]
            internal_labels = [-100 for _ in input_ids]  # Note: -100, not o_label_id
        else:
            o_label = self.o_label
            external_labels = labels
            internal_labels = [
                NO_LABEL for _ in input_ids
            ]  # Note: NO_LABEL, not o_label

        if "s2t" in example:
            s2t = example["s2t"]
        else:
            internal_tokens = self.tokenizer.convert_ids_to_tokens(input_ids)
            external_tokens = example["tokens"]
            assert isinstance(internal_tokens[0], str)  # cannot deal with strides yet
            s2t, _ = alignments.get_alignments(external_tokens, internal_tokens[1:-1])

        for i, (label, indices) in enumerate(zip(external_labels, s2t)):
            if not indices:
                continue
            j = indices[0]
            try:
                internal_labels[j + 1] = label
                if self.label_all_word_pieces:
                    for j in indices[1:]:
                        internal_labels[j + 1] = label
            except IndexError:
                # This can happen if the internal tokenizer is truncating the input.
                # So, some of the original tokens/labels will then be skipped.
                nskipped = len(s2t) - i
                warnings.warn(
                    f"[convert_external_to_internal] Truncation at index {i}. "
                    "Internal labels will skip {nskipped} external labels."
                )
                break

        return dict(
            s2t=s2t,
            labels=internal_labels,
            internal_labels=internal_labels,
            external_labels=external_labels,
        )

    def convert_internal_to_external(self, example: dict, rt="str") -> dict:
        """Convert internal labels to external labels.

         Input `example` should have as keys:
          - text - input text
          - tokens - token forms according to external tokenizer
          - labels - label ids of **internal** tokenizer (predicted label ids)
          - input_ids - input ids according to internal tokenizer

        Returns a dict with keys:
          - s2t - alignment between tokens and internal tokens
          - labels - labels (or ids) corresponding to **external** tokenizer
          - internal_labels - original input labels (or ids), according to internal tokenizer
          - external_labels - copy of labels

        If `rt` is "ids", then both labels and external_labels are returned as ids.
        If `rt` is "str", they are returned as strings (which is the default).

        This function can be called to convert a sequence of predicted labels
        into a sequence of external labels, aligned with the original external tokens.
        """
        if rt not in ("str", "ids"):
            raise ValueError

        def backfill(label, i):
            # when s2t[j] is empty, then some external tokens are not
            # represented by the internal tokenizer;
            # but the corresponding labels may still need to be set!
            # in particular they need to be filled in backwards
            # (resetting O labels backwards) when the current label is
            # an I-label.

            label_str = self.id2label.get(label, label)
            if label_str[0] == "I":
                j = i - 1
                while j > 0 and not s2t[j]:
                    external_labels[j] = label
                    j -= 1

        input_ids = example["input_ids"]

        if "labels" not in example:  # non-training/debug only
            example["labels"] = [-100 for _ in input_ids]
        else:
            assert isinstance(example["labels"][0], int)

        if rt == "str":
            no_label = NO_LABEL
            o_label = self.o_label
            internal_labels = [
                self.id2label.get(x, no_label) for x in example["labels"]
            ]
        else:
            no_label = -100
            o_label = self.o_label_id
            internal_labels = example["labels"][:]

        external_labels = [o_label for _ in example["tokens"]]

        if "s2t" in example:
            s2t = example["s2t"]
        else:
            internal_tokens = self.tokenizer.convert_ids_to_tokens(input_ids)
            external_tokens = example["tokens"]
            assert isinstance(internal_tokens[0], str)  # cannot deal with strides yet
            s2t, _ = alignments.get_alignments(external_tokens, internal_tokens[1:-1])

        for i, indices in enumerate(s2t):
            if not indices:
                continue

            # this takes its clue from the first internal label
            # this logic could/should be made more flexible

            j = indices[0]
            try:
                label = internal_labels[j + 1]
            except IndexError:
                nskipped = len(s2t) - i
                warnings.warn(
                    f"[convert_internal_to_external] Truncation at index {i}. "
                    "The last {nskipped} labels will be forced to be 'O'."
                )
                break

            if label != no_label and label != o_label:
                external_labels[i] = label
                backfill(label, i)

        return dict(
            s2t=s2t,
            labels=external_labels,
            external_labels=external_labels,
            internal_labels=internal_labels,
        )

    def __str__(self):
        return (
            f"TokenizerBridge(tokenizer={self.tokenizer.__class__.__name__}"
            f"({self.tokenizer.name_or_path!r}), "
            f"label2id={self.label2id}, "
            f"max_length={self.max_length}, o_label={self.o_label}, "
            f"o_label_id={self.o_label_id}, label_all_word_pieces={self.label_all_word_pieces})"
        )

    __repr__ = __str__


USE_BACKFILL = False


class TokenizerBridgeWithOffsets(TokenizerBridgeBase):
    def __init__(
        self,
        internal_tokenizer: AutoTokenizer,
        label2id: dict,
        max_length: int = 512,
        o_label: str = "O",
        label_all_word_pieces: bool = True,
        use_first_piece_for_label: bool = False,
        use_last_piece_for_label: bool = True,
    ):
        assert o_label in label2id
        self.tokenizer = internal_tokenizer
        self.label2id = label2id
        self.id2label = inside_out(label2id)
        self.o_label = o_label
        self.o_label_id = label2id[o_label]
        self.max_length = max_length
        self.label_all_word_pieces = label_all_word_pieces
        self.use_first_piece_for_label = use_first_piece_for_label
        self.use_last_piece_for_label = use_last_piece_for_label

        if not label_all_word_pieces:
            assert use_first_piece_for_label
        if use_last_piece_for_label:
            assert label_all_word_pieces

    def __call__(self, example: dict, **kwargs):
        """Call tokenizer on example['text'] with keyword arguments `kwargs`.

        Returns a dict with at least the keys
        - input_ids
        - token_type_ids
        - attention_mask
        - offset_mapping

        Note: This always tokenizes with return_offsets_mapping=True , truncation=True and
        max_length=self.max_length.
        """
        kwargs.update(
            dict(
                return_offsets_mapping=True, truncation=True, max_length=self.max_length
            )
        )
        return self.tokenizer(example["text"], **kwargs)

    def convert_external_to_internal(self, example: dict, rt="ids") -> dict:
        """Convert external (Spacy) labels to internal labels.

        Example should have keys:
        - input_ids - according to the internal tokenizer
        - offsets_mapping - offset mapping of internal tokens
        - tokens - external (Spacy) tokens
        - labels - **external** (Spacy) labels
        - trailing_whitespace - booleans indicating if external tokens
                                are followed by white-space in text

        Returns a dict with keys
        - labels - **internal** labels
        - internal_labels - internal labels
        - external_labels - copy of the original (Spacy) labels
        - length - number of internal tokens (including CLS and SEP)
        - token_map - mapping of text char indices to external token indices
        """
        if rt not in ("ids", "str"):
            raise ValueError

        input_ids = example["input_ids"]
        offset_mapping = example["offset_mapping"]
        tokens = example["tokens"]

        text = []
        labels = []  # labels[i] = external label for char index i
        token_map = []  # token_map[i] = external token index for char index i

        for i, (token, label, ws) in enumerate(
            zip(example["tokens"], example["labels"], example["trailing_whitespace"])
        ):
            text.append(token)
            labels.extend([label for _ in token])
            token_map.extend([i for _ in token])

            if ws:
                text.append(" ")
                labels.append("O")
                token_map.append(-1)

        text = "".join(text)
        internal_labels = [NO_LABEL for _ in input_ids]
        seen = set()

        for j, (start, end) in enumerate(
            offset_mapping
        ):  # iterate over internal tokens
            if start == end == 0:
                continue

            if token_map[start] < 0:  # skip added ws
                start += 1

            # In the Deberta tokenizer the token spans include exactly _one_
            # preceding space (if there is a space just before the token).
            # So in principle it should be ok to use 'if' instead of 'while' here.
            #
            # Deberta also treats U+200B as white-space character, even though
            # Python and Spacy do NOT treat that as such. So, we should not
            # simply use the Python isspace function!

            while start < len(token_map) and isspace(tokens[token_map[start]]):
                start += 1

            if start >= len(token_map):
                nskipped = len(internal_labels) - j
                warnings.warn(
                    f"[convert_external_to_internal]: skipping {nskipped} internal tokens"
                )
                break

            # associate the label either with all word pieces or only with the first
            token_index = token_map[start]
            if token_index in seen and not self.label_all_word_pieces:
                continue

            internal_labels[j] = labels[start]
            seen.add(token_index)

        if rt == "ids":
            internal_labels = [self.label2id.get(x, -100) for x in internal_labels]
            external_labels = [self.label2id[x] for x in example["labels"]]
        else:
            external_labels = example["labels"][:]

        return dict(
            labels=internal_labels,
            external_labels=external_labels,
            internal_labels=internal_labels,
            length=len(input_ids),
            token_map=token_map,
        )

    def convert_internal_to_external(self, example: dict, rt="str") -> dict:
        """Convert internal labels to external (Spacy) labels.

        Example should have as keys:
        - input_ids - input_ids of internal tokenizer
        - offset_mapping - offsets_mapping of internal tokenizer
        - labels - label ids of internal tokenizer (predicted ids)
        - token_map - mapping of char indices to external token indices

        Returns a dict with keys:
        - labels - **external** labels
        - internal_labels - copy of input labels
        - external_labels - copy of external labels
        """
        if rt not in ("str", "ids"):
            raise ValueError

        def backfill(label, i):
            if not USE_BACKFILL:
                # published methods on Kaggle are skipping this step,
                # but it is needed
                return
            label_str = self.id2label.get(label, label)
            if label_str[0] == "I":
                j = i - 1
                while j > 0 and external_labels[j] in (self.o_label, self.o_label_id):
                    external_labels[j] = label
                    j -= 1

        input_ids = example["input_ids"]
        if "labels" not in example:  # non-training
            example["labels"] = [-100 for _ in input_ids]

        labels = example["labels"]
        tokens = example["tokens"]
        offsets = example["offset_mapping"]
        token_map = example["token_map"]

        if rt == "str":
            o_label = self.o_label
            internal_labels = [self.id2label.get(x, NO_LABEL) for x in labels]
        else:
            o_label = self.o_label_id
            internal_labels = labels[:]

        external_labels = [o_label for _ in example["tokens"]]

        seen = set()
        for i, (label, (start, end)) in enumerate(zip(internal_labels, offsets)):
            if start == end == 0:
                continue

            if token_map[start] < 0:
                start += 1

            # See comments in convert_external_to_internal

            while start < len(token_map) and isspace(tokens[token_map[start]]):
                start += 1

            if start >= len(token_map):
                nskipped = len(internal_labels) - i
                warnings.warn(
                    f"[convert_internal_to_external]: skipping {nskipped} internal labels"
                )
                break

            token_index = token_map[start]
            assert token_index >= 0

            if not self.label_all_word_pieces:  # get label from first prediction
                if token_index not in seen:
                    external_labels[token_index] = label
                    backfill(label, token_index)

            elif self.use_first_piece_for_label:  # also get label from first prediction
                if token_index not in seen:
                    external_labels[token_index] = label
                    backfill(label, token_index)

            elif self.use_last_piece_for_label:
                # requires training with labels associated with all word pieces;
                # gets label in effect from the last word piece!

                # this is the logic that everyone is implictly using
                # seems either incorrect or suboptimal, since it's possible that
                # different word pieces could get different predictions, and its
                # not sure that the last word piece is the best

                external_labels[token_index] = label
                backfill(label, token_index)

            else:
                # add logic to use _all_ predicted labels and somehow merge them
                # if they are not identical
                raise NotImplementedError

            seen.add(token_index)

        return dict(
            labels=external_labels,
            internal_labels=internal_labels,
            external_labels=external_labels,
        )

    def there_and_back_again(self, example, redo=False, **kwargs):
        """
        Roundtrip conversion test.
        """
        if redo:
            kwargs.update(
                dict(
                    return_offsets_mapping=True,
                    truncation=True,
                    max_length=self.max_length,
                )
            )
        return super().there_and_back_again(example, redo=redo, **kwargs)

    def __str__(self):
        return (
            f"TokenizerBridgeWithOffsets(tokenizer={self.tokenizer.__class__.__name__}"
            f"({self.tokenizer.name_or_path!r}), "
            f"label2id={self.label2id}, "
            f"max_length={self.max_length}, o_label={self.o_label}, "
            f"o_label_id={self.o_label_id}, label_all_word_pieces={self.label_all_word_pieces}, "
            f"use_first_piece_for_label={self.use_first_piece_for_label}, "
            f"use_last_piece_for_label={self.use_last_piece_for_label})"
        )

    __repr__ = __str__


def test1(model="microsoft/deberta-v3-base", klas=TokenizerBridge):
    import spacy

    nlp = spacy.load("en_core_web_lg")

    tokenizer = AutoTokenizer.from_pretrained(model)

    texts = (
        "This is a test! (123)456-7890 ?",
        "This is #$%# pete@gmail.com https://querty/abc- xyz",
        "Address: 123 Dirt Rd. Apt. 4,\n   Down-town     MA\n\n  02200\n",
    )

    def fake_label(token):
        label = token.lower().strip()
        if not label:
            # we never want to label white-space as sth positive
            # since that would not work with most tokenizers;
            return "O"
        return "B-" + label[0]

    for text in texts:
        doc = nlp(text)
        tokens = [x.text for x in doc]
        trailing_ws = [bool(x.whitespace_) for x in doc]

        labels = [fake_label(x) for x in tokens]
        o_label = "O"
        label_set = set(labels)
        label_set.add(o_label)
        labels2id = dict((k, i) for (i, k) in enumerate(label_set))

        bridge = klas(tokenizer, labels2id, o_label=o_label)
        example = dict(
            text=text, tokens=tokens, trailing_whitespace=trailing_ws, labels=labels
        )

        z = bridge.there_and_back_again(example, redo=True)
        assert z["ok"], f"Failure for {text!r}"
        assert example["labels"] == labels, f"{labels} vs {example['labels']}"

    print("OK")


def test2(klas=TokenizerBridge, path="datasets/train.json"):
    # We don't trigger failures in the current kaggle and extra training data.

    import pandas as pd
    from datasets import Dataset
    from prepare_data import LABEL2ID, tokenizer, load

    bridge = klas(tokenizer, label2id=LABEL2ID, max_length=3072)

    df = pd.read_json(path)
    df.rename(columns={"full_text": "text"}, inplace=True)
    df["pii"] = df.labels.apply(lambda labels: any(x != "O" for x in labels))
    df = df[df.pii]

    ds = Dataset.from_pandas(df)

    ds = ds.map(bridge, fn_kwargs=dict(max_length=3072, truncation=True), num_proc=6)
    ds = ds.map(bridge.convert_external_to_internal)
    ds = ds.map(bridge.convert_internal_to_external)
    ds = ds.map(bridge.there_and_back_again)

    df = ds.to_pandas()
    assert all(df.ok)

    print("OK")


def test3(model="microsoft/deberta-v3-base", klas=TokenizerBridge, alt=False):
    #
    # Fails with current default settings for TokenizerBridgeWithOffsets
    #

    from prepare_data import LABEL2ID
    import spacy

    nlp = spacy.load("en_core_web_lg")
    tokenizer = AutoTokenizer.from_pretrained(model)
    bridge = klas(
        tokenizer,
        label2id=LABEL2ID,
        label_all_word_pieces=True,
        use_first_piece_for_label=False,
        use_last_piece_for_label=True,
    )

    if alt:
        ZWS = chr(0x200B)  # to spice it up a bit
        text = f"He lives at 123 {ZWS}Dirt{ZWS} Rd. Apt. 4\n   {ZWS}    Ducktown CA  01234\n\n"

        tokens = [
            "He",
            "lives",
            "at",
            "123",
            "\u200bDirt\u200b",
            "Rd",
            ".",
            "Apt",
            ".",
            "4",
            "\n   ",
            "\u200b",
            "   ",
            "Ducktown",
            "CA",
            " ",
            "01234",
            "\n\n",
        ]

        labels = [
            "O",
            "O",
            "O",
            "B-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "O",
        ]

    else:
        text = "He lives at 123 Dirt Rd. Apt. 4\n       Ducktown CA  01234\n\n"
        tokens = [
            "He",
            "lives",
            "at",
            "123",
            "Dirt",
            "Rd",
            ".",
            "Apt",
            ".",
            "4",
            "\n       ",
            "Ducktown",
            "CA",
            " ",
            "01234",
            "\n\n",
        ]
        labels = [
            "O",
            "O",
            "O",
            "B-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "I-STREET_ADDRESS",
            "O",
        ]

    doc = nlp(text)
    ws = [bool(x.whitespace_) for x in doc]

    assert tokens == [x.text for x in doc]
    assert len(labels) == len(tokens)

    example = dict(text=text, tokens=tokens, labels=labels, trailing_whitespace=ws)
    example.update(bridge(example))
    example.update(bridge.convert_external_to_internal(example))
    assert example["labels"] == example["internal_labels"]

    example.update(bridge.convert_internal_to_external(example))
    assert example["labels"] == example["external_labels"]
    assert (
        example["labels"] == labels
    ), f"Failed:\nExpected:\n{labels}\n\nActual:\n{example['labels']}"

    print("OK")
