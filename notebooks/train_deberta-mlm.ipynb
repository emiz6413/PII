{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8222b4f9-d909-4ed6-bcc3-e9c198c4146f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=PII\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=PII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd90573-2c22-4fc9-87c9-0ff92501c6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_RUN_GROUP=mlm-small-filter+M+P-gamma=0\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_RUN_GROUP=mlm-small-filter+M+P-gamma=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8266e2f-b765-44d0-bf2b-d5b777566a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U seqeval evaluate transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7acc420-c6da-4c9e-b3f7-e2cf7fd88101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spacy.lang.en import English\n",
    "from transformers.tokenization_utils import PreTrainedTokenizerBase\n",
    "from transformers.models.deberta_v2 import DebertaV2ForTokenClassification, DebertaV2TokenizerFast\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.trainer_utils import EvalPrediction, PredictionOutput\n",
    "from transformers.data.data_collator import DataCollatorForTokenClassification\n",
    "from sklearn.model_selection import KFold\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "from seqeval.metrics import recall_score, precision_score\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a6b54ae-7e21-460b-87e4-837dab004bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memiz6413\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=WANB_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa81d1-44e4-47f5-8b59-c69faa43b1e0",
   "metadata": {},
   "source": [
    "## Config & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b135e92b-8f18-481f-a181-8cbd3e960915",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../dataset/\")\n",
    "OUTPUT_DIR = \"output\"\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9088ae3-972e-456c-9c89-df6423941856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING_MODEL_PATH = \"hf-internal-testing/tiny-random-deberta-v2\"\n",
    "TOKENIZER_CHECKPOINT = \"microsoft/deberta-v3-small\"\n",
    "MODEL_CHECKPOINT_SUFFIX = \"emiz6413/deberta-v3-small_mlm_original-only_\"\n",
    "TRAINING_MAX_LENGTH = 1024 if \"tiny-random\" not in TOKENIZER_CHECKPOINT else 512\n",
    "EVAL_MAX_LENGTH = 3072 if \"tiny-random\" not in TOKENIZER_CHECKPOINT else 512\n",
    "CONF_THRESH = 0.9\n",
    "LR = 2e-5  # 1.5e-5 ~ 3e-5 for base # 5e-6 ~ 1e-5 for large\n",
    "LR_SCHEDULER_TYPE = \"cosine_with_restarts\"\n",
    "NUM_EPOCHS = 2 if \"tiny-random\" not in TOKENIZER_CHECKPOINT else 0.1\n",
    "BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = 16\n",
    "GRAD_ACCUMULATION_STEPS = 1\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "AMP = True\n",
    "FREEZE_EMBEDDING = False\n",
    "FREEZE_LAYERS = 0\n",
    "GAMMA = 0\n",
    "MASK_P = 0\n",
    "# training data\n",
    "N_SPLITS = 4\n",
    "FILTER_ORIGINAL = True\n",
    "EXTRA_1 = True\n",
    "EXTRA_2 = True\n",
    "EXTRA_3 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cf06306-7564-4833-8625-b9d961b1ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    fp16=AMP,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION_STEPS,\n",
    "    report_to=\"wandb\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=10,\n",
    "    metric_for_best_model=\"f5\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=False,\n",
    "    overwrite_output_dir=True,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2dd277-98f6-40b3-ab12-6bdc872eb140",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "069d4817-9ef6-49b4-b24b-428e1f14a7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "external_1 datapoints:  4434\n",
      "external_2 datapoints:  2000\n",
      "len(extra_data): {len(extra_data)}\n"
     ]
    }
   ],
   "source": [
    "with DATA_DIR.joinpath(\"train.json\").open(\"r\") as f:\n",
    "    original_data = json.load(f)\n",
    "\n",
    "extra_data = []  #\n",
    "\n",
    "# Moth\n",
    "if EXTRA_1:\n",
    "    with DATA_DIR.joinpath(\"pii_dataset_fixed.json\").open(\"r\") as f:\n",
    "        external_1 = json.load(f)\n",
    "    print(\"external_1 datapoints: \", len(external_1))\n",
    "    extra_data.extend(external_1)\n",
    "\n",
    "# PJMathmatician\n",
    "if EXTRA_2:\n",
    "    with DATA_DIR.joinpath(\"moredata_dataset_fixed.json\").open(\"r\") as f:\n",
    "        external_2 = json.load(f)\n",
    "    print(\"external_2 datapoints: \", len(external_2))\n",
    "    extra_data.extend(external_2)\n",
    "\n",
    "# Nicholas\n",
    "if EXTRA_3:\n",
    "    with DATA_DIR.joinpath(\"mixtral-8x7b-v1.json\").open(\"r\") as f:\n",
    "        external_3 = json.load(f)\n",
    "    print(\"external_3 datapoints: \", len(external_3))\n",
    "    extra_data.extend(external_3)\n",
    "\n",
    "print(\"len(extra_data): {len(extra_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44a0aa6c-023e-4746-a88f-5c25639313ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = [\n",
    "    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O'\n",
    "]\n",
    "id2label = {i: l for i, l in enumerate(all_labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "target = [l for l in all_labels if l != \"O\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a957d-ad6c-4dce-b79a-6091f1829067",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19c78c9f-dc7d-41ec-9b0f-52b68aa657aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerBase, label2id: dict, max_length: int) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, example: dict) -> dict:\n",
    "        # rebuild text from tokens\n",
    "        text, labels, token_map = [], [], []\n",
    "\n",
    "        for idx, (t, l, ws) in enumerate(\n",
    "            zip(example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"])\n",
    "        ):\n",
    "            text.append(t)\n",
    "            labels.extend([l] * len(t))\n",
    "            token_map.extend([idx]*len(t))\n",
    "\n",
    "            if ws:\n",
    "                text.append(\" \")\n",
    "                labels.append(\"O\")\n",
    "                token_map.append(-1)\n",
    "\n",
    "        text = \"\".join(text)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # actual tokenization\n",
    "        tokenized = self.tokenizer(\n",
    "            \"\".join(text),\n",
    "            return_offsets_mapping=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "        token_labels = []\n",
    "\n",
    "        for start_idx, end_idx in tokenized.offset_mapping:\n",
    "            # CLS token\n",
    "            if start_idx == 0 and end_idx == 0:\n",
    "                token_labels.append(self.label2id[\"O\"])\n",
    "                continue\n",
    "\n",
    "            # case when token starts with whitespace\n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "\n",
    "            token_labels.append(self.label2id[labels[start_idx]])\n",
    "\n",
    "        length = len(tokenized.input_ids)\n",
    "\n",
    "        return {**tokenized, \"labels\": token_labels, \"length\": length, \"token_map\": token_map}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa96105-cceb-4c58-8d8b-798c1d979f6b",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8458121-227b-461b-a695-920074d7224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCutOut:\n",
    "    def __init__(self, mask_p: float, mask_token_id: int) -> None:\n",
    "        self.mask_p = mask_p\n",
    "        self.mask_token_id = mask_token_id\n",
    "\n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        if self.mask_p == 0:\n",
    "            return batch\n",
    "        new_input_ids_list = []\n",
    "        for input_ids in batch[\"input_ids\"]:\n",
    "            mask = np.random.binomial(1, p=self.mask_p, size=(len(input_ids),))\n",
    "            new_input_ids = np.where(mask, self.mask_token_id, input_ids)\n",
    "            new_input_ids_list.append(new_input_ids.tolist())\n",
    "        batch[\"input_ids\"] = new_input_ids_list\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5ebe2-43d4-4a85-9cda-7210f0470aa2",
   "metadata": {},
   "source": [
    "## Instantiate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec0b7a12-ed50-4c0b-8d35-e2a026fad4d5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53386e9bcd3842b68bf7cebd0fa03bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9c9f8b6944477cba84c1794588106a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f143e3c703644b8a207a6de64e100bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DebertaV2TokenizerFast.from_pretrained(TOKENIZER_CHECKPOINT)\n",
    "train_encoder = CustomTokenizer(tokenizer=tokenizer, label2id=label2id, max_length=TRAINING_MAX_LENGTH)\n",
    "eval_encoder = CustomTokenizer(tokenizer=tokenizer, label2id=label2id, max_length=EVAL_MAX_LENGTH)\n",
    "train_augmentation = RandomCutOut(mask_p=MASK_P, mask_token_id=tokenizer.mask_token_id)\n",
    "\n",
    "ds = DatasetDict()\n",
    "\n",
    "for key, data in zip([\"original\", \"extra\"], [original_data, extra_data]):\n",
    "    ds[key] = Dataset.from_dict({\n",
    "        \"full_text\": [x[\"full_text\"] for x in data],\n",
    "        \"document\": [str(x[\"document\"]) for x in data],\n",
    "        \"tokens\": [x[\"tokens\"] for x in data],\n",
    "        \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "        \"provided_labels\": [x[\"labels\"] for x in data],\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4218f10a-128d-49a6-a594-ef19b5435df9",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15c7679b-dec3-4c41-8758-a5c5b1f576f1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MetricsComputer:\n",
    "    def __init__(self, all_labels: list[str], beta: float = 5.0) -> None:\n",
    "        self.all_labels = all_labels\n",
    "        self.beta = beta\n",
    "\n",
    "    def __call__(self, preds: EvalPrediction) -> dict[str, float]:\n",
    "        predictions, labels = preds\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "        # Remove ignored index (special tokens)\n",
    "        true_predictions = [\n",
    "            [self.all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [self.all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "\n",
    "        recall = recall_score(true_labels, true_predictions)\n",
    "        precision = precision_score(true_labels, true_predictions)\n",
    "        f5_score = (1 + self.beta ** 2) * recall * precision / ((self.beta ** 2) * precision + recall)\n",
    "\n",
    "        results = {\n",
    "            'recall': recall,\n",
    "            'precision': precision,\n",
    "            'f5': f5_score\n",
    "        }\n",
    "        return results\n",
    "\n",
    "# compute_metrics = MetricsComputer(all_labels=all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e470ae7a-fea0-48b5-ad4f-46be0ba5cd59",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "    idx = 0\n",
    "    spans = []\n",
    "    span = []\n",
    "\n",
    "    for i, token in enumerate(document):\n",
    "        if token != target[idx]:\n",
    "            idx = 0\n",
    "            span = []\n",
    "            continue\n",
    "        span.append(i)\n",
    "        idx += 1\n",
    "        if idx == len(target):\n",
    "            spans.append(span)\n",
    "            span = []\n",
    "            idx = 0\n",
    "            continue\n",
    "\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4fd14a5-1ca7-464b-9a6a-2274abc76761",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PRFScore:\n",
    "    \"\"\"A precision / recall / F score.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        tp: int = 0,\n",
    "        fp: int = 0,\n",
    "        fn: int = 0,\n",
    "    ) -> None:\n",
    "        self.tp = tp\n",
    "        self.fp = fp\n",
    "        self.fn = fn\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.tp + self.fp + self.fn\n",
    "\n",
    "    def __iadd__(self, other):  # in-place add\n",
    "        self.tp += other.tp\n",
    "        self.fp += other.fp\n",
    "        self.fn += other.fn\n",
    "        return self\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return PRFScore(\n",
    "            tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn\n",
    "        )\n",
    "\n",
    "    def score_set(self, cand: set, gold: set) -> None:\n",
    "        self.tp += len(cand.intersection(gold))\n",
    "        self.fp += len(cand - gold)\n",
    "        self.fn += len(gold - cand)\n",
    "\n",
    "    @property\n",
    "    def precision(self) -> float:\n",
    "        return self.tp / (self.tp + self.fp + 1e-100)\n",
    "\n",
    "    @property\n",
    "    def recall(self) -> float:\n",
    "        return self.tp / (self.tp + self.fn + 1e-100)\n",
    "\n",
    "    @property\n",
    "    def f1(self) -> float:\n",
    "        p = self.precision\n",
    "        r = self.recall\n",
    "        return 2 * ((p * r) / (p + r + 1e-100))\n",
    "\n",
    "    @property\n",
    "    def f5(self) -> float:\n",
    "        beta = 5\n",
    "        p = self.precision\n",
    "        r = self.recall\n",
    "\n",
    "        fbeta = (1+(beta**2))*p*r / ((beta**2)*p + r + 1e-100)\n",
    "        return fbeta\n",
    "\n",
    "    def to_dict(self) -> dict[str, float]:\n",
    "        return {\"p\": self.precision, \"r\": self.recall, \"f5\": self.f5}\n",
    "\n",
    "\n",
    "class MetricsComputerV2:\n",
    "    nlp = English()\n",
    "\n",
    "    def __init__(self, eval_ds: Dataset, label2id: dict, conf_thresh: float = 0.9) -> None:\n",
    "        self.ds = eval_ds.remove_columns(\"labels\").rename_columns({\"provided_labels\": \"labels\"})\n",
    "        self.gt_df = self.create_gt_df(self.ds)\n",
    "        self.label2id = label2id\n",
    "        self.confth = conf_thresh\n",
    "        self._search_gt()\n",
    "\n",
    "    def __call__(self, eval_preds: EvalPrediction) -> dict:\n",
    "        pred_df = self.create_pred_df(eval_preds.predictions)\n",
    "        return self.compute_metrics_from_df(self.gt_df, pred_df)\n",
    "\n",
    "    def _search_gt(self) -> None:\n",
    "        email_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
    "        phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n",
    "        self.emails = []\n",
    "        self.phone_nums = []\n",
    "\n",
    "        for _data in self.ds:\n",
    "            # email\n",
    "            for token_idx, token in enumerate(_data[\"tokens\"]):\n",
    "                if re.fullmatch(email_regex, token) is not None:\n",
    "                    self.emails.append(\n",
    "                        {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
    "                    )\n",
    "            # phone number\n",
    "            matches = phone_num_regex.findall(_data[\"full_text\"])\n",
    "            if not matches:\n",
    "                continue\n",
    "            for match in matches:\n",
    "                target = [t.text for t in self.nlp.tokenizer(match)]\n",
    "                matched_spans = find_span(target, _data[\"tokens\"])\n",
    "            for matched_span in matched_spans:\n",
    "                for intermediate, token_idx in enumerate(matched_span):\n",
    "                    prefix = \"I\" if intermediate else \"B\"\n",
    "                    self.phone_nums.append(\n",
    "                        {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": _data[\"tokens\"][token_idx]}\n",
    "                    )\n",
    "\n",
    "    @staticmethod\n",
    "    def create_gt_df(ds: Dataset):\n",
    "        gt = []\n",
    "        for row in ds:\n",
    "            for token_idx, (token, label) in enumerate(zip(row[\"tokens\"], row[\"labels\"])):\n",
    "                if label == \"O\":\n",
    "                    continue\n",
    "                gt.append(\n",
    "                    {\"document\": row[\"document\"], \"token\": token_idx, \"label\": label, \"token_str\": token}\n",
    "                )\n",
    "        gt_df = pd.DataFrame(gt)\n",
    "        gt_df[\"row_id\"] = gt_df.index\n",
    "\n",
    "        return gt_df\n",
    "\n",
    "    def create_pred_df(self, prediction: np.ndarray) -> pd.DataFrame:\n",
    "        ### construct prediction df\n",
    "        o_index = self.label2id[\"O\"]\n",
    "        preds = prediction.argmax(-1)\n",
    "        preds_without_o = prediction[:,:,:o_index].argmax(-1)\n",
    "        o_preds = prediction[:,:,o_index]\n",
    "        preds_final = np.where(o_preds < self.confth, preds_without_o , preds)\n",
    "\n",
    "        triplets = set()\n",
    "        processed = []\n",
    "\n",
    "        # Iterate over document\n",
    "        for p_doc, token_map, offsets, tokens, doc in zip(\n",
    "            preds_final, self.ds[\"token_map\"], self.ds[\"offset_mapping\"], self.ds[\"tokens\"], self.ds[\"document\"]\n",
    "        ):\n",
    "            # Iterate over sequence\n",
    "            for p_token, (start_idx, end_idx) in zip(p_doc, offsets):\n",
    "                label_pred = id2label[p_token]\n",
    "\n",
    "                if start_idx + end_idx == 0:\n",
    "                    # [CLS] token i.e. BOS\n",
    "                    continue\n",
    "\n",
    "                if token_map[start_idx] == -1:\n",
    "                    start_idx += 1\n",
    "\n",
    "                # ignore \"\\n\\n\"\n",
    "                while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "                    start_idx += 1\n",
    "\n",
    "                if start_idx >= len(token_map):\n",
    "                    break\n",
    "\n",
    "                token_id = token_map[start_idx]\n",
    "                triplet = (label_pred, token_id, tokens[token_id])\n",
    "\n",
    "                # ignore \"O\", preds, phone number and  email\n",
    "                if label_pred in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"B-PHONE_NUM\") or token_id == -1:\n",
    "                    continue\n",
    "\n",
    "                if triplet in triplets:\n",
    "                    continue\n",
    "\n",
    "                processed.append(\n",
    "                    {\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]}\n",
    "                )\n",
    "                triplets.add(triplet)\n",
    "\n",
    "        pred_df = pd.DataFrame(processed + self.emails + self.phone_nums)\n",
    "        pred_df[\"row_id\"] = list(range(len(pred_df)))\n",
    "\n",
    "        return pred_df\n",
    "\n",
    "    def compute_metrics_from_df(self, gt_df, pred_df):\n",
    "        \"\"\"\n",
    "        Compute the LB metric (lb) and other auxiliary metrics\n",
    "        \"\"\"\n",
    "\n",
    "        references = {(row.document, row.token, row.label) for row in gt_df.itertuples()}\n",
    "        predictions = {(row.document, row.token, row.label) for row in pred_df.itertuples()}\n",
    "\n",
    "        score_per_type = defaultdict(PRFScore)\n",
    "        references = set(references)\n",
    "\n",
    "        for ex in predictions:\n",
    "            pred_type = ex[-1] # (document, token, label)\n",
    "            if pred_type != 'O':\n",
    "                pred_type = pred_type[2:] # avoid B- and I- prefix\n",
    "\n",
    "            if pred_type not in score_per_type:\n",
    "                score_per_type[pred_type] = PRFScore()\n",
    "\n",
    "            if ex in references:\n",
    "                score_per_type[pred_type].tp += 1\n",
    "                references.remove(ex)\n",
    "            else:\n",
    "                score_per_type[pred_type].fp += 1\n",
    "\n",
    "        for doc, tok, ref_type in references:\n",
    "            if ref_type != 'O':\n",
    "                ref_type = ref_type[2:] # avoid B- and I- prefix\n",
    "\n",
    "            if ref_type not in score_per_type:\n",
    "                score_per_type[ref_type] = PRFScore()\n",
    "            score_per_type[ref_type].fn += 1\n",
    "\n",
    "        totals = PRFScore()\n",
    "\n",
    "        for prf in score_per_type.values():\n",
    "            totals += prf\n",
    "\n",
    "        return {\n",
    "            \"precision\": totals.precision,\n",
    "            \"recall\": totals.recall,\n",
    "            \"f5\": totals.f5,\n",
    "            **{\n",
    "                f\"{v_k}-{k}\": v_v\n",
    "                for k in set([l[2:] for l in self.label2id.keys() if l!= 'O'])\n",
    "                for v_k, v_v in score_per_type[k].to_dict().items()\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a7d8dc-6579-4678-a2ba-27d22e6aac00",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1700da69-ff6a-4fb6-9901-aaf93d589b29",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelInit:\n",
    "    def __init__(\n",
    "        self,\n",
    "        checkpoint: str,\n",
    "        id2label: dict,\n",
    "        label2id: dict,\n",
    "        freeze_embedding: bool,\n",
    "        freeze_layers: int,\n",
    "    ) -> None:\n",
    "        self.model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "            checkpoint,\n",
    "            num_labels=len(id2label),\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            ignore_mismatched_sizes=True,\n",
    "            use_safetensors=True,\n",
    "            token=\"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\"\n",
    "        )\n",
    "        for param in self.model.deberta.embeddings.parameters():\n",
    "            param.requires_grad = False if freeze_embedding else True\n",
    "        for layer in self.model.deberta.encoder.layer[:freeze_layers]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.weight = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "    def __call__(self) -> DebertaV2ForTokenClassification:\n",
    "        self.model.load_state_dict(self.weight)\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74884fb-fc4d-4684-a79c-fdfecb64198a",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7eaea9c-c63a-4a77-932f-22ff25360923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split according to document id\n",
    "folds = [\n",
    "    (\n",
    "        np.array([i for i, d in enumerate(ds[\"original\"][\"document\"]) if int(d) % N_SPLITS != s]),\n",
    "        np.array([i for i, d in enumerate(ds[\"original\"][\"document\"]) if int(d) % N_SPLITS == s])\n",
    "    )\n",
    "    for s in range(N_SPLITS)\n",
    "]\n",
    "\n",
    "exclude_indices = []\n",
    "if FILTER_ORIGINAL:\n",
    "    negative_idxs = [i for i, labels in enumerate(ds[\"original\"][\"provided_labels\"]) if not any(np.array(labels) != \"O\")]\n",
    "    exclude_indices = negative_idxs[len(negative_idxs)//3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12100606-72e2-4caa-bf10-495073e243c7",
   "metadata": {},
   "source": [
    "## Trainer with custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0970f634-639e-4158-8a6e-5c9cb0fb7098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self, weight = None, gamma = 2., reduction = \"mean\", ignore_index: int = -100\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss(weight=weight, reduction=\"none\", ignore_index=ignore_index)\n",
    "        self.ignore_index = ignore_index\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        logits = logits.permute(0, 2, 1)  # b, seq, c -> b, c, seq\n",
    "        mask = target != self.ignore_index\n",
    "        ce_loss = self.ce(logits, target)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        f_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            f_loss = torch.mean(torch.masked_select(f_loss, mask))\n",
    "        elif self.reduction == \"sum\":\n",
    "            f_loss = torch.sum(torch.masked_select(f_loss, mask))\n",
    "        else:\n",
    "            f_loss = torch.where(mask, f_loss, 0)\n",
    "\n",
    "        return f_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "270032b6-ba91-4de3-a151-8ef32a3300e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = FocalLoss(gamma=GAMMA)\n",
    "\n",
    "class FocalLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        _, outputs = super().compute_loss(model, inputs, True)\n",
    "        labels = inputs[\"labels\"]\n",
    "        loss = loss_fn(logits=outputs[\"logits\"], target=labels)\n",
    "        outputs[\"loss\"] = loss\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21887fb3-e3b1-497c-92ad-64def1bc9511",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec8b773-e2e8-454a-b21d-b9e5cdf9eb09",
   "metadata": {},
   "source": [
    "#### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3d40f-e75d-4b05-85b0-7e0d620ce36a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fac9e3ba8044b38a6c86d3b0b43a480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/866 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9acd71e0244974aedb40089a71180e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/568M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at emiz6413/deberta-v3-small_mlm_original-only_fold-1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfde6e7f3cd64180a592f81343ba37cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1076 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d38d32801f419dbe11f105074e96ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1075 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b692657033a4d50ade161d443c4eb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1075 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050b10d6144e41c588ea6a74c6b85dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1075 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c44123b32749bbadd8c2dc844966fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/1075 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c165b52ac9134809a0b634f53f0355fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/1075 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70aea6012f1448f187a4f0ed5d1bec51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/1075 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7463403dc9240b686d2aa01128e4da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/1075 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210c594d92f74b55a658f8c292bbdea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/215 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023f74844edc47ecb5de89edc040e156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/215 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05fefa61484412496edb13a2955150d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79ef904027a438c8b43687422162440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79adbd7399314cda8e4ceb4df76079ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c814720598a4132a5224e7d0285a824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794ce369de0948349a4a92f5a29547ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c96e4d120843c7b96b5ec975b082e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/notebooks/wandb/run-20240220_081826-gnh14x8l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/emiz6413/PII/runs/gnh14x8l\" target=\"_blank\">fold-1</a></strong> to <a href=\"https://wandb.ai/emiz6413/PII\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='807' max='807' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [807/807 26:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F5</th>\n",
       "      <th>P-email</th>\n",
       "      <th>R-email</th>\n",
       "      <th>F5-email</th>\n",
       "      <th>P-username</th>\n",
       "      <th>R-username</th>\n",
       "      <th>F5-username</th>\n",
       "      <th>P-street Address</th>\n",
       "      <th>R-street Address</th>\n",
       "      <th>F5-street Address</th>\n",
       "      <th>P-url Personal</th>\n",
       "      <th>R-url Personal</th>\n",
       "      <th>F5-url Personal</th>\n",
       "      <th>P-name Student</th>\n",
       "      <th>R-name Student</th>\n",
       "      <th>F5-name Student</th>\n",
       "      <th>P-phone Num</th>\n",
       "      <th>R-phone Num</th>\n",
       "      <th>F5-phone Num</th>\n",
       "      <th>P-id Num</th>\n",
       "      <th>R-id Num</th>\n",
       "      <th>F5-id Num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>0.025557</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>0.031389</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>0.008823</td>\n",
       "      <td>0.341253</td>\n",
       "      <td>0.651099</td>\n",
       "      <td>0.629129</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.605993</td>\n",
       "      <td>0.354733</td>\n",
       "      <td>0.667183</td>\n",
       "      <td>0.645321</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.004456</td>\n",
       "      <td>0.534423</td>\n",
       "      <td>0.853022</td>\n",
       "      <td>0.833901</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266129</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.904110</td>\n",
       "      <td>0.565482</td>\n",
       "      <td>0.862229</td>\n",
       "      <td>0.845171</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990476</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.341108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>0.580734</td>\n",
       "      <td>0.869505</td>\n",
       "      <td>0.853188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.867631</td>\n",
       "      <td>0.595767</td>\n",
       "      <td>0.871517</td>\n",
       "      <td>0.856274</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990476</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.665718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>0.660638</td>\n",
       "      <td>0.853022</td>\n",
       "      <td>0.843574</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.671856</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.858758</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990476</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.304539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>0.666317</td>\n",
       "      <td>0.872253</td>\n",
       "      <td>0.862006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.888383</td>\n",
       "      <td>0.671819</td>\n",
       "      <td>0.874613</td>\n",
       "      <td>0.864575</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990476</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.669528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.581786</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.885003</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959732</td>\n",
       "      <td>0.588057</td>\n",
       "      <td>0.899381</td>\n",
       "      <td>0.881433</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990476</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.804501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.002510</td>\n",
       "      <td>0.611578</td>\n",
       "      <td>0.899725</td>\n",
       "      <td>0.883711</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.619251</td>\n",
       "      <td>0.896285</td>\n",
       "      <td>0.881124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.876404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>0.656344</td>\n",
       "      <td>0.902473</td>\n",
       "      <td>0.889641</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958659</td>\n",
       "      <td>0.675613</td>\n",
       "      <td>0.896285</td>\n",
       "      <td>0.885165</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990476</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.839888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.002685</td>\n",
       "      <td>0.600182</td>\n",
       "      <td>0.905220</td>\n",
       "      <td>0.887864</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.561404</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.943311</td>\n",
       "      <td>0.601442</td>\n",
       "      <td>0.904025</td>\n",
       "      <td>0.886864</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990476</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.769014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>0.607735</td>\n",
       "      <td>0.906593</td>\n",
       "      <td>0.889765</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965129</td>\n",
       "      <td>0.613445</td>\n",
       "      <td>0.904025</td>\n",
       "      <td>0.887849</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990476</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.767932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.657974</td>\n",
       "      <td>0.901099</td>\n",
       "      <td>0.888472</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.660249</td>\n",
       "      <td>0.902477</td>\n",
       "      <td>0.889920</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990476</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.771186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.002322</td>\n",
       "      <td>0.660302</td>\n",
       "      <td>0.902473</td>\n",
       "      <td>0.889919</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.942242</td>\n",
       "      <td>0.670507</td>\n",
       "      <td>0.900929</td>\n",
       "      <td>0.889176</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990476</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.766854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>0.665319</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.891552</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.940113</td>\n",
       "      <td>0.676744</td>\n",
       "      <td>0.900929</td>\n",
       "      <td>0.889594</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990476</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.803371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>0.671764</td>\n",
       "      <td>0.905220</td>\n",
       "      <td>0.893280</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.682298</td>\n",
       "      <td>0.900929</td>\n",
       "      <td>0.889961</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990476</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.839888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>0.002350</td>\n",
       "      <td>0.670397</td>\n",
       "      <td>0.905220</td>\n",
       "      <td>0.893187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.680702</td>\n",
       "      <td>0.900929</td>\n",
       "      <td>0.889856</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990476</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.839888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [108/108 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f5</td><td></td></tr><tr><td>eval/f5-EMAIL</td><td></td></tr><tr><td>eval/f5-ID_NUM</td><td></td></tr><tr><td>eval/f5-NAME_STUDENT</td><td></td></tr><tr><td>eval/f5-PHONE_NUM</td><td></td></tr><tr><td>eval/f5-STREET_ADDRESS</td><td></td></tr><tr><td>eval/f5-URL_PERSONAL</td><td></td></tr><tr><td>eval/f5-USERNAME</td><td></td></tr><tr><td>eval/loss</td><td></td></tr><tr><td>eval/p-EMAIL</td><td></td></tr><tr><td>eval/p-ID_NUM</td><td></td></tr><tr><td>eval/p-NAME_STUDENT</td><td></td></tr><tr><td>eval/p-PHONE_NUM</td><td></td></tr><tr><td>eval/p-STREET_ADDRESS</td><td></td></tr><tr><td>eval/p-URL_PERSONAL</td><td></td></tr><tr><td>eval/p-USERNAME</td><td></td></tr><tr><td>eval/precision</td><td></td></tr><tr><td>eval/r-EMAIL</td><td></td></tr><tr><td>eval/r-ID_NUM</td><td></td></tr><tr><td>eval/r-NAME_STUDENT</td><td></td></tr><tr><td>eval/r-PHONE_NUM</td><td></td></tr><tr><td>eval/r-STREET_ADDRESS</td><td></td></tr><tr><td>eval/r-URL_PERSONAL</td><td></td></tr><tr><td>eval/r-USERNAME</td><td></td></tr><tr><td>eval/recall</td><td></td></tr><tr><td>eval/runtime</td><td></td></tr><tr><td>eval/samples_per_second</td><td></td></tr><tr><td>eval/steps_per_second</td><td></td></tr><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/total_flos</td><td></td></tr><tr><td>train/train_loss</td><td></td></tr><tr><td>train/train_runtime</td><td></td></tr><tr><td>train/train_samples_per_second</td><td></td></tr><tr><td>train/train_steps_per_second</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f5</td><td>0.89319</td></tr><tr><td>eval/f5-EMAIL</td><td>1.0</td></tr><tr><td>eval/f5-ID_NUM</td><td>0.83989</td></tr><tr><td>eval/f5-NAME_STUDENT</td><td>0.88986</td></tr><tr><td>eval/f5-PHONE_NUM</td><td>0.99048</td></tr><tr><td>eval/f5-STREET_ADDRESS</td><td>0.0</td></tr><tr><td>eval/f5-URL_PERSONAL</td><td>0.94118</td></tr><tr><td>eval/f5-USERNAME</td><td>0.0</td></tr><tr><td>eval/loss</td><td>0.00235</td></tr><tr><td>eval/p-EMAIL</td><td>1.0</td></tr><tr><td>eval/p-ID_NUM</td><td>0.62162</td></tr><tr><td>eval/p-NAME_STUDENT</td><td>0.6807</td></tr><tr><td>eval/p-PHONE_NUM</td><td>0.8</td></tr><tr><td>eval/p-STREET_ADDRESS</td><td>0.0</td></tr><tr><td>eval/p-URL_PERSONAL</td><td>0.54237</td></tr><tr><td>eval/p-USERNAME</td><td>0.0</td></tr><tr><td>eval/precision</td><td>0.6704</td></tr><tr><td>eval/r-EMAIL</td><td>1.0</td></tr><tr><td>eval/r-ID_NUM</td><td>0.85185</td></tr><tr><td>eval/r-NAME_STUDENT</td><td>0.90093</td></tr><tr><td>eval/r-PHONE_NUM</td><td>1.0</td></tr><tr><td>eval/r-STREET_ADDRESS</td><td>0.0</td></tr><tr><td>eval/r-URL_PERSONAL</td><td>0.9697</td></tr><tr><td>eval/r-USERNAME</td><td>0.0</td></tr><tr><td>eval/recall</td><td>0.90522</td></tr><tr><td>eval/runtime</td><td>45.6039</td></tr><tr><td>eval/samples_per_second</td><td>37.585</td></tr><tr><td>eval/steps_per_second</td><td>2.368</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>807</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0079</td></tr><tr><td>train/total_flos</td><td>6470808588334080.0</td></tr><tr><td>train/train_loss</td><td>0.12452</td></tr><tr><td>train/train_runtime</td><td>1621.1458</td></tr><tr><td>train/train_samples_per_second</td><td>15.917</td></tr><tr><td>train/train_steps_per_second</td><td>0.498</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fold-1</strong>: <a href=\"https://wandb.ai/emiz6413/PII/runs/gnh14x8l\" target=\"_blank\">https://wandb.ai/emiz6413/PII/runs/gnh14x8l</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240220_081826-gnh14x8l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0ee88c677f495db87e940d2515ff2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/866 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b029949b35624ce1934092af19595903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/568M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at emiz6413/deberta-v3-small_mlm_original-only_fold-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6aac6b84dc4d989bdcec704d788606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1078 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90296a6e1ff84d1ab5b959028bdb791c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1078 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266dffca66c1453b93af2291f52811ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1078 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ee0cbebcc94db3aff0cca32d6c592f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1078 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430075b0aab44e24933fe1e1cdfbed75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/1078 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0eb641105e4296a7cbbd4a063c69d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/1078 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ac15d807024d9897c3865b46d9dc89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/1078 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2dcaab960dc4931b180974d13740f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/1078 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5e1f3c62c6439797682c0094c793c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/212 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ffbbe45d3ab470aa6f988c7ee13a8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/211 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413e56bea0ef4820b773f93675016f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/211 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989a82b84c4c428d92ae012465df95dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/211 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a489de05c06040abb81e2e4289901bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/211 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4bf3e962e046e4af3aa2f2ccd13446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/211 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7b94202f874529af21499b60ef1839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/211 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886af2fd2ab3479ca41f8221f349a886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/211 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/notebooks/wandb/run-20240220_084732-24cbbzwi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/emiz6413/PII/runs/24cbbzwi\" target=\"_blank\">fold-2</a></strong> to <a href=\"https://wandb.ai/emiz6413/PII\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='810' max='810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [810/810 25:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F5</th>\n",
       "      <th>P-email</th>\n",
       "      <th>R-email</th>\n",
       "      <th>F5-email</th>\n",
       "      <th>P-username</th>\n",
       "      <th>R-username</th>\n",
       "      <th>F5-username</th>\n",
       "      <th>P-street Address</th>\n",
       "      <th>R-street Address</th>\n",
       "      <th>F5-street Address</th>\n",
       "      <th>P-url Personal</th>\n",
       "      <th>R-url Personal</th>\n",
       "      <th>F5-url Personal</th>\n",
       "      <th>P-name Student</th>\n",
       "      <th>R-name Student</th>\n",
       "      <th>F5-name Student</th>\n",
       "      <th>P-phone Num</th>\n",
       "      <th>R-phone Num</th>\n",
       "      <th>F5-phone Num</th>\n",
       "      <th>P-id Num</th>\n",
       "      <th>R-id Num</th>\n",
       "      <th>F5-id Num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.235200</td>\n",
       "      <td>0.025898</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.012676</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>0.007646</td>\n",
       "      <td>0.499353</td>\n",
       "      <td>0.523035</td>\n",
       "      <td>0.522083</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194030</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.547812</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.685112</td>\n",
       "      <td>0.540881</td>\n",
       "      <td>0.520424</td>\n",
       "      <td>0.521182</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.632231</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.819446</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.683012</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.798398</td>\n",
       "      <td>0.633448</td>\n",
       "      <td>0.836611</td>\n",
       "      <td>0.826417</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.651811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0.521170</td>\n",
       "      <td>0.917344</td>\n",
       "      <td>0.891286</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.683012</td>\n",
       "      <td>0.356164</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.874515</td>\n",
       "      <td>0.521814</td>\n",
       "      <td>0.922844</td>\n",
       "      <td>0.896349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>0.577257</td>\n",
       "      <td>0.901084</td>\n",
       "      <td>0.882053</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954128</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.685413</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.802685</td>\n",
       "      <td>0.569668</td>\n",
       "      <td>0.909228</td>\n",
       "      <td>0.888851</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.002727</td>\n",
       "      <td>0.599628</td>\n",
       "      <td>0.872629</td>\n",
       "      <td>0.857611</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.686620</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.583085</td>\n",
       "      <td>0.886536</td>\n",
       "      <td>0.869139</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.859504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.002569</td>\n",
       "      <td>0.607209</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.874565</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.485981</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.703654</td>\n",
       "      <td>0.601005</td>\n",
       "      <td>0.904690</td>\n",
       "      <td>0.887443</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.860434</td>\n",
       "      <td>0.853715</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945455</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.686620</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.920052</td>\n",
       "      <td>0.733161</td>\n",
       "      <td>0.856278</td>\n",
       "      <td>0.850783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.620148</td>\n",
       "      <td>0.909214</td>\n",
       "      <td>0.893201</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945455</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.685413</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.927345</td>\n",
       "      <td>0.621259</td>\n",
       "      <td>0.910741</td>\n",
       "      <td>0.894707</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>0.633776</td>\n",
       "      <td>0.905149</td>\n",
       "      <td>0.890484</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954128</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.686620</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.941019</td>\n",
       "      <td>0.627883</td>\n",
       "      <td>0.906203</td>\n",
       "      <td>0.891012</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>0.673096</td>\n",
       "      <td>0.898374</td>\n",
       "      <td>0.886957</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954128</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.937250</td>\n",
       "      <td>0.671186</td>\n",
       "      <td>0.898638</td>\n",
       "      <td>0.887076</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.755054</td>\n",
       "      <td>0.860434</td>\n",
       "      <td>0.855840</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954128</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.902537</td>\n",
       "      <td>0.765182</td>\n",
       "      <td>0.857791</td>\n",
       "      <td>0.853817</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.894309</td>\n",
       "      <td>0.886226</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954128</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.731436</td>\n",
       "      <td>0.894100</td>\n",
       "      <td>0.886517</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.712269</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.880491</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954128</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.711031</td>\n",
       "      <td>0.897126</td>\n",
       "      <td>0.888185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.894309</td>\n",
       "      <td>0.884718</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954128</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.802685</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.900151</td>\n",
       "      <td>0.890360</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.694241</td>\n",
       "      <td>0.898374</td>\n",
       "      <td>0.888328</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954128</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.868984</td>\n",
       "      <td>0.696262</td>\n",
       "      <td>0.901664</td>\n",
       "      <td>0.891548</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='106' max='106' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [106/106 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f5</td><td></td></tr><tr><td>eval/f5-EMAIL</td><td></td></tr><tr><td>eval/f5-ID_NUM</td><td></td></tr><tr><td>eval/f5-NAME_STUDENT</td><td></td></tr><tr><td>eval/f5-PHONE_NUM</td><td></td></tr><tr><td>eval/f5-STREET_ADDRESS</td><td></td></tr><tr><td>eval/f5-URL_PERSONAL</td><td></td></tr><tr><td>eval/f5-USERNAME</td><td></td></tr><tr><td>eval/loss</td><td></td></tr><tr><td>eval/p-EMAIL</td><td></td></tr><tr><td>eval/p-ID_NUM</td><td></td></tr><tr><td>eval/p-NAME_STUDENT</td><td></td></tr><tr><td>eval/p-PHONE_NUM</td><td></td></tr><tr><td>eval/p-STREET_ADDRESS</td><td></td></tr><tr><td>eval/p-URL_PERSONAL</td><td></td></tr><tr><td>eval/p-USERNAME</td><td></td></tr><tr><td>eval/precision</td><td></td></tr><tr><td>eval/r-EMAIL</td><td></td></tr><tr><td>eval/r-ID_NUM</td><td></td></tr><tr><td>eval/r-NAME_STUDENT</td><td></td></tr><tr><td>eval/r-PHONE_NUM</td><td></td></tr><tr><td>eval/r-STREET_ADDRESS</td><td></td></tr><tr><td>eval/r-URL_PERSONAL</td><td></td></tr><tr><td>eval/r-USERNAME</td><td></td></tr><tr><td>eval/recall</td><td></td></tr><tr><td>eval/runtime</td><td></td></tr><tr><td>eval/samples_per_second</td><td></td></tr><tr><td>eval/steps_per_second</td><td></td></tr><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/total_flos</td><td></td></tr><tr><td>train/train_loss</td><td></td></tr><tr><td>train/train_runtime</td><td></td></tr><tr><td>train/train_samples_per_second</td><td></td></tr><tr><td>train/train_steps_per_second</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f5</td><td>0.88833</td></tr><tr><td>eval/f5-EMAIL</td><td>0.99522</td></tr><tr><td>eval/f5-ID_NUM</td><td>0.99726</td></tr><tr><td>eval/f5-NAME_STUDENT</td><td>0.89155</td></tr><tr><td>eval/f5-PHONE_NUM</td><td>1.0</td></tr><tr><td>eval/f5-STREET_ADDRESS</td><td>0.68783</td></tr><tr><td>eval/f5-URL_PERSONAL</td><td>0.86898</td></tr><tr><td>eval/f5-USERNAME</td><td>0.95413</td></tr><tr><td>eval/loss</td><td>0.00175</td></tr><tr><td>eval/p-EMAIL</td><td>0.88889</td></tr><tr><td>eval/p-ID_NUM</td><td>0.93333</td></tr><tr><td>eval/p-NAME_STUDENT</td><td>0.69626</td></tr><tr><td>eval/p-PHONE_NUM</td><td>1.0</td></tr><tr><td>eval/p-STREET_ADDRESS</td><td>0.88235</td></tr><tr><td>eval/p-URL_PERSONAL</td><td>0.52083</td></tr><tr><td>eval/p-USERNAME</td><td>0.44444</td></tr><tr><td>eval/precision</td><td>0.69424</td></tr><tr><td>eval/r-EMAIL</td><td>1.0</td></tr><tr><td>eval/r-ID_NUM</td><td>1.0</td></tr><tr><td>eval/r-NAME_STUDENT</td><td>0.90166</td></tr><tr><td>eval/r-PHONE_NUM</td><td>1.0</td></tr><tr><td>eval/r-STREET_ADDRESS</td><td>0.68182</td></tr><tr><td>eval/r-URL_PERSONAL</td><td>0.89286</td></tr><tr><td>eval/r-USERNAME</td><td>1.0</td></tr><tr><td>eval/recall</td><td>0.89837</td></tr><tr><td>eval/runtime</td><td>40.831</td></tr><tr><td>eval/samples_per_second</td><td>41.366</td></tr><tr><td>eval/steps_per_second</td><td>2.596</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>810</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0053</td></tr><tr><td>train/total_flos</td><td>6522494033081856.0</td></tr><tr><td>train/train_loss</td><td>0.12695</td></tr><tr><td>train/train_runtime</td><td>1557.3774</td></tr><tr><td>train/train_samples_per_second</td><td>16.613</td></tr><tr><td>train/train_steps_per_second</td><td>0.52</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fold-2</strong>: <a href=\"https://wandb.ai/emiz6413/PII/runs/24cbbzwi\" target=\"_blank\">https://wandb.ai/emiz6413/PII/runs/24cbbzwi</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240220_084732-24cbbzwi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7455b2512db43fd9fcebb5ef9c1ed85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/866 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f805d6cc85e5486d8d89669fe7092aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/568M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at emiz6413/deberta-v3-small_mlm_original-only_fold-3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b69fbc29dc495091125e70562256dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1077 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4225c9304244afb2c17b75826247fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1076 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755a78182033421299e011fb790c957c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1076 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc45ab6568524335add0fafb9e34324c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1076 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678a0e0f9dac436cac7302cf85afd1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/1076 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ed573f265346369cbb903bf38f85d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/1076 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e7b15388894fcc990525c075432b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/1076 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c775a0da95314c5c9ea5914caddbbec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/1076 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a7e3d4296b49b5a518b7141146d999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf63409b1854dc0b224f234b27c26e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6de88932a74d97bc299209d81a299a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/213 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a5d53ab5764986ae01901221cce238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/213 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d154de4bab478487f28ee579aaa657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/213 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6146d163dd8b4f6c9de7aebbe6f954ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/213 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d69a0f189b48b8865df84048d2685b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/213 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d5f7aa4c924c03b9f8d30d0566b365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/213 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for fold_idx, (train_idx, eval_idx) in enumerate(folds):\n",
    "    args.run_name = f\"fold-{fold_idx}\"\n",
    "    args.output_dir = os.path.join(OUTPUT_DIR, f\"fold_{fold_idx}\")\n",
    "    if Path(args.output_dir).joinpath(\"eval_result.json\").exists():\n",
    "        continue\n",
    "    model_init = ModelInit(\n",
    "        MODEL_CHECKPOINT_SUFFIX + f\"fold-{fold_idx}\",\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        freeze_embedding=FREEZE_EMBEDDING,\n",
    "        freeze_layers=FREEZE_LAYERS,\n",
    "    )\n",
    "    original_ds = ds[\"original\"].select([i for i in train_idx if i not in exclude_indices])\n",
    "    train_ds = concatenate_datasets([original_ds, ds[\"extra\"]])\n",
    "    train_ds = train_ds.map(train_encoder, num_proc=os.cpu_count())\n",
    "    train_ds.set_transform(train_augmentation)\n",
    "    eval_ds = ds[\"original\"].select(eval_idx)\n",
    "    eval_ds = eval_ds.map(eval_encoder, num_proc=os.cpu_count())\n",
    "    trainer = FocalLossTrainer(\n",
    "        args=args,\n",
    "        model_init=model_init,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=MetricsComputerV2(eval_ds=eval_ds, label2id=label2id),\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16),\n",
    "    )\n",
    "    trainer.train()\n",
    "    eval_res = trainer.evaluate(eval_dataset=eval_ds)\n",
    "    with open(os.path.join(args.output_dir, \"eval_result.json\"), \"w\") as f:\n",
    "        json.dump(eval_res, f)\n",
    "    del trainer, model_init\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af79fbd-6091-43d8-8db6-a8c24fe94a3b",
   "metadata": {},
   "source": [
    "#### Train with full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd36d3-579a-4965-982b-00c2e1887c50",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_ds = ds[\"original\"].select([i for i in range(len(ds[\"original\"])) if i not in exclude_indices])\n",
    "train_ds = concatenate_datasets([original_ds, ds[\"extra\"]])\n",
    "train_ds = train_ds.map(train_encoder, num_proc=os.cpu_count())\n",
    "train_ds.set_transform(train_augmentation)\n",
    "args.evaluation_strategy = \"no\"\n",
    "args.save_strategy = \"no\"\n",
    "args.run_name = f\"all_data\"\n",
    "model_init = ModelInit(\n",
    "        MODEL_CHECKPOINT_SUFFIX + \"all\",\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        freeze_embedding=FREEZE_EMBEDDING,\n",
    "        freeze_layers=FREEZE_LAYERS,\n",
    "    )\n",
    "trainer = FocalLossTrainer(\n",
    "    args=args,\n",
    "    model_init=model_init,\n",
    "    train_dataset=train_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16),\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(os.path.join(OUTPUT_DIR, \"all\"))\n",
    "\n",
    "# log cv result\n",
    "results = dict()\n",
    "for res_json_path in Path(OUTPUT_DIR).glob(\"fold*/eval_result.json\"):\n",
    "    fold = res_json_path.parent.name.split(\"_\")[-1]\n",
    "    with open(res_json_path, \"r\") as f:\n",
    "        res = json.load(f)\n",
    "        results[fold] = {k.replace(\"eval_\", \"\"): v for k, v in res.items()}\n",
    "results[\"cv\"] = {key: np.mean([r[key] for r in results.values()]) for key in results[\"0\"].keys()}\n",
    "table = wandb.Table(columns=[\"fold\"] + list(results[\"0\"].keys()))\n",
    "for f, res in results.items():\n",
    "    table.add_data(f, *[res[c] for c in table.columns if c != \"fold\"])\n",
    "wandb.log({\"eval_result\": table})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0159a3d2-c25b-4d54-b017-64993c6f1c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
