{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8222b4f9-d909-4ed6-bcc3-e9c198c4146f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=PII\n",
      "env: WANDB_RUN_GROUP=large-3072-lwise-rectified-filter+MP-CE\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=PII\n",
    "%env WANDB_RUN_GROUP=large-3072-lwise-rectified-filter+MP-CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7acc420-c6da-4c9e-b3f7-e2cf7fd88101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spacy.lang.en import English\n",
    "from transformers.tokenization_utils import PreTrainedTokenizerBase\n",
    "from transformers.models.deberta_v2 import DebertaV2ForTokenClassification, DebertaV2TokenizerFast\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from transformers.data.data_collator import DataCollatorForTokenClassification\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "201c1684-de81-4b76-a1a8-832e7d488fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa81d1-44e4-47f5-8b59-c69faa43b1e0",
   "metadata": {},
   "source": [
    "## Config & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b135e92b-8f18-481f-a181-8cbd3e960915",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../datasets/\")\n",
    "OUTPUT_DIR = \"output\"\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9088ae3-972e-456c-9c89-df6423941856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING_MODEL_PATH = \"hf-internal-testing/tiny-random-deberta-v2\"\n",
    "TRAINING_MODEL_PATH = \"microsoft/deberta-v3-large\"\n",
    "# TRAINING_MODEL_PATH = \"Gladiator/microsoft-deberta-v3-large_ner_conll2003\"\n",
    "TRAINING_MAX_LENGTH = 3072 if \"tiny-random\" not in TRAINING_MODEL_PATH else 512\n",
    "EVAL_MAX_LENGTH = 3072 if \"tiny-random\" not in TRAINING_MODEL_PATH else 512\n",
    "CONF_THRESH = 0.9\n",
    "MAX_LR = 2.5e-5  # 1.5e-5 ~ 3e-5 for base # 5e-6 ~ 1e-5 for large\n",
    "LR_SCHEDULER_TYPE = \"linear\"\n",
    "NUM_EPOCHS = 3 if \"tiny-random\" not in TRAINING_MODEL_PATH else 0.1\n",
    "BATCH_SIZE = 1\n",
    "EVAL_BATCH_SIZE = 8\n",
    "GRAD_ACCUMULATION_STEPS = 16 // BATCH_SIZE\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "AMP = True\n",
    "FREEZE_EMBEDDING = False\n",
    "FREEZE_LAYERS = 6\n",
    "# training data\n",
    "N_SPLITS = 4\n",
    "NEGATIVE_RATIO = 0.3\n",
    "MOTH = False\n",
    "PJMATHMATICIAN = False\n",
    "NICHOLAS = False\n",
    "MPWARE = True\n",
    "TONYAROBERTSON = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cf06306-7564-4833-8625-b9d961b1ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    fp16=AMP,\n",
    "    learning_rate=MAX_LR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION_STEPS,\n",
    "    report_to=\"wandb\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=10,\n",
    "    metric_for_best_model=\"f5\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    overwrite_output_dir=True,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2dd277-98f6-40b3-ab12-6bdc872eb140",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "069d4817-9ef6-49b4-b24b-428e1f14a7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPWARE's datapoints:  2692\n",
      "len(extra_data): 2692\n"
     ]
    }
   ],
   "source": [
    "with DATA_DIR.joinpath(\"train.json\").open(\"r\") as f:\n",
    "    original_data = json.load(f)\n",
    "\n",
    "extra_data = []  #\n",
    "\n",
    "if MOTH:\n",
    "    with DATA_DIR.joinpath(\"pii_dataset_fixed.json\").open(\"r\") as f:\n",
    "        external = json.load(f)\n",
    "    print(\"Moth's datapoints: \", len(external))\n",
    "    extra_data.extend(external)\n",
    "\n",
    "if PJMATHMATICIAN:\n",
    "    with DATA_DIR.joinpath(\"moredata_dataset_fixed.json\").open(\"r\") as f:\n",
    "        external = json.load(f)\n",
    "    print(\"PJMathmatician's datapoints: \", len(external))\n",
    "    extra_data.extend(external)\n",
    "\n",
    "if NICHOLAS:\n",
    "    with DATA_DIR.joinpath(\"mixtral-8x7b-v1.json\").open(\"r\") as f:\n",
    "        external = json.load(f)\n",
    "    print(\"Nicholas' datapoints: \", len(external))\n",
    "    extra_data.extend(external)\n",
    "\n",
    "if MPWARE:\n",
    "    with DATA_DIR.joinpath(\"mpware_mixtral8x7b_v1.1-no-i-username.json\").open(\"r\") as f:\n",
    "        external = json.load(f)\n",
    "    print(\"MPWARE's datapoints: \", len(external))\n",
    "    extra_data.extend(external)\n",
    "\n",
    "if TONYAROBERTSON:\n",
    "    with DATA_DIR.joinpath(\"Fake_data_1850_218.json\").open(\"r\") as f:\n",
    "        external = json.load(f)\n",
    "    print(\"tonyarobertson's datapoints: \", len(external))\n",
    "    extra_data.extend(external)\n",
    "\n",
    "print(f\"len(extra_data): {len(extra_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44a0aa6c-023e-4746-a88f-5c25639313ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = [\n",
    "    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O'\n",
    "]\n",
    "id2label = {i: l for i, l in enumerate(all_labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "target = [l for l in all_labels if l != \"O\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a957d-ad6c-4dce-b79a-6091f1829067",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19c78c9f-dc7d-41ec-9b0f-52b68aa657aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerBase, label2id: dict, max_length: int) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, example: dict) -> dict:\n",
    "        # rebuild text from tokens\n",
    "        text, labels, token_map = [], [], []\n",
    "\n",
    "        for idx, (t, l, ws) in enumerate(\n",
    "            zip(example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"])\n",
    "        ):\n",
    "            text.append(t)\n",
    "            labels.extend([l] * len(t))\n",
    "            token_map.extend([idx]*len(t))\n",
    "\n",
    "            if ws:\n",
    "                text.append(\" \")\n",
    "                labels.append(\"O\")\n",
    "                token_map.append(-1)\n",
    "\n",
    "        text = \"\".join(text)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # actual tokenization\n",
    "        tokenized = self.tokenizer(\n",
    "            \"\".join(text),\n",
    "            return_offsets_mapping=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "        token_labels = []\n",
    "\n",
    "        for start_idx, end_idx in tokenized.offset_mapping:\n",
    "            # CLS token\n",
    "            if start_idx == 0 and end_idx == 0:\n",
    "                token_labels.append(self.label2id[\"O\"])\n",
    "                continue\n",
    "\n",
    "            # case when token starts with whitespace\n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "\n",
    "            token_labels.append(self.label2id[labels[start_idx]])\n",
    "\n",
    "        length = len(tokenized.input_ids)\n",
    "\n",
    "        return {**tokenized, \"labels\": token_labels, \"length\": length, \"token_map\": token_map}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5ebe2-43d4-4a85-9cda-7210f0470aa2",
   "metadata": {},
   "source": [
    "## Instantiate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec0b7a12-ed50-4c0b-8d35-e2a026fad4d5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177dc65424194b1190eedb896835ea0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123d7e3a80054a7389cbf5b28084eb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spm.model:   0%|          | 0.00/2.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6228eb0c261e412e91f6ad895e400b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DebertaV2TokenizerFast.from_pretrained(TRAINING_MODEL_PATH)\n",
    "train_encoder = CustomTokenizer(tokenizer=tokenizer, label2id=label2id, max_length=TRAINING_MAX_LENGTH)\n",
    "eval_encoder = CustomTokenizer(tokenizer=tokenizer, label2id=label2id, max_length=EVAL_MAX_LENGTH)\n",
    "\n",
    "ds = DatasetDict()\n",
    "\n",
    "for key, data in zip([\"original\", \"extra\"], [original_data, extra_data]):\n",
    "    ds[key] = Dataset.from_dict({\n",
    "        \"full_text\": [x[\"full_text\"] for x in data],\n",
    "        \"document\": [str(x[\"document\"]) for x in data],\n",
    "        \"tokens\": [x[\"tokens\"] for x in data],\n",
    "        \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "        \"provided_labels\": [x[\"labels\"] for x in data],\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4218f10a-128d-49a6-a594-ef19b5435df9",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e470ae7a-fea0-48b5-ad4f-46be0ba5cd59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "    idx = 0\n",
    "    spans = []\n",
    "    span = []\n",
    "\n",
    "    for i, token in enumerate(document):\n",
    "        if token != target[idx]:\n",
    "            idx = 0\n",
    "            span = []\n",
    "            continue\n",
    "        span.append(i)\n",
    "        idx += 1\n",
    "        if idx == len(target):\n",
    "            spans.append(span)\n",
    "            span = []\n",
    "            idx = 0\n",
    "            continue\n",
    "\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4fd14a5-1ca7-464b-9a6a-2274abc76761",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PRFScore:\n",
    "    \"\"\"A precision / recall / F score.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        tp: int = 0,\n",
    "        fp: int = 0,\n",
    "        fn: int = 0,\n",
    "    ) -> None:\n",
    "        self.tp = tp\n",
    "        self.fp = fp\n",
    "        self.fn = fn\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.tp + self.fp + self.fn\n",
    "\n",
    "    def __iadd__(self, other):  # in-place add\n",
    "        self.tp += other.tp\n",
    "        self.fp += other.fp\n",
    "        self.fn += other.fn\n",
    "        return self\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return PRFScore(\n",
    "            tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn\n",
    "        )\n",
    "\n",
    "    def score_set(self, cand: set, gold: set) -> None:\n",
    "        self.tp += len(cand.intersection(gold))\n",
    "        self.fp += len(cand - gold)\n",
    "        self.fn += len(gold - cand)\n",
    "\n",
    "    @property\n",
    "    def precision(self) -> float:\n",
    "        return self.tp / (self.tp + self.fp + 1e-100)\n",
    "\n",
    "    @property\n",
    "    def recall(self) -> float:\n",
    "        return self.tp / (self.tp + self.fn + 1e-100)\n",
    "\n",
    "    @property\n",
    "    def f1(self) -> float:\n",
    "        p = self.precision\n",
    "        r = self.recall\n",
    "        return 2 * ((p * r) / (p + r + 1e-100))\n",
    "\n",
    "    @property\n",
    "    def f5(self) -> float:\n",
    "        beta = 5\n",
    "        p = self.precision\n",
    "        r = self.recall\n",
    "\n",
    "        fbeta = (1+(beta**2))*p*r / ((beta**2)*p + r + 1e-100)\n",
    "        return fbeta\n",
    "\n",
    "    def to_dict(self) -> dict[str, float]:\n",
    "        return {\"p\": self.precision, \"r\": self.recall, \"f5\": self.f5}\n",
    "\n",
    "\n",
    "class MetricsComputerV2:\n",
    "    nlp = English()\n",
    "\n",
    "    def __init__(self, eval_ds: Dataset, label2id: dict, conf_thresh: float = 0.9) -> None:\n",
    "        self.ds = eval_ds.remove_columns(\"labels\").rename_columns({\"provided_labels\": \"labels\"})\n",
    "        self.gt_df = self.create_gt_df(self.ds)\n",
    "        self.label2id = label2id\n",
    "        self.confth = conf_thresh\n",
    "        self._search_gt()\n",
    "\n",
    "    def __call__(self, eval_preds: EvalPrediction) -> dict:\n",
    "        pred_df = self.create_pred_df(eval_preds.predictions)\n",
    "        return self.compute_metrics_from_df(self.gt_df, pred_df)\n",
    "\n",
    "    def _search_gt(self) -> None:\n",
    "        email_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
    "        phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n",
    "        self.emails = []\n",
    "        self.phone_nums = []\n",
    "\n",
    "        for _data in self.ds:\n",
    "            # email\n",
    "            for token_idx, token in enumerate(_data[\"tokens\"]):\n",
    "                if re.fullmatch(email_regex, token) is not None:\n",
    "                    self.emails.append(\n",
    "                        {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
    "                    )\n",
    "            # phone number\n",
    "            matches = phone_num_regex.findall(_data[\"full_text\"])\n",
    "            if not matches:\n",
    "                continue\n",
    "            for match in matches:\n",
    "                target = [t.text for t in self.nlp.tokenizer(match)]\n",
    "                matched_spans = find_span(target, _data[\"tokens\"])\n",
    "            for matched_span in matched_spans:\n",
    "                for intermediate, token_idx in enumerate(matched_span):\n",
    "                    prefix = \"I\" if intermediate else \"B\"\n",
    "                    self.phone_nums.append(\n",
    "                        {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": _data[\"tokens\"][token_idx]}\n",
    "                    )\n",
    "\n",
    "    @staticmethod\n",
    "    def create_gt_df(ds: Dataset):\n",
    "        gt = []\n",
    "        for row in ds:\n",
    "            for token_idx, (token, label) in enumerate(zip(row[\"tokens\"], row[\"labels\"])):\n",
    "                if label == \"O\":\n",
    "                    continue\n",
    "                gt.append(\n",
    "                    {\"document\": row[\"document\"], \"token\": token_idx, \"label\": label, \"token_str\": token}\n",
    "                )\n",
    "        gt_df = pd.DataFrame(gt)\n",
    "        gt_df[\"row_id\"] = gt_df.index\n",
    "\n",
    "        return gt_df\n",
    "\n",
    "    def create_pred_df(self, prediction: np.ndarray) -> pd.DataFrame:\n",
    "        ### construct prediction df\n",
    "        o_index = self.label2id[\"O\"]\n",
    "        preds = prediction.argmax(-1)\n",
    "        preds_without_o = prediction[:,:,:o_index].argmax(-1)\n",
    "        o_preds = prediction[:,:,o_index]\n",
    "        preds_final = np.where(o_preds < self.confth, preds_without_o , preds)\n",
    "\n",
    "        pairs = set()\n",
    "        processed = []\n",
    "\n",
    "        # Iterate over document\n",
    "        for p_doc, token_map, offsets, tokens, doc in zip(\n",
    "            preds_final, self.ds[\"token_map\"], self.ds[\"offset_mapping\"], self.ds[\"tokens\"], self.ds[\"document\"]\n",
    "        ):\n",
    "            # Iterate over sequence\n",
    "            for p_token, (start_idx, end_idx) in zip(p_doc, offsets):\n",
    "                label_pred = id2label[p_token]\n",
    "\n",
    "                if start_idx + end_idx == 0:\n",
    "                    # [CLS] token i.e. BOS\n",
    "                    continue\n",
    "\n",
    "                if token_map[start_idx] == -1:\n",
    "                    start_idx += 1\n",
    "\n",
    "                # ignore \"\\n\\n\"\n",
    "                while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "                    start_idx += 1\n",
    "\n",
    "                if start_idx >= len(token_map):\n",
    "                    break\n",
    "\n",
    "                token_id = token_map[start_idx]\n",
    "                pair = (doc, token_id)\n",
    "\n",
    "                # ignore \"O\", preds, phone number and  email\n",
    "                if label_pred in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\") or token_id == -1:\n",
    "                    continue\n",
    "\n",
    "                if pair in pairs:\n",
    "                    continue\n",
    "\n",
    "                processed.append(\n",
    "                    {\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]}\n",
    "                )\n",
    "                pairs.add(pair)\n",
    "\n",
    "        pred_df = pd.DataFrame(processed + self.emails + self.phone_nums)\n",
    "        pred_df[\"row_id\"] = list(range(len(pred_df)))\n",
    "\n",
    "        return pred_df\n",
    "\n",
    "    def compute_metrics_from_df(self, gt_df, pred_df):\n",
    "        \"\"\"\n",
    "        Compute the LB metric (lb) and other auxiliary metrics\n",
    "        \"\"\"\n",
    "\n",
    "        references = {(row.document, row.token, row.label) for row in gt_df.itertuples()}\n",
    "        predictions = {(row.document, row.token, row.label) for row in pred_df.itertuples()}\n",
    "\n",
    "        score_per_type = defaultdict(PRFScore)\n",
    "        references = set(references)\n",
    "\n",
    "        for ex in predictions:\n",
    "            pred_type = ex[-1] # (document, token, label)\n",
    "            if pred_type != 'O':\n",
    "                pred_type = pred_type[2:] # avoid B- and I- prefix\n",
    "\n",
    "            if pred_type not in score_per_type:\n",
    "                score_per_type[pred_type] = PRFScore()\n",
    "\n",
    "            if ex in references:\n",
    "                score_per_type[pred_type].tp += 1\n",
    "                references.remove(ex)\n",
    "            else:\n",
    "                score_per_type[pred_type].fp += 1\n",
    "\n",
    "        for doc, tok, ref_type in references:\n",
    "            if ref_type != 'O':\n",
    "                ref_type = ref_type[2:] # avoid B- and I- prefix\n",
    "\n",
    "            if ref_type not in score_per_type:\n",
    "                score_per_type[ref_type] = PRFScore()\n",
    "            score_per_type[ref_type].fn += 1\n",
    "\n",
    "        totals = PRFScore()\n",
    "\n",
    "        for prf in score_per_type.values():\n",
    "            totals += prf\n",
    "\n",
    "        return {\n",
    "            \"precision\": totals.precision,\n",
    "            \"recall\": totals.recall,\n",
    "            \"f5\": totals.f5,\n",
    "            **{\n",
    "                f\"{v_k}-{k}\": v_v\n",
    "                for k in set([l[2:] for l in self.label2id.keys() if l!= 'O'])\n",
    "                for v_k, v_v in score_per_type[k].to_dict().items()\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a7d8dc-6579-4678-a2ba-27d22e6aac00",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1700da69-ff6a-4fb6-9901-aaf93d589b29",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForTokenClassification: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class ModelInit:\n",
    "    def __init__(\n",
    "        self,\n",
    "        checkpoint: str,\n",
    "        id2label: dict,\n",
    "        label2id: dict,\n",
    "        freeze_embedding: bool,\n",
    "        freeze_layers: int,\n",
    "    ) -> None:\n",
    "        self.model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "            checkpoint,\n",
    "            num_labels=len(id2label),\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        for param in self.model.deberta.embeddings.parameters():\n",
    "            param.requires_grad = False if freeze_embedding else True\n",
    "        for layer in self.model.deberta.encoder.layer[:freeze_layers]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.weight = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "    def __call__(self) -> DebertaV2ForTokenClassification:\n",
    "        self.model.load_state_dict(self.weight)\n",
    "        return self.model\n",
    "\n",
    "model_init = ModelInit(\n",
    "    TRAINING_MODEL_PATH,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    freeze_embedding=FREEZE_EMBEDDING,\n",
    "    freeze_layers=FREEZE_LAYERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74884fb-fc4d-4684-a79c-fdfecb64198a",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7eaea9c-c63a-4a77-932f-22ff25360923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split according to document id\n",
    "folds = [\n",
    "    (\n",
    "        np.array([i for i, d in enumerate(ds[\"original\"][\"document\"]) if int(d) % N_SPLITS != s]),\n",
    "        np.array([i for i, d in enumerate(ds[\"original\"][\"document\"]) if int(d) % N_SPLITS == s])\n",
    "    )\n",
    "    for s in range(N_SPLITS)\n",
    "]\n",
    "\n",
    "negative_idxs = [i for i, labels in enumerate(ds[\"original\"][\"provided_labels\"]) if not any(np.array(labels) != \"O\")]\n",
    "exclude_indices = negative_idxs[int(len(negative_idxs) * NEGATIVE_RATIO):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12100606-72e2-4caa-bf10-495073e243c7",
   "metadata": {},
   "source": [
    "## Layer-wise lr Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0970f634-639e-4158-8a6e-5c9cb0fb7098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RectifiedLinearWiseLr:\n",
    "    def __init__(self, start_layer: int, end_layer: int, max_lr: float, min_lr: float = 0.0) -> None:\n",
    "        lin = np.arange(end_layer - start_layer)\n",
    "        lin = lin / lin.max()\n",
    "        self.learning_rates = min_lr + lin * (max_lr - min_lr)\n",
    "        self.start_layer = start_layer\n",
    "        self.end_layer = end_layer\n",
    "        \n",
    "    def get_lr_from_layer_name(self, layer_name: str) -> float:\n",
    "        for i in range(self.start_layer, self.end_layer):\n",
    "            if f\"layer.{i}\" in layer_name:\n",
    "                return self.learning_rates[i - self.start_layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "270032b6-ba91-4de3-a151-8ef32a3300e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerWiseLrTrainer(Trainer):\n",
    "    def create_optimizer(self) -> torch.optim.Optimizer:\n",
    "        if self.optimizer is not None:\n",
    "            return self.optimizer\n",
    "        lr_assigner = RectifiedLinearWiseLr(\n",
    "            start_layer=max(0, FREEZE_LAYERS,\n",
    "            end_layer=self.model.config.num_hidden_layers,\n",
    "            max_lr=self.args.learning_rate,\n",
    "            min_lr=0.0\n",
    "        )\n",
    "        decay_parameters = get_parameter_names(self.model, ALL_LAYERNORM_LAYERS)\n",
    "        decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "        optim_param_groups = [\n",
    "            {\n",
    "                \"params\": p,\n",
    "                \"weight_decay\": self.args.weight_decay if n in decay_parameters else 0.0,\n",
    "                \"lr\":  lr_assigner.get_lr_from_layer_name(n) or self.args.learning_rate\n",
    "            } \n",
    "            for n, p in self.model.named_parameters() if p.requires_grad\n",
    "        ]\n",
    "\n",
    "        optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(self.args)\n",
    "        self.optimizer = optimizer_cls(optim_param_groups, **optimizer_kwargs)\n",
    "        \n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21887fb3-e3b1-497c-92ad-64def1bc9511",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec8b773-e2e8-454a-b21d-b9e5cdf9eb09",
   "metadata": {},
   "source": [
    "#### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3d40f-e75d-4b05-85b0-7e0d620ce36a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64a105e32ac4c95941f411896816cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/588 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28bbd1627cc404d91af04ae2f7734cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/588 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0332120ff4402ba166b6e32d514f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/588 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9166fdd76a4170b359f48e3903e658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/588 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1444a1d7d174f3faaae0b6b006736bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/588 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc993ab9668a4cf5a7372ee4688bd453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/588 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a3599df3e34751b7d7670f149a9ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/588 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd6e687a654492d929f859f10a11470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/588 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f352a6eaa4d6422ea1d4de7a96e58fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/213 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560326de56c94fbe812b2cd581bcf204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/213 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba55d46a4d34d07b21e53436906e2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/212 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bebb8df71db4ce6a63d4688365ab386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/212 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaca6e962e5447f0ac2442cd9f9f6672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/212 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cad6abbcd64799a589d98913e25ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/212 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c773e1965ec64f9cbcbbd4b3624533ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/212 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e019f67a1c514778a1a2795861367b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/212 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DebertaV2ForTokenClassification.forward` and have been ignored: tokens, provided_labels, full_text, token_map, trailing_whitespace, document, length, offset_mapping. If tokens, provided_labels, full_text, token_map, trailing_whitespace, document, length, offset_mapping are not expected by `DebertaV2ForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4704\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 882\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memiz6413\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/notebooks/wandb/run-20240308_152919-3gu19g6s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/emiz6413/PII/runs/3gu19g6s\" target=\"_blank\">fold-0</a></strong> to <a href=\"https://wandb.ai/emiz6413/PII\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='882' max='882' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [882/882 1:41:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F5</th>\n",
       "      <th>P-email</th>\n",
       "      <th>R-email</th>\n",
       "      <th>F5-email</th>\n",
       "      <th>P-street Address</th>\n",
       "      <th>R-street Address</th>\n",
       "      <th>F5-street Address</th>\n",
       "      <th>P-id Num</th>\n",
       "      <th>R-id Num</th>\n",
       "      <th>F5-id Num</th>\n",
       "      <th>P-name Student</th>\n",
       "      <th>R-name Student</th>\n",
       "      <th>F5-name Student</th>\n",
       "      <th>P-phone Num</th>\n",
       "      <th>R-phone Num</th>\n",
       "      <th>F5-phone Num</th>\n",
       "      <th>P-url Personal</th>\n",
       "      <th>R-url Personal</th>\n",
       "      <th>F5-url Personal</th>\n",
       "      <th>P-username</th>\n",
       "      <th>R-username</th>\n",
       "      <th>F5-username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.977200</td>\n",
       "      <td>1.214208</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.019757</td>\n",
       "      <td>0.003979</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.015489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.033011</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.118361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.383481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.003959</td>\n",
       "      <td>0.408046</td>\n",
       "      <td>0.323708</td>\n",
       "      <td>0.326302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.237082</td>\n",
       "      <td>0.484680</td>\n",
       "      <td>0.290970</td>\n",
       "      <td>0.295512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.215517</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.337900</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.748930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.718023</td>\n",
       "      <td>0.344165</td>\n",
       "      <td>0.779264</td>\n",
       "      <td>0.743131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274725</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.907821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.642431</td>\n",
       "      <td>0.819149</td>\n",
       "      <td>0.810573</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.839941</td>\n",
       "      <td>0.650538</td>\n",
       "      <td>0.809365</td>\n",
       "      <td>0.801835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>0.522401</td>\n",
       "      <td>0.886018</td>\n",
       "      <td>0.862917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.879412</td>\n",
       "      <td>0.521266</td>\n",
       "      <td>0.881271</td>\n",
       "      <td>0.858468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438596</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>0.614072</td>\n",
       "      <td>0.875380</td>\n",
       "      <td>0.861284</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.844904</td>\n",
       "      <td>0.599772</td>\n",
       "      <td>0.879599</td>\n",
       "      <td>0.864093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.802469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>0.636957</td>\n",
       "      <td>0.890578</td>\n",
       "      <td>0.877145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.630153</td>\n",
       "      <td>0.894649</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.834862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.690867</td>\n",
       "      <td>0.896657</td>\n",
       "      <td>0.886500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.878120</td>\n",
       "      <td>0.702490</td>\n",
       "      <td>0.896321</td>\n",
       "      <td>0.886909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.864048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.942249</td>\n",
       "      <td>0.913314</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.880707</td>\n",
       "      <td>0.513218</td>\n",
       "      <td>0.941472</td>\n",
       "      <td>0.912195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968703</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.622153</td>\n",
       "      <td>0.913374</td>\n",
       "      <td>0.897221</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.623570</td>\n",
       "      <td>0.911371</td>\n",
       "      <td>0.895475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977444</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.693195</td>\n",
       "      <td>0.913374</td>\n",
       "      <td>0.902350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.691624</td>\n",
       "      <td>0.911371</td>\n",
       "      <td>0.900369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>0.588910</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>0.915409</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.883309</td>\n",
       "      <td>0.581686</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.913456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986343</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.838710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.639293</td>\n",
       "      <td>0.934650</td>\n",
       "      <td>0.918332</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.883309</td>\n",
       "      <td>0.636986</td>\n",
       "      <td>0.933110</td>\n",
       "      <td>0.916719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986343</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.002327</td>\n",
       "      <td>0.564384</td>\n",
       "      <td>0.939210</td>\n",
       "      <td>0.915816</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.559204</td>\n",
       "      <td>0.939799</td>\n",
       "      <td>0.915826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986343</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.787879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>0.595950</td>\n",
       "      <td>0.939210</td>\n",
       "      <td>0.918854</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.920354</td>\n",
       "      <td>0.593220</td>\n",
       "      <td>0.936455</td>\n",
       "      <td>0.916069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986343</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>0.654584</td>\n",
       "      <td>0.933131</td>\n",
       "      <td>0.918104</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.883309</td>\n",
       "      <td>0.653756</td>\n",
       "      <td>0.931438</td>\n",
       "      <td>0.916466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986343</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForTokenClassification.forward` and have been ignored: tokens, provided_labels, full_text, token_map, trailing_whitespace, document, length, offset_mapping. If tokens, provided_labels, full_text, token_map, trailing_whitespace, document, length, offset_mapping are not expected by `DebertaV2ForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_0/checkpoint-750\n",
      "Configuration saved in output/fold_0/checkpoint-750/config.json\n",
      "Model weights saved in output/fold_0/checkpoint-750/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_0/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_0/checkpoint-750/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForTokenClassification.forward` and have been ignored: tokens, provided_labels, full_text, token_map, trailing_whitespace, document, length, offset_mapping. If tokens, provided_labels, full_text, token_map, trailing_whitespace, document, length, offset_mapping are not expected by `DebertaV2ForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_0/checkpoint-800\n",
      "Configuration saved in output/fold_0/checkpoint-800/config.json\n",
      "Model weights saved in output/fold_0/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_0/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_0/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_0/checkpoint-700] due to args.save_total_limit\n",
      "Deleting older checkpoint [output/fold_0/checkpoint-750] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForTokenClassification.forward` and have been ignored: tokens, provided_labels, full_text, token_map, trailing_whitespace, document, length, offset_mapping. If tokens, provided_labels, full_text, token_map, trailing_whitespace, document, length, offset_mapping are not expected by `DebertaV2ForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_0/checkpoint-850\n",
      "Configuration saved in output/fold_0/checkpoint-850/config.json\n",
      "Model weights saved in output/fold_0/checkpoint-850/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_0/checkpoint-850/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_0/checkpoint-850/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from output/fold_0/checkpoint-800 (score: 0.9188540058329044).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForTokenClassification.forward` and have been ignored: tokens, provided_labels, full_text, token_map, trailing_whitespace, document, length, offset_mapping. If tokens, provided_labels, full_text, token_map, trailing_whitespace, document, length, offset_mapping are not expected by `DebertaV2ForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='213' max='213' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [213/213 02:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a579029c4bd34f2e88f3c7ba0525a837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f5</td><td>▁▁▃▇▇█████████████</td></tr><tr><td>eval/f5-EMAIL</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/f5-ID_NUM</td><td>▁▂▃▆▇█▇▇██▇▇██▇███</td></tr><tr><td>eval/f5-NAME_STUDENT</td><td>▁▁▃▇▇█████████████</td></tr><tr><td>eval/f5-PHONE_NUM</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/f5-STREET_ADDRESS</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/f5-URL_PERSONAL</td><td>▁▄▇▇██▇▇▇█████████</td></tr><tr><td>eval/f5-USERNAME</td><td>▁▁▁▁▁▁▁▁████▇█▇▇▇▇</td></tr><tr><td>eval/loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/p-EMAIL</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/p-ID_NUM</td><td>▁▄▇▅▇▇██▇▇████████</td></tr><tr><td>eval/p-NAME_STUDENT</td><td>▁▁▆▄▇▆▇▇█▆▇█▇▇▇▇█▇</td></tr><tr><td>eval/p-PHONE_NUM</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/p-STREET_ADDRESS</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/p-URL_PERSONAL</td><td>▁▃▃▃▅▅█▇▆▅▆▇▇▇▇▇▇▇</td></tr><tr><td>eval/p-USERNAME</td><td>▁▁▁▁▁▁▁▁█▆▅▆▅▆▄▃▄▃</td></tr><tr><td>eval/precision</td><td>▁▃▅▄▇▆▇▇█▆▇█▇▇▇▇█▇</td></tr><tr><td>eval/r-EMAIL</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/r-ID_NUM</td><td>▂▁▂▆▇█▇▇██▇▇██▇███</td></tr><tr><td>eval/r-NAME_STUDENT</td><td>▁▁▃▇▇█████████████</td></tr><tr><td>eval/r-PHONE_NUM</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/r-STREET_ADDRESS</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/r-URL_PERSONAL</td><td>▁▄████▇▇▇█████████</td></tr><tr><td>eval/r-USERNAME</td><td>▁▁▁▁▁▁▁▁██████████</td></tr><tr><td>eval/recall</td><td>▁▁▃▇▇█▇███████████</td></tr><tr><td>eval/runtime</td><td>█▅▂▃▁▄▁▁▄▂▃▂▃▁▂▅▃▃</td></tr><tr><td>eval/samples_per_second</td><td>▁▄▇▆█▅██▅▇▆▇▆█▇▄▆▆</td></tr><tr><td>eval/steps_per_second</td><td>▁▄▇▆█▅██▅▇▆▇▆█▇▄▆▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▂▃▅▆███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f5</td><td>0.91885</td></tr><tr><td>eval/f5-EMAIL</td><td>1.0</td></tr><tr><td>eval/f5-ID_NUM</td><td>0.92035</td></tr><tr><td>eval/f5-NAME_STUDENT</td><td>0.91607</td></tr><tr><td>eval/f5-PHONE_NUM</td><td>0.0</td></tr><tr><td>eval/f5-STREET_ADDRESS</td><td>0.0</td></tr><tr><td>eval/f5-URL_PERSONAL</td><td>0.98634</td></tr><tr><td>eval/f5-USERNAME</td><td>0.76471</td></tr><tr><td>eval/loss</td><td>0.00212</td></tr><tr><td>eval/p-EMAIL</td><td>1.0</td></tr><tr><td>eval/p-ID_NUM</td><td>0.85714</td></tr><tr><td>eval/p-NAME_STUDENT</td><td>0.59322</td></tr><tr><td>eval/p-PHONE_NUM</td><td>0.0</td></tr><tr><td>eval/p-STREET_ADDRESS</td><td>0.0</td></tr><tr><td>eval/p-URL_PERSONAL</td><td>0.73529</td></tr><tr><td>eval/p-USERNAME</td><td>0.11111</td></tr><tr><td>eval/precision</td><td>0.59595</td></tr><tr><td>eval/r-EMAIL</td><td>1.0</td></tr><tr><td>eval/r-ID_NUM</td><td>0.92308</td></tr><tr><td>eval/r-NAME_STUDENT</td><td>0.93645</td></tr><tr><td>eval/r-PHONE_NUM</td><td>0.0</td></tr><tr><td>eval/r-STREET_ADDRESS</td><td>0.0</td></tr><tr><td>eval/r-URL_PERSONAL</td><td>1.0</td></tr><tr><td>eval/r-USERNAME</td><td>1.0</td></tr><tr><td>eval/recall</td><td>0.93921</td></tr><tr><td>eval/runtime</td><td>177.6699</td></tr><tr><td>eval/samples_per_second</td><td>9.557</td></tr><tr><td>eval/steps_per_second</td><td>1.199</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>882</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0054</td></tr><tr><td>train/total_flos</td><td>2.020810357893024e+16</td></tr><tr><td>train/train_loss</td><td>0.20331</td></tr><tr><td>train/train_runtime</td><td>6112.7693</td></tr><tr><td>train/train_samples_per_second</td><td>2.309</td></tr><tr><td>train/train_steps_per_second</td><td>0.144</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fold-0</strong>: <a href=\"https://wandb.ai/emiz6413/PII/runs/3gu19g6s\" target=\"_blank\">https://wandb.ai/emiz6413/PII/runs/3gu19g6s</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240308_152919-3gu19g6s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c0cb14a6f344f9b350082aa72e7804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/590 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a2bff28bb0430e9440ce61478e43f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/590 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7df566f9eea491492451283a5d302c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/589 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f9848f97b34994ad776df83ab169e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/589 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487c39b5ad704b01b390c54b2eb9e10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/589 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3608f390815e483899b8cc9d208dec51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/589 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961ce7663d1d4bbab1c1c00f1865d889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/589 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa934fc93284c478da2dcb24781f482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/589 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafb13488a7c4107af15ce89c93c2b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/215 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2970076ce6884aa4b7779acc44df4c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/215 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165ab0d25f004badb8934804cf9eb4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f10b65d943343869e12b0186a864cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef77511372b4670ac8f3d79436deccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31d8672e26c405b985772929f0e348c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bef8a7ccb0f4bc19e7c025f8b80ec89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea13bdcc31584c14a2cd025032c21937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DebertaV2ForTokenClassification.forward` and have been ignored: tokens, provided_labels, full_text, token_map, trailing_whitespace, document, length, offset_mapping. If tokens, provided_labels, full_text, token_map, trailing_whitespace, document, length, offset_mapping are not expected by `DebertaV2ForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4714\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 882\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/notebooks/wandb/run-20240308_171513-1av07sso</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/emiz6413/PII/runs/1av07sso\" target=\"_blank\">fold-1</a></strong> to <a href=\"https://wandb.ai/emiz6413/PII\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='882' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 24/882 01:11 < 46:24, 0.31 it/s, Epoch 0.08/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for fold_idx, (train_idx, eval_idx) in enumerate(folds):\n",
    "    args.run_name = f\"fold-{fold_idx}\"\n",
    "    args.output_dir = os.path.join(OUTPUT_DIR, f\"fold_{fold_idx}\")\n",
    "    if Path(args.output_dir).joinpath(\"eval_result.json\").exists():\n",
    "        continue\n",
    "    original_ds = ds[\"original\"].select([i for i in train_idx if i not in exclude_indices])\n",
    "    train_ds = concatenate_datasets([original_ds, ds[\"extra\"]])\n",
    "    train_ds = train_ds.map(train_encoder, num_proc=os.cpu_count())\n",
    "    eval_ds = ds[\"original\"].select(eval_idx)\n",
    "    eval_ds = eval_ds.map(eval_encoder, num_proc=os.cpu_count())\n",
    "    trainer = LayerWiseLrTrainer(\n",
    "        args=args,\n",
    "        model_init=model_init,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=MetricsComputerV2(eval_ds=eval_ds, label2id=label2id),\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16),\n",
    "    )\n",
    "    trainer.train()\n",
    "    eval_res = trainer.evaluate(eval_dataset=eval_ds)\n",
    "    with open(os.path.join(args.output_dir, \"eval_result.json\"), \"w\") as f:\n",
    "        json.dump(eval_res, f)\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    wandb.alert(title=\"Run finished\", text=f\"{wandb.run.group} {args.run_name} f5={eval_res['eval_f5']:.3f}\")\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6373c548-f263-4271-a4eb-47c16bc42a1b",
   "metadata": {},
   "source": [
    "### log CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e784702c-9436-4a7a-8478-e3b41889d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(name=\"cv\")\n",
    "results = dict()\n",
    "for res_json_path in Path(OUTPUT_DIR).glob(\"fold*/eval_result.json\"):\n",
    "    fold = res_json_path.parent.name.split(\"_\")[-1]\n",
    "    with open(res_json_path, \"r\") as f:\n",
    "        res = json.load(f)\n",
    "        results[fold] = {k.replace(\"eval_\", \"\"): v for k, v in res.items()}\n",
    "results[\"cv\"] = {key: np.mean([r[key] for r in results.values()]) for key in results[\"0\"].keys()}\n",
    "table = wandb.Table(columns=[\"fold\"] + list(results[\"0\"].keys()))\n",
    "for f, res in results.items():\n",
    "    table.add_data(f, *[res[c] for c in table.columns if c != \"fold\"])\n",
    "wandb.log({\"eval_result\": table})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af79fbd-6091-43d8-8db6-a8c24fe94a3b",
   "metadata": {},
   "source": [
    "#### Train with full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd36d3-579a-4965-982b-00c2e1887c50",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_ds = ds[\"original\"].select([i for i in range(len(ds[\"original\"])) if i not in exclude_indices])\n",
    "train_ds = concatenate_datasets([original_ds, ds[\"extra\"]])\n",
    "train_ds = train_ds.map(train_encoder, num_proc=os.cpu_count())\n",
    "args.evaluation_strategy = \"no\"\n",
    "args.save_strategy = \"no\"\n",
    "args.run_name = f\"all_data\"\n",
    "trainer = LayerWiseLrTrainer(\n",
    "    args=args,\n",
    "    model_init=model_init,\n",
    "    train_dataset=train_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16),\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(os.path.join(OUTPUT_DIR, \"all\"))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0159a3d2-c25b-4d54-b017-64993c6f1c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
