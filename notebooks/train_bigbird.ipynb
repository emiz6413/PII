{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=PII\n",
      "env: WANDB_RUN_GROUP=bigbird-base-3072-filter+T-CE\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=PII\n",
    "%env WANDB_RUN_GROUP=bigbird-base-3072-filter+T-CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q seqeval evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spacy.lang.en import English\n",
    "from transformers.tokenization_utils import PreTrainedTokenizerBase\n",
    "from transformers.models.big_bird import BigBirdForTokenClassification, BigBirdTokenizerFast\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.trainer_utils import EvalPrediction, PredictionOutput\n",
    "from transformers.data.data_collator import DataCollatorForTokenClassification\n",
    "from sklearn.model_selection import KFold\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "from seqeval.metrics import recall_score, precision_score\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memiz6413\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"eff994fe72307679c21248b6e7859e26960b8db7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../dataset/\")\n",
    "OUTPUT_DIR = \"output\"\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAINING_MODEL_PATH = \"google/bigbird-roberta-base\"\n",
    "TRAINING_MAX_LENGTH = 3072 if \"tiny-random\" not in TRAINING_MODEL_PATH else 512\n",
    "EVAL_MAX_LENGTH = 3072 if \"tiny-random\" not in TRAINING_MODEL_PATH else 512\n",
    "CONF_THRESH = 0.9\n",
    "LR = 2.5e-5  # 1.5e-5 ~ 3e-5 for base # 5e-6 ~ 1e-5 for large\n",
    "LR_SCHEDULER_TYPE = \"linear\"\n",
    "NUM_EPOCHS = 3 if \"tiny-random\" not in TRAINING_MODEL_PATH else 0.1\n",
    "BATCH_SIZE = 8\n",
    "EVAL_BATCH_SIZE = 8\n",
    "GRAD_ACCUMULATION_STEPS = 16 // BATCH_SIZE\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "FREEZE_EMBEDDING = False\n",
    "FREEZE_LAYERS = 0\n",
    "GAMMA = 0\n",
    "MASK_P = 0\n",
    "# training data\n",
    "N_SPLITS = 4\n",
    "FILTER_ORIGINAL = True\n",
    "MOTH = False\n",
    "PJMATHMATICIAN = False\n",
    "NICHOLAS = False\n",
    "MPWARE = False\n",
    "TONYAROBERTSON = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION_STEPS,\n",
    "    report_to=\"wandb\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=10,\n",
    "    metric_for_best_model=\"f5\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    overwrite_output_dir=True,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tonyarobertson's datapoints:  1850\n",
      "len(extra_data): 1850\n"
     ]
    }
   ],
   "source": [
    "with DATA_DIR.joinpath(\"train.json\").open(\"r\") as f:\n",
    "    original_data = json.load(f)\n",
    "\n",
    "extra_data = []  #\n",
    "\n",
    "if MOTH:\n",
    "    with DATA_DIR.joinpath(\"pii_dataset_fixed.json\").open(\"r\") as f:\n",
    "        external = json.load(f)\n",
    "    print(\"Moth's datapoints: \", len(external))\n",
    "    extra_data.extend(external)\n",
    "\n",
    "if PJMATHMATICIAN:\n",
    "    with DATA_DIR.joinpath(\"moredata_dataset_fixed.json\").open(\"r\") as f:\n",
    "        external = json.load(f)\n",
    "    print(\"PJMathmatician's datapoints: \", len(external))\n",
    "    extra_data.extend(external)\n",
    "\n",
    "if NICHOLAS:\n",
    "    with DATA_DIR.joinpath(\"mixtral-8x7b-v1.json\").open(\"r\") as f:\n",
    "        external = json.load(f)\n",
    "    print(\"Nicholas' datapoints: \", len(external))\n",
    "    extra_data.extend(external)\n",
    "    \n",
    "if MPWARE:\n",
    "    with DATA_DIR.joinpath(\"mpware_mixtral8x7b_v1.1.json\").open(\"r\") as f:\n",
    "        external = json.load(f)\n",
    "    print(\"MPWARE's datapoints: \", len(external))\n",
    "    extra_data.extend(external)\n",
    "    \n",
    "if TONYAROBERTSON:\n",
    "    with DATA_DIR.joinpath(\"Fake_data_1850_218.json\").open(\"r\") as f:\n",
    "        external = json.load(f)\n",
    "    print(\"tonyarobertson's datapoints: \", len(external))\n",
    "    extra_data.extend(external)\n",
    "    \n",
    "print(f\"len(extra_data): {len(extra_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_labels = [\n",
    "    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O'\n",
    "]\n",
    "id2label = {i: l for i, l in enumerate(all_labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "target = [l for l in all_labels if l != \"O\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerBase, label2id: dict, max_length: int) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, example: dict) -> dict:\n",
    "        # rebuild text from tokens\n",
    "        text, labels, token_map = [], [], []\n",
    "\n",
    "        for idx, (t, l, ws) in enumerate(\n",
    "            zip(example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"])\n",
    "        ):\n",
    "            text.append(t)\n",
    "            labels.extend([l] * len(t))\n",
    "            token_map.extend([idx]*len(t))\n",
    "\n",
    "            if ws:\n",
    "                text.append(\" \")\n",
    "                labels.append(\"O\")\n",
    "                token_map.append(-1)\n",
    "\n",
    "        text = \"\".join(text)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # actual tokenization\n",
    "        tokenized = self.tokenizer(\n",
    "            \"\".join(text),\n",
    "            return_offsets_mapping=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "        token_labels = []\n",
    "\n",
    "        for start_idx, end_idx in tokenized.offset_mapping:\n",
    "            # CLS token\n",
    "            if start_idx == 0 and end_idx == 0:\n",
    "                token_labels.append(self.label2id[\"O\"])\n",
    "                continue\n",
    "\n",
    "            # case when token starts with whitespace\n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "            \n",
    "            try:\n",
    "                token_labels.append(self.label2id[labels[start_idx]])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        length = len(tokenized.input_ids)\n",
    "\n",
    "        return {**tokenized, \"labels\": token_labels, \"length\": length, \"token_map\": token_map}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BigBirdTokenizerFast.from_pretrained(TRAINING_MODEL_PATH)\n",
    "train_encoder = CustomTokenizer(tokenizer=tokenizer, label2id=label2id, max_length=TRAINING_MAX_LENGTH)\n",
    "eval_encoder = CustomTokenizer(tokenizer=tokenizer, label2id=label2id, max_length=EVAL_MAX_LENGTH)\n",
    "\n",
    "ds = DatasetDict()\n",
    "\n",
    "for key, data in zip([\"original\", \"extra\"], [original_data, extra_data]):\n",
    "    ds[key] = Dataset.from_dict({\n",
    "        \"full_text\": [x[\"full_text\"] for x in data],\n",
    "        \"document\": [str(x[\"document\"]) for x in data],\n",
    "        \"tokens\": [x[\"tokens\"] for x in data],\n",
    "        \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "        \"provided_labels\": [x[\"labels\"] for x in data],\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "    idx = 0\n",
    "    spans = []\n",
    "    span = []\n",
    "\n",
    "    for i, token in enumerate(document):\n",
    "        if token != target[idx]:\n",
    "            idx = 0\n",
    "            span = []\n",
    "            continue\n",
    "        span.append(i)\n",
    "        idx += 1\n",
    "        if idx == len(target):\n",
    "            spans.append(span)\n",
    "            span = []\n",
    "            idx = 0\n",
    "            continue\n",
    "    \n",
    "    return spans\n",
    "\n",
    "\n",
    "class PRFScore:\n",
    "    \"\"\"A precision / recall / F score.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        tp: int = 0,\n",
    "        fp: int = 0,\n",
    "        fn: int = 0,\n",
    "    ) -> None:\n",
    "        self.tp = tp\n",
    "        self.fp = fp\n",
    "        self.fn = fn\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.tp + self.fp + self.fn\n",
    "\n",
    "    def __iadd__(self, other):  # in-place add\n",
    "        self.tp += other.tp\n",
    "        self.fp += other.fp\n",
    "        self.fn += other.fn\n",
    "        return self\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return PRFScore(\n",
    "            tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn\n",
    "        )\n",
    "\n",
    "    def score_set(self, cand: set, gold: set) -> None:\n",
    "        self.tp += len(cand.intersection(gold))\n",
    "        self.fp += len(cand - gold)\n",
    "        self.fn += len(gold - cand)\n",
    "\n",
    "    @property\n",
    "    def precision(self) -> float:\n",
    "        return self.tp / (self.tp + self.fp + 1e-100)\n",
    "\n",
    "    @property\n",
    "    def recall(self) -> float:\n",
    "        return self.tp / (self.tp + self.fn + 1e-100)\n",
    "\n",
    "    @property\n",
    "    def f1(self) -> float:\n",
    "        p = self.precision\n",
    "        r = self.recall\n",
    "        return 2 * ((p * r) / (p + r + 1e-100))\n",
    "\n",
    "    @property\n",
    "    def f5(self) -> float:\n",
    "        beta = 5\n",
    "        p = self.precision\n",
    "        r = self.recall\n",
    "\n",
    "        fbeta = (1+(beta**2))*p*r / ((beta**2)*p + r + 1e-100)\n",
    "        return fbeta\n",
    "\n",
    "    def to_dict(self) -> dict[str, float]:\n",
    "        return {\"p\": self.precision, \"r\": self.recall, \"f5\": self.f5}\n",
    "\n",
    "\n",
    "class MetricsComputerV2:\n",
    "    nlp = English()\n",
    "\n",
    "    def __init__(self, eval_ds: Dataset, label2id: dict, conf_thresh: float = 0.9) -> None:\n",
    "        self.ds = eval_ds.remove_columns(\"labels\").rename_columns({\"provided_labels\": \"labels\"})\n",
    "        self.gt_df = self.create_gt_df(self.ds)\n",
    "        self.label2id = label2id\n",
    "        self.confth = conf_thresh\n",
    "        self._search_gt()\n",
    "        \n",
    "    def __call__(self, eval_preds: EvalPrediction) -> dict:\n",
    "        pred_df = self.create_pred_df(eval_preds.predictions)\n",
    "        return self.compute_metrics_from_df(self.gt_df, pred_df)\n",
    "    \n",
    "    def _search_gt(self) -> None:\n",
    "        email_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
    "        phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n",
    "        self.emails = []\n",
    "        self.phone_nums = []\n",
    "\n",
    "        for _data in self.ds:\n",
    "            # email\n",
    "            for token_idx, token in enumerate(_data[\"tokens\"]):\n",
    "                if re.fullmatch(email_regex, token) is not None:\n",
    "                    self.emails.append(\n",
    "                        {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
    "                    )\n",
    "            # phone number\n",
    "            matches = phone_num_regex.findall(_data[\"full_text\"])\n",
    "            if not matches:\n",
    "                continue\n",
    "            for match in matches:\n",
    "                target = [t.text for t in self.nlp.tokenizer(match)]\n",
    "                matched_spans = find_span(target, _data[\"tokens\"])\n",
    "            for matched_span in matched_spans:\n",
    "                for intermediate, token_idx in enumerate(matched_span):\n",
    "                    prefix = \"I\" if intermediate else \"B\"\n",
    "                    self.phone_nums.append(\n",
    "                        {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": _data[\"tokens\"][token_idx]}\n",
    "                    )\n",
    "        \n",
    "    @staticmethod\n",
    "    def create_gt_df(ds: Dataset):\n",
    "        gt = []\n",
    "        for row in ds:\n",
    "            for token_idx, (token, label) in enumerate(zip(row[\"tokens\"], row[\"labels\"])):\n",
    "                if label == \"O\":\n",
    "                    continue\n",
    "                gt.append(\n",
    "                    {\"document\": row[\"document\"], \"token\": token_idx, \"label\": label, \"token_str\": token}\n",
    "                )\n",
    "        gt_df = pd.DataFrame(gt)\n",
    "        gt_df[\"row_id\"] = gt_df.index\n",
    "        \n",
    "        return gt_df\n",
    "    \n",
    "    def create_pred_df(self, prediction: np.ndarray) -> pd.DataFrame:\n",
    "        ### construct prediction df\n",
    "        o_index = self.label2id[\"O\"]\n",
    "        preds = prediction.argmax(-1)\n",
    "        preds_without_o = prediction[:,:,:o_index].argmax(-1)\n",
    "        o_preds = prediction[:,:,o_index]\n",
    "        preds_final = np.where(o_preds < self.confth, preds_without_o , preds)\n",
    "\n",
    "        pairs = set()\n",
    "        processed = []\n",
    "\n",
    "        # Iterate over document\n",
    "        for p_doc, token_map, offsets, tokens, doc in zip(\n",
    "            preds_final, self.ds[\"token_map\"], self.ds[\"offset_mapping\"], self.ds[\"tokens\"], self.ds[\"document\"]\n",
    "        ):\n",
    "            # Iterate over sequence\n",
    "            for p_token, (start_idx, end_idx) in zip(p_doc, offsets):\n",
    "                label_pred = id2label[p_token]\n",
    "\n",
    "                if start_idx + end_idx == 0:\n",
    "                    # [CLS] token i.e. BOS\n",
    "                    continue\n",
    "\n",
    "                if token_map[start_idx] == -1:\n",
    "                    start_idx += 1\n",
    "\n",
    "                # ignore \"\\n\\n\"\n",
    "                while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "                    start_idx += 1\n",
    "\n",
    "                if start_idx >= len(token_map): \n",
    "                    break\n",
    "\n",
    "                token_id = token_map[start_idx]\n",
    "                pair = (doc, token_id)\n",
    "\n",
    "                # ignore \"O\", preds, phone number and  email\n",
    "                if label_pred in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\") or token_id == -1:\n",
    "                    continue   \n",
    "\n",
    "                if pair in pairs:\n",
    "                    continue\n",
    "\n",
    "                processed.append(\n",
    "                    {\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]}\n",
    "                )\n",
    "                pairs.add(pair)\n",
    "\n",
    "        pred_df = pd.DataFrame(processed + self.emails + self.phone_nums)\n",
    "        pred_df[\"row_id\"] = list(range(len(pred_df)))\n",
    "\n",
    "        return pred_df\n",
    "        \n",
    "    def compute_metrics_from_df(self, gt_df, pred_df):\n",
    "        \"\"\"\n",
    "        Compute the LB metric (lb) and other auxiliary metrics\n",
    "        \"\"\"\n",
    "\n",
    "        references = {(row.document, row.token, row.label) for row in gt_df.itertuples()}\n",
    "        predictions = {(row.document, row.token, row.label) for row in pred_df.itertuples()}\n",
    "\n",
    "        score_per_type = defaultdict(PRFScore)\n",
    "        references = set(references)\n",
    "\n",
    "        for ex in predictions:\n",
    "            pred_type = ex[-1] # (document, token, label)\n",
    "            if pred_type != 'O':\n",
    "                pred_type = pred_type[2:] # avoid B- and I- prefix\n",
    "\n",
    "            if pred_type not in score_per_type:\n",
    "                score_per_type[pred_type] = PRFScore()\n",
    "\n",
    "            if ex in references:\n",
    "                score_per_type[pred_type].tp += 1\n",
    "                references.remove(ex)\n",
    "            else:\n",
    "                score_per_type[pred_type].fp += 1\n",
    "\n",
    "        for doc, tok, ref_type in references:\n",
    "            if ref_type != 'O':\n",
    "                ref_type = ref_type[2:] # avoid B- and I- prefix\n",
    "\n",
    "            if ref_type not in score_per_type:\n",
    "                score_per_type[ref_type] = PRFScore()\n",
    "            score_per_type[ref_type].fn += 1\n",
    "\n",
    "        totals = PRFScore()\n",
    "\n",
    "        for prf in score_per_type.values():\n",
    "            totals += prf\n",
    "\n",
    "        return {\n",
    "            \"precision\": totals.precision,\n",
    "            \"recall\": totals.recall,\n",
    "            \"f5\": totals.f5,\n",
    "            **{\n",
    "                f\"{v_k}-{k}\": v_v \n",
    "                for k in set([l[2:] for l in self.label2id.keys() if l!= 'O'])\n",
    "                for v_k, v_v in score_per_type[k].to_dict().items()\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class ModelInit:\n",
    "    model_class = BigBirdForTokenClassification\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        checkpoint: str, \n",
    "        id2label: dict, \n",
    "        label2id: dict,\n",
    "        freeze_embedding: bool,\n",
    "        freeze_layers: int,\n",
    "    ) -> None:\n",
    "        self.model = self.model_class.from_pretrained(\n",
    "            checkpoint,\n",
    "            num_labels=len(id2label),\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        for param in self.model.base_model.embeddings.parameters():\n",
    "            param.requires_grad = False if freeze_embedding else True\n",
    "        for layer in self.model.base_model.encoder.layer[:freeze_layers]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.weight = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "    def __call__(self) -> model_class:\n",
    "        self.model.load_state_dict(self.weight)\n",
    "        return self.model\n",
    "\n",
    "model_init = ModelInit(\n",
    "    TRAINING_MODEL_PATH, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id, \n",
    "    freeze_embedding=FREEZE_EMBEDDING, \n",
    "    freeze_layers=FREEZE_LAYERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split according to document id\n",
    "folds = [\n",
    "    (\n",
    "        np.array([i for i, d in enumerate(ds[\"original\"][\"document\"]) if int(d) % N_SPLITS != s]),\n",
    "        np.array([i for i, d in enumerate(ds[\"original\"][\"document\"]) if int(d) % N_SPLITS == s])\n",
    "    )\n",
    "    for s in range(N_SPLITS)\n",
    "]\n",
    "\n",
    "exclude_indices = []\n",
    "if FILTER_ORIGINAL:\n",
    "    negative_idxs = [i for i, labels in enumerate(ds[\"original\"][\"provided_labels\"]) if not any(np.array(labels) != \"O\")]\n",
    "    exclude_indices = negative_idxs[len(negative_idxs)//3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fec587082e04a6b8fbe5d209f070639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/502 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7f0dcf274c4fbda2dd388806eb6d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/502 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40bb5380103f4e66892602770f50f0d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/502 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17374628f17449eb68c498cadcf14e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/502 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41fd78ce75b8479280548e4f90f28712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/502 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7912e6350ac472bb21a98c31391ff1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/502 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93c142e29f84a89ba58fef7e00863eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/502 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6122acd4ddbe4c39a213640bfeffd43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/501 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90285cc409c443aeb276d6a1af2834f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/213 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85ba3471ac746f6b1c834ecf0ceeed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/213 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb478d6fb0cd4880a43c3f1e76f8351c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/212 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd9366ffa5041a08ea8796dd870403b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/212 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fb7968405e4ccb827103bdc89283bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/212 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e78e5ad71f4450b2716d96ed14c18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/212 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed83fe730ec34cf68f5fd54bbee175ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/212 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cad686fae6c4770b60b2da6b5b43d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/212 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4015\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 753\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e15c790bfb842fcb421e8251f5d448c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666974149023493, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/notebooks/wandb/run-20240228_010648-qr7a1lbv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/emiz6413/PII/runs/qr7a1lbv\" target=\"_blank\">fold-0</a></strong> to <a href=\"https://wandb.ai/emiz6413/PII\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='753' max='753' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [753/753 34:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F5</th>\n",
       "      <th>P-id Num</th>\n",
       "      <th>R-id Num</th>\n",
       "      <th>F5-id Num</th>\n",
       "      <th>P-street Address</th>\n",
       "      <th>R-street Address</th>\n",
       "      <th>F5-street Address</th>\n",
       "      <th>P-username</th>\n",
       "      <th>R-username</th>\n",
       "      <th>F5-username</th>\n",
       "      <th>P-email</th>\n",
       "      <th>R-email</th>\n",
       "      <th>F5-email</th>\n",
       "      <th>P-phone Num</th>\n",
       "      <th>R-phone Num</th>\n",
       "      <th>F5-phone Num</th>\n",
       "      <th>P-name Student</th>\n",
       "      <th>R-name Student</th>\n",
       "      <th>F5-name Student</th>\n",
       "      <th>P-url Personal</th>\n",
       "      <th>R-url Personal</th>\n",
       "      <th>F5-url Personal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.158400</td>\n",
       "      <td>0.025468</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.012158</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.007559</td>\n",
       "      <td>0.403433</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.146496</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.079511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.530769</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.118966</td>\n",
       "      <td>0.223881</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.563584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.005239</td>\n",
       "      <td>0.341487</td>\n",
       "      <td>0.530395</td>\n",
       "      <td>0.519345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.624625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331176</td>\n",
       "      <td>0.513378</td>\n",
       "      <td>0.502740</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.703759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>0.004230</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.683891</td>\n",
       "      <td>0.673381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.471299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473928</td>\n",
       "      <td>0.683946</td>\n",
       "      <td>0.672485</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.822289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>0.003136</td>\n",
       "      <td>0.647462</td>\n",
       "      <td>0.717325</td>\n",
       "      <td>0.714361</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.791304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.676800</td>\n",
       "      <td>0.707358</td>\n",
       "      <td>0.706132</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.796325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>0.488605</td>\n",
       "      <td>0.814590</td>\n",
       "      <td>0.794210</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.883309</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.471899</td>\n",
       "      <td>0.814381</td>\n",
       "      <td>0.792266</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.723338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.664103</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.781660</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664275</td>\n",
       "      <td>0.774247</td>\n",
       "      <td>0.769349</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.897898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.002680</td>\n",
       "      <td>0.714689</td>\n",
       "      <td>0.768997</td>\n",
       "      <td>0.766756</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.848665</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.704615</td>\n",
       "      <td>0.765886</td>\n",
       "      <td>0.763333</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>0.594156</td>\n",
       "      <td>0.834347</td>\n",
       "      <td>0.821572</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.921713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.582249</td>\n",
       "      <td>0.822742</td>\n",
       "      <td>0.809877</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>0.609183</td>\n",
       "      <td>0.826748</td>\n",
       "      <td>0.815545</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600985</td>\n",
       "      <td>0.816054</td>\n",
       "      <td>0.804974</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>0.561546</td>\n",
       "      <td>0.838906</td>\n",
       "      <td>0.823266</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.916300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546961</td>\n",
       "      <td>0.827759</td>\n",
       "      <td>0.811731</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.002537</td>\n",
       "      <td>0.674564</td>\n",
       "      <td>0.822188</td>\n",
       "      <td>0.815326</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.910949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.673611</td>\n",
       "      <td>0.811037</td>\n",
       "      <td>0.804722</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.949772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.002819</td>\n",
       "      <td>0.592712</td>\n",
       "      <td>0.840426</td>\n",
       "      <td>0.827130</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.908297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.585596</td>\n",
       "      <td>0.829431</td>\n",
       "      <td>0.816358</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.002731</td>\n",
       "      <td>0.608168</td>\n",
       "      <td>0.837386</td>\n",
       "      <td>0.825421</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.910949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.597590</td>\n",
       "      <td>0.829431</td>\n",
       "      <td>0.817237</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.914373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>0.604144</td>\n",
       "      <td>0.841945</td>\n",
       "      <td>0.829389</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.903039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.599040</td>\n",
       "      <td>0.834448</td>\n",
       "      <td>0.822024</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.914373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_0/checkpoint-50\n",
      "Configuration saved in output/fold_0/checkpoint-50/config.json\n",
      "Model weights saved in output/fold_0/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_0/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_0/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_0/checkpoint-100] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_0/checkpoint-100\n",
      "Configuration saved in output/fold_0/checkpoint-100/config.json\n",
      "Model weights saved in output/fold_0/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_0/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_0/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_0/checkpoint-50] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_0/checkpoint-150\n",
      "Configuration saved in output/fold_0/checkpoint-150/config.json\n",
      "Model weights saved in output/fold_0/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_0/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_0/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_0/checkpoint-100] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_0/checkpoint-200\n",
      "Configuration saved in output/fold_0/checkpoint-200/config.json\n",
      "Model weights saved in output/fold_0/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_0/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_0/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_0/checkpoint-150] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_0/checkpoint-250\n",
      "Configuration saved in output/fold_0/checkpoint-250/config.json\n",
      "Model weights saved in output/fold_0/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_0/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_0/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_0/checkpoint-200] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_0/checkpoint-300\n",
      "Configuration saved in output/fold_0/checkpoint-300/config.json\n",
      "Model weights saved in output/fold_0/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_0/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_0/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_0/checkpoint-250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_0/checkpoint-350\n",
      "Configuration saved in output/fold_0/checkpoint-350/config.json\n",
      "Model weights saved in output/fold_0/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_0/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_0/checkpoint-350/special_tokens_map.json\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_0/checkpoint-400\n",
      "Configuration saved in output/fold_0/checkpoint-400/config.json\n",
      "Model weights saved in output/fold_0/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_0/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_0/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_0/checkpoint-350] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_0/checkpoint-450\n",
      "Configuration saved in output/fold_0/checkpoint-450/config.json\n",
      "Model weights saved in output/fold_0/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_0/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_0/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_0/checkpoint-300] due to args.save_total_limit\n",
      "Deleting older checkpoint [output/fold_0/checkpoint-400] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_0/checkpoint-500\n",
      "Configuration saved in output/fold_0/checkpoint-500/config.json\n",
      "Model weights saved in output/fold_0/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_0/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_0/checkpoint-500/special_tokens_map.json\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_0/checkpoint-550\n",
      "Configuration saved in output/fold_0/checkpoint-550/config.json\n",
      "Model weights saved in output/fold_0/checkpoint-550/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_0/checkpoint-550/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_0/checkpoint-550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_0/checkpoint-450] due to args.save_total_limit\n",
      "Deleting older checkpoint [output/fold_0/checkpoint-500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_0/checkpoint-600\n",
      "Configuration saved in output/fold_0/checkpoint-600/config.json\n",
      "Model weights saved in output/fold_0/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_0/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_0/checkpoint-600/special_tokens_map.json\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_0/checkpoint-650\n",
      "Configuration saved in output/fold_0/checkpoint-650/config.json\n",
      "Model weights saved in output/fold_0/checkpoint-650/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_0/checkpoint-650/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_0/checkpoint-650/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_0/checkpoint-550] due to args.save_total_limit\n",
      "Deleting older checkpoint [output/fold_0/checkpoint-600] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_0/checkpoint-700\n",
      "Configuration saved in output/fold_0/checkpoint-700/config.json\n",
      "Model weights saved in output/fold_0/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_0/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_0/checkpoint-700/special_tokens_map.json\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_0/checkpoint-750\n",
      "Configuration saved in output/fold_0/checkpoint-750/config.json\n",
      "Model weights saved in output/fold_0/checkpoint-750/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_0/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_0/checkpoint-750/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_0/checkpoint-650] due to args.save_total_limit\n",
      "Deleting older checkpoint [output/fold_0/checkpoint-700] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from output/fold_0/checkpoint-750 (score: 0.8293890712270399).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1698\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='213' max='213' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [213/213 00:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21446f990bb543beadfec442d7750cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f5</td><td>▁▂▅▇▇██▇████████</td></tr><tr><td>eval/f5-EMAIL</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/f5-ID_NUM</td><td>▁▂▆▅▇██▇████████</td></tr><tr><td>eval/f5-NAME_STUDENT</td><td>▁▂▅▇▇███████████</td></tr><tr><td>eval/f5-PHONE_NUM</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/f5-STREET_ADDRESS</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/f5-URL_PERSONAL</td><td>▁▅▆▇▇▆▇▆█████▇▇▇</td></tr><tr><td>eval/f5-USERNAME</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/p-EMAIL</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/p-ID_NUM</td><td>▁▅██▅▇▇▇▇▇▆▆▆▆▅▅</td></tr><tr><td>eval/p-NAME_STUDENT</td><td>▁▆▄▆█▆██▇▇▆█▇▇▇▇</td></tr><tr><td>eval/p-PHONE_NUM</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/p-STREET_ADDRESS</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/p-URL_PERSONAL</td><td>▁▃▅▆▇█▆█▇▆▇▇▇███</td></tr><tr><td>eval/p-USERNAME</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/precision</td><td>█▂▁▃▅▃▅▆▄▄▄▅▄▄▄▄</td></tr><tr><td>eval/r-EMAIL</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/r-ID_NUM</td><td>▁▂▆▅▇██▇████████</td></tr><tr><td>eval/r-NAME_STUDENT</td><td>▁▂▅▇▇█▇▇████████</td></tr><tr><td>eval/r-PHONE_NUM</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/r-STREET_ADDRESS</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/r-URL_PERSONAL</td><td>▁▅▆▇▇▆▇▆█████▇▇▇</td></tr><tr><td>eval/r-USERNAME</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/recall</td><td>▁▂▅▇▇██▇████████</td></tr><tr><td>eval/runtime</td><td>▅▅▄▄█▁▄▁▅▄▅▃█▁▅▄</td></tr><tr><td>eval/samples_per_second</td><td>▄▄▅▅▁█▅█▄▅▄▆▁█▄▅</td></tr><tr><td>eval/steps_per_second</td><td>▄▄▅▅▁█▅█▄▅▄▆▁█▄▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▂▃▄▆███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f5</td><td>0.82939</td></tr><tr><td>eval/f5-EMAIL</td><td>1.0</td></tr><tr><td>eval/f5-ID_NUM</td><td>0.90304</td></tr><tr><td>eval/f5-NAME_STUDENT</td><td>0.82202</td></tr><tr><td>eval/f5-PHONE_NUM</td><td>0.0</td></tr><tr><td>eval/f5-STREET_ADDRESS</td><td>0.0</td></tr><tr><td>eval/f5-URL_PERSONAL</td><td>0.91437</td></tr><tr><td>eval/f5-USERNAME</td><td>0.0</td></tr><tr><td>eval/loss</td><td>0.00275</td></tr><tr><td>eval/p-EMAIL</td><td>1.0</td></tr><tr><td>eval/p-ID_NUM</td><td>0.58537</td></tr><tr><td>eval/p-NAME_STUDENT</td><td>0.59904</td></tr><tr><td>eval/p-PHONE_NUM</td><td>0.0</td></tr><tr><td>eval/p-STREET_ADDRESS</td><td>0.0</td></tr><tr><td>eval/p-URL_PERSONAL</td><td>0.7931</td></tr><tr><td>eval/p-USERNAME</td><td>0.0</td></tr><tr><td>eval/precision</td><td>0.60414</td></tr><tr><td>eval/r-EMAIL</td><td>1.0</td></tr><tr><td>eval/r-ID_NUM</td><td>0.92308</td></tr><tr><td>eval/r-NAME_STUDENT</td><td>0.83445</td></tr><tr><td>eval/r-PHONE_NUM</td><td>0.0</td></tr><tr><td>eval/r-STREET_ADDRESS</td><td>0.0</td></tr><tr><td>eval/r-URL_PERSONAL</td><td>0.92</td></tr><tr><td>eval/r-USERNAME</td><td>0.0</td></tr><tr><td>eval/recall</td><td>0.84195</td></tr><tr><td>eval/runtime</td><td>50.8753</td></tr><tr><td>eval/samples_per_second</td><td>33.376</td></tr><tr><td>eval/steps_per_second</td><td>4.187</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>753</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0053</td></tr><tr><td>train/total_flos</td><td>8824037546575872.0</td></tr><tr><td>train/train_loss</td><td>0.12024</td></tr><tr><td>train/train_runtime</td><td>2094.7636</td></tr><tr><td>train/train_samples_per_second</td><td>5.75</td></tr><tr><td>train/train_steps_per_second</td><td>0.359</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fold-0</strong>: <a href=\"https://wandb.ai/emiz6413/PII/runs/qr7a1lbv\" target=\"_blank\">https://wandb.ai/emiz6413/PII/runs/qr7a1lbv</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240228_010648-qr7a1lbv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c13aa6639046a1940c2be010d09d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/503 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a42d98f6a6544628cbfc9f74616a486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/502 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29c243f2bae48b68a901792ab8f99a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/502 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f4ff3d88ba42eebf15ad3ac3eddd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/502 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abfac412e2d44e4a5a2e73881651dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/502 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfde299b52fb4aedb0de4c00b5c4a4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/502 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4414b40e2a0457a8bf2abe47304d344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/502 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29ee084e75d4b92bfc8cdc599c55c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/502 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a90179706e41baa97aa9b07decb9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/215 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc450e71af3043aeab9c8d3a288456df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/215 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b994d717b143bf91dd0aa5b3e0ec27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19cbd2ffcc1403fa594e4380db960bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec5bc08759341358e86f28f4e439c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb2a92677f144499a86c4c78b3a9142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80492f0a80284ae9a5834aad620d5feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a9c2a5092a4a5c923c8d1228feb783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4017\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 753\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a218ce3815f43748f28ca6016967528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666905063514908, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/notebooks/wandb/run-20240228_014329-2r5potgd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/emiz6413/PII/runs/2r5potgd\" target=\"_blank\">fold-1</a></strong> to <a href=\"https://wandb.ai/emiz6413/PII\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='753' max='753' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [753/753 35:06, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F5</th>\n",
       "      <th>P-id Num</th>\n",
       "      <th>R-id Num</th>\n",
       "      <th>F5-id Num</th>\n",
       "      <th>P-street Address</th>\n",
       "      <th>R-street Address</th>\n",
       "      <th>F5-street Address</th>\n",
       "      <th>P-username</th>\n",
       "      <th>R-username</th>\n",
       "      <th>F5-username</th>\n",
       "      <th>P-email</th>\n",
       "      <th>R-email</th>\n",
       "      <th>F5-email</th>\n",
       "      <th>P-phone Num</th>\n",
       "      <th>R-phone Num</th>\n",
       "      <th>F5-phone Num</th>\n",
       "      <th>P-name Student</th>\n",
       "      <th>R-name Student</th>\n",
       "      <th>F5-name Student</th>\n",
       "      <th>P-url Personal</th>\n",
       "      <th>R-url Personal</th>\n",
       "      <th>F5-url Personal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.135500</td>\n",
       "      <td>0.025666</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>0.031391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.008468</td>\n",
       "      <td>0.365019</td>\n",
       "      <td>0.263736</td>\n",
       "      <td>0.266581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.397436</td>\n",
       "      <td>0.239938</td>\n",
       "      <td>0.243652</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.421622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.029200</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>0.390456</td>\n",
       "      <td>0.494505</td>\n",
       "      <td>0.489489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.366548</td>\n",
       "      <td>0.478328</td>\n",
       "      <td>0.472783</td>\n",
       "      <td>0.557692</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.859749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.004404</td>\n",
       "      <td>0.566248</td>\n",
       "      <td>0.557692</td>\n",
       "      <td>0.558017</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.831711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>0.510836</td>\n",
       "      <td>0.514265</td>\n",
       "      <td>0.534483</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.912797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>0.557328</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.756861</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.452174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.540130</td>\n",
       "      <td>0.770898</td>\n",
       "      <td>0.758435</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.808756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.003697</td>\n",
       "      <td>0.559120</td>\n",
       "      <td>0.837912</td>\n",
       "      <td>0.822145</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.882603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.544057</td>\n",
       "      <td>0.821981</td>\n",
       "      <td>0.806143</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.594727</td>\n",
       "      <td>0.836538</td>\n",
       "      <td>0.823658</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.839888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.582155</td>\n",
       "      <td>0.828173</td>\n",
       "      <td>0.814928</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.865672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.002768</td>\n",
       "      <td>0.628450</td>\n",
       "      <td>0.813187</td>\n",
       "      <td>0.804096</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.552408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.623529</td>\n",
       "      <td>0.820433</td>\n",
       "      <td>0.810588</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.752315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>0.651397</td>\n",
       "      <td>0.800824</td>\n",
       "      <td>0.793820</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.841069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.642494</td>\n",
       "      <td>0.781734</td>\n",
       "      <td>0.775272</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.627033</td>\n",
       "      <td>0.847527</td>\n",
       "      <td>0.836218</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.617813</td>\n",
       "      <td>0.848297</td>\n",
       "      <td>0.836297</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.869666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.003796</td>\n",
       "      <td>0.537301</td>\n",
       "      <td>0.880495</td>\n",
       "      <td>0.859382</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.914205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.534286</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.848023</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>0.605338</td>\n",
       "      <td>0.872253</td>\n",
       "      <td>0.857707</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.595989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.591815</td>\n",
       "      <td>0.873065</td>\n",
       "      <td>0.857393</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.002919</td>\n",
       "      <td>0.614865</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.860990</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.897790</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.618202</td>\n",
       "      <td>0.862229</td>\n",
       "      <td>0.849334</td>\n",
       "      <td>0.589286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.002692</td>\n",
       "      <td>0.608987</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.860542</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.696756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.598936</td>\n",
       "      <td>0.871517</td>\n",
       "      <td>0.856524</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>0.622549</td>\n",
       "      <td>0.872253</td>\n",
       "      <td>0.859001</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.696756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.613786</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.854782</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1714\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_1/checkpoint-50\n",
      "Configuration saved in output/fold_1/checkpoint-50/config.json\n",
      "Model weights saved in output/fold_1/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_1/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_1/checkpoint-50/special_tokens_map.json\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1714\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_1/checkpoint-100\n",
      "Configuration saved in output/fold_1/checkpoint-100/config.json\n",
      "Model weights saved in output/fold_1/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_1/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_1/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_1/checkpoint-50] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1714\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_1/checkpoint-150\n",
      "Configuration saved in output/fold_1/checkpoint-150/config.json\n",
      "Model weights saved in output/fold_1/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_1/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_1/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_1/checkpoint-100] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1714\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_1/checkpoint-200\n",
      "Configuration saved in output/fold_1/checkpoint-200/config.json\n",
      "Model weights saved in output/fold_1/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_1/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_1/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_1/checkpoint-150] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1714\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_1/checkpoint-250\n",
      "Configuration saved in output/fold_1/checkpoint-250/config.json\n",
      "Model weights saved in output/fold_1/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_1/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_1/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_1/checkpoint-200] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1714\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_1/checkpoint-300\n",
      "Configuration saved in output/fold_1/checkpoint-300/config.json\n",
      "Model weights saved in output/fold_1/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_1/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_1/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_1/checkpoint-250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1714\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_1/checkpoint-350\n",
      "Configuration saved in output/fold_1/checkpoint-350/config.json\n",
      "Model weights saved in output/fold_1/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_1/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_1/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_1/checkpoint-300] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1714\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_1/checkpoint-400\n",
      "Configuration saved in output/fold_1/checkpoint-400/config.json\n",
      "Model weights saved in output/fold_1/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_1/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_1/checkpoint-400/special_tokens_map.json\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1714\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_1/checkpoint-450\n",
      "Configuration saved in output/fold_1/checkpoint-450/config.json\n",
      "Model weights saved in output/fold_1/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_1/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_1/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_1/checkpoint-400] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1714\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_1/checkpoint-500\n",
      "Configuration saved in output/fold_1/checkpoint-500/config.json\n",
      "Model weights saved in output/fold_1/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_1/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_1/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_1/checkpoint-350] due to args.save_total_limit\n",
      "Deleting older checkpoint [output/fold_1/checkpoint-450] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1714\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_1/checkpoint-550\n",
      "Configuration saved in output/fold_1/checkpoint-550/config.json\n",
      "Model weights saved in output/fold_1/checkpoint-550/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_1/checkpoint-550/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_1/checkpoint-550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_1/checkpoint-500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1714\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_1/checkpoint-600\n",
      "Configuration saved in output/fold_1/checkpoint-600/config.json\n",
      "Model weights saved in output/fold_1/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_1/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_1/checkpoint-600/special_tokens_map.json\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1714\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_1/checkpoint-650\n",
      "Configuration saved in output/fold_1/checkpoint-650/config.json\n",
      "Model weights saved in output/fold_1/checkpoint-650/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_1/checkpoint-650/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_1/checkpoint-650/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_1/checkpoint-550] due to args.save_total_limit\n",
      "Deleting older checkpoint [output/fold_1/checkpoint-600] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1714\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_1/checkpoint-700\n",
      "Configuration saved in output/fold_1/checkpoint-700/config.json\n",
      "Model weights saved in output/fold_1/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_1/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_1/checkpoint-700/special_tokens_map.json\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1714\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_1/checkpoint-750\n",
      "Configuration saved in output/fold_1/checkpoint-750/config.json\n",
      "Model weights saved in output/fold_1/checkpoint-750/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_1/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_1/checkpoint-750/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_1/checkpoint-700] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from output/fold_1/checkpoint-650 (score: 0.8609898107714703).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1714\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='215' max='215' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [215/215 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715514f306d742999d01a2074fd43877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f5</td><td>▁▃▅▅▇███▇███████</td></tr><tr><td>eval/f5-EMAIL</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/f5-ID_NUM</td><td>▁▁▁▇▄█▇▅▇▆█▆█▆▆█</td></tr><tr><td>eval/f5-NAME_STUDENT</td><td>▁▃▅▅▇███▇███████</td></tr><tr><td>eval/f5-PHONE_NUM</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/f5-STREET_ADDRESS</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/f5-URL_PERSONAL</td><td>▁▄▇█▇█▇▆█▇██████</td></tr><tr><td>eval/f5-USERNAME</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/p-EMAIL</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/p-ID_NUM</td><td>▁▁▁▆██▆▅▇▆▇▇▅▆▆▅</td></tr><tr><td>eval/p-NAME_STUDENT</td><td>▁▅▅█▇▇▇███▇▇████</td></tr><tr><td>eval/p-PHONE_NUM</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/p-STREET_ADDRESS</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/p-URL_PERSONAL</td><td>▁▃▇▆▇▆▇▇██▆█▇██▇</td></tr><tr><td>eval/p-USERNAME</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/precision</td><td>█▁▁▃▃▃▄▄▄▄▃▄▄▄▄▄</td></tr><tr><td>eval/r-EMAIL</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/r-ID_NUM</td><td>▁▁▁▇▄█▇▅▇▆█▅█▆▆█</td></tr><tr><td>eval/r-NAME_STUDENT</td><td>▁▃▅▅▇███▇███████</td></tr><tr><td>eval/r-PHONE_NUM</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/r-STREET_ADDRESS</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/r-URL_PERSONAL</td><td>▁▄▇█▇█▇▆█▇██████</td></tr><tr><td>eval/r-USERNAME</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/recall</td><td>▁▃▅▅▇██▇▇███████</td></tr><tr><td>eval/runtime</td><td>▅▅▆▁▅▇▃▆▅▇▂█▃▄▅▃</td></tr><tr><td>eval/samples_per_second</td><td>▄▄▃█▄▂▆▃▄▂▆▁▆▅▃▆</td></tr><tr><td>eval/steps_per_second</td><td>▄▄▃█▃▂▆▃▄▂▆▁▆▅▃▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▂▃▄▆███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f5</td><td>0.86099</td></tr><tr><td>eval/f5-EMAIL</td><td>1.0</td></tr><tr><td>eval/f5-ID_NUM</td><td>0.89779</td></tr><tr><td>eval/f5-NAME_STUDENT</td><td>0.84933</td></tr><tr><td>eval/f5-PHONE_NUM</td><td>1.0</td></tr><tr><td>eval/f5-STREET_ADDRESS</td><td>0.0</td></tr><tr><td>eval/f5-URL_PERSONAL</td><td>0.97389</td></tr><tr><td>eval/f5-USERNAME</td><td>0.0</td></tr><tr><td>eval/loss</td><td>0.00292</td></tr><tr><td>eval/p-EMAIL</td><td>1.0</td></tr><tr><td>eval/p-ID_NUM</td><td>0.5102</td></tr><tr><td>eval/p-NAME_STUDENT</td><td>0.6182</td></tr><tr><td>eval/p-PHONE_NUM</td><td>1.0</td></tr><tr><td>eval/p-STREET_ADDRESS</td><td>0.0</td></tr><tr><td>eval/p-URL_PERSONAL</td><td>0.58929</td></tr><tr><td>eval/p-USERNAME</td><td>0.0</td></tr><tr><td>eval/precision</td><td>0.61486</td></tr><tr><td>eval/r-EMAIL</td><td>1.0</td></tr><tr><td>eval/r-ID_NUM</td><td>0.92593</td></tr><tr><td>eval/r-NAME_STUDENT</td><td>0.86223</td></tr><tr><td>eval/r-PHONE_NUM</td><td>1.0</td></tr><tr><td>eval/r-STREET_ADDRESS</td><td>0.0</td></tr><tr><td>eval/r-URL_PERSONAL</td><td>1.0</td></tr><tr><td>eval/r-USERNAME</td><td>0.0</td></tr><tr><td>eval/recall</td><td>0.875</td></tr><tr><td>eval/runtime</td><td>51.872</td></tr><tr><td>eval/samples_per_second</td><td>33.043</td></tr><tr><td>eval/steps_per_second</td><td>4.145</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>753</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0043</td></tr><tr><td>train/total_flos</td><td>8832721068942336.0</td></tr><tr><td>train/train_loss</td><td>0.1159</td></tr><tr><td>train/train_runtime</td><td>2109.9738</td></tr><tr><td>train/train_samples_per_second</td><td>5.711</td></tr><tr><td>train/train_steps_per_second</td><td>0.357</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fold-1</strong>: <a href=\"https://wandb.ai/emiz6413/PII/runs/2r5potgd\" target=\"_blank\">https://wandb.ai/emiz6413/PII/runs/2r5potgd</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240228_014329-2r5potgd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfb18b3bf724dcea84a30240a0aa1ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/505 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43e325423fb43188519cc1262c0ac80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/505 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bd1eadc066432689ed854b8adf66f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/505 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddae50c9e024f73a20f39638067861e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/505 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d887884ace8f4885a68682eff178fe3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/505 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c121e899e6476a8a47c994543b793c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/505 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0030bd9aa1489ba99d3dab63d3ae0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/505 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc652db2232487f9a617c39d5ece68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/505 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f966919f6cf455ebfcc20a93768f0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/212 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eefbd9767e0340868d6f32ed5870d366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/211 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791298c32af3419da92829ec57a0d60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/211 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2baa1f34614a25b6a5a8ec76853c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/211 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f65237380e42748d0ae6fc8ce096e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/211 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068067fa6d0840cd844494560f397311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/211 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45224851a3048d985b2408cebc4ec42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/211 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c274b8a0994b66bb485fda453fe373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/211 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4040\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 756\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/notebooks/wandb/run-20240228_022025-3tefsyka</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/emiz6413/PII/runs/3tefsyka\" target=\"_blank\">fold-2</a></strong> to <a href=\"https://wandb.ai/emiz6413/PII\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='756' max='756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [756/756 34:56, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F5</th>\n",
       "      <th>P-id Num</th>\n",
       "      <th>R-id Num</th>\n",
       "      <th>F5-id Num</th>\n",
       "      <th>P-street Address</th>\n",
       "      <th>R-street Address</th>\n",
       "      <th>F5-street Address</th>\n",
       "      <th>P-username</th>\n",
       "      <th>R-username</th>\n",
       "      <th>F5-username</th>\n",
       "      <th>P-email</th>\n",
       "      <th>R-email</th>\n",
       "      <th>F5-email</th>\n",
       "      <th>P-phone Num</th>\n",
       "      <th>R-phone Num</th>\n",
       "      <th>F5-phone Num</th>\n",
       "      <th>P-name Student</th>\n",
       "      <th>R-name Student</th>\n",
       "      <th>F5-name Student</th>\n",
       "      <th>P-url Personal</th>\n",
       "      <th>R-url Personal</th>\n",
       "      <th>F5-url Personal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.140300</td>\n",
       "      <td>0.027620</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.012676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>0.395664</td>\n",
       "      <td>0.197832</td>\n",
       "      <td>0.201711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.318739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.454167</td>\n",
       "      <td>0.164902</td>\n",
       "      <td>0.169043</td>\n",
       "      <td>0.221053</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.686792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.005750</td>\n",
       "      <td>0.488304</td>\n",
       "      <td>0.452575</td>\n",
       "      <td>0.453852</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.734982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.461180</td>\n",
       "      <td>0.449319</td>\n",
       "      <td>0.449764</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>0.420408</td>\n",
       "      <td>0.697832</td>\n",
       "      <td>0.680559</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.147727</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.776801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.404782</td>\n",
       "      <td>0.717095</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.467497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.004143</td>\n",
       "      <td>0.552135</td>\n",
       "      <td>0.753388</td>\n",
       "      <td>0.742972</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.366197</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.823944</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.537155</td>\n",
       "      <td>0.765507</td>\n",
       "      <td>0.753192</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.605479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>0.648276</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.759006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.931129</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.823944</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.634395</td>\n",
       "      <td>0.753404</td>\n",
       "      <td>0.748007</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.874832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.652893</td>\n",
       "      <td>0.749322</td>\n",
       "      <td>0.745090</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.735250</td>\n",
       "      <td>0.731589</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.926121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.649103</td>\n",
       "      <td>0.784553</td>\n",
       "      <td>0.778306</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.252427</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.656331</td>\n",
       "      <td>0.768533</td>\n",
       "      <td>0.763512</td>\n",
       "      <td>0.380282</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.910506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.002970</td>\n",
       "      <td>0.593463</td>\n",
       "      <td>0.787263</td>\n",
       "      <td>0.777498</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.868190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.582579</td>\n",
       "      <td>0.779123</td>\n",
       "      <td>0.769142</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.929801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.002725</td>\n",
       "      <td>0.674970</td>\n",
       "      <td>0.771003</td>\n",
       "      <td>0.766807</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.753404</td>\n",
       "      <td>0.749653</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.934754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>0.638858</td>\n",
       "      <td>0.788618</td>\n",
       "      <td>0.781571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.438202</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.823944</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631068</td>\n",
       "      <td>0.786687</td>\n",
       "      <td>0.779296</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.934754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>0.685647</td>\n",
       "      <td>0.783198</td>\n",
       "      <td>0.778935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.931129</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.679625</td>\n",
       "      <td>0.767020</td>\n",
       "      <td>0.763245</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.933511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.677083</td>\n",
       "      <td>0.792683</td>\n",
       "      <td>0.787512</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.671466</td>\n",
       "      <td>0.776097</td>\n",
       "      <td>0.771473</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.931034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>0.609091</td>\n",
       "      <td>0.817073</td>\n",
       "      <td>0.806481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.252427</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.596413</td>\n",
       "      <td>0.804841</td>\n",
       "      <td>0.794167</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.934754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.667040</td>\n",
       "      <td>0.806233</td>\n",
       "      <td>0.799814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.651365</td>\n",
       "      <td>0.794251</td>\n",
       "      <td>0.787606</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.912281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1689\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_2/checkpoint-50\n",
      "Configuration saved in output/fold_2/checkpoint-50/config.json\n",
      "Model weights saved in output/fold_2/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_2/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_2/checkpoint-50/special_tokens_map.json\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1689\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_2/checkpoint-100\n",
      "Configuration saved in output/fold_2/checkpoint-100/config.json\n",
      "Model weights saved in output/fold_2/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_2/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_2/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_2/checkpoint-50] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1689\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_2/checkpoint-150\n",
      "Configuration saved in output/fold_2/checkpoint-150/config.json\n",
      "Model weights saved in output/fold_2/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_2/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_2/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_2/checkpoint-100] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1689\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_2/checkpoint-200\n",
      "Configuration saved in output/fold_2/checkpoint-200/config.json\n",
      "Model weights saved in output/fold_2/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_2/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_2/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_2/checkpoint-150] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1689\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_2/checkpoint-250\n",
      "Configuration saved in output/fold_2/checkpoint-250/config.json\n",
      "Model weights saved in output/fold_2/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_2/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_2/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_2/checkpoint-200] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1689\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_2/checkpoint-300\n",
      "Configuration saved in output/fold_2/checkpoint-300/config.json\n",
      "Model weights saved in output/fold_2/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_2/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_2/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_2/checkpoint-250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1689\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_2/checkpoint-350\n",
      "Configuration saved in output/fold_2/checkpoint-350/config.json\n",
      "Model weights saved in output/fold_2/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_2/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_2/checkpoint-350/special_tokens_map.json\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1689\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_2/checkpoint-400\n",
      "Configuration saved in output/fold_2/checkpoint-400/config.json\n",
      "Model weights saved in output/fold_2/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_2/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_2/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_2/checkpoint-300] due to args.save_total_limit\n",
      "Deleting older checkpoint [output/fold_2/checkpoint-350] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1689\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_2/checkpoint-450\n",
      "Configuration saved in output/fold_2/checkpoint-450/config.json\n",
      "Model weights saved in output/fold_2/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_2/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_2/checkpoint-450/special_tokens_map.json\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1689\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_2/checkpoint-500\n",
      "Configuration saved in output/fold_2/checkpoint-500/config.json\n",
      "Model weights saved in output/fold_2/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_2/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_2/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_2/checkpoint-450] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1689\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_2/checkpoint-550\n",
      "Configuration saved in output/fold_2/checkpoint-550/config.json\n",
      "Model weights saved in output/fold_2/checkpoint-550/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_2/checkpoint-550/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_2/checkpoint-550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_2/checkpoint-400] due to args.save_total_limit\n",
      "Deleting older checkpoint [output/fold_2/checkpoint-500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1689\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_2/checkpoint-600\n",
      "Configuration saved in output/fold_2/checkpoint-600/config.json\n",
      "Model weights saved in output/fold_2/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_2/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_2/checkpoint-600/special_tokens_map.json\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1689\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_2/checkpoint-650\n",
      "Configuration saved in output/fold_2/checkpoint-650/config.json\n",
      "Model weights saved in output/fold_2/checkpoint-650/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_2/checkpoint-650/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_2/checkpoint-650/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_2/checkpoint-550] due to args.save_total_limit\n",
      "Deleting older checkpoint [output/fold_2/checkpoint-600] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1689\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_2/checkpoint-700\n",
      "Configuration saved in output/fold_2/checkpoint-700/config.json\n",
      "Model weights saved in output/fold_2/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_2/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_2/checkpoint-700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_2/checkpoint-650] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1689\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_2/checkpoint-750\n",
      "Configuration saved in output/fold_2/checkpoint-750/config.json\n",
      "Model weights saved in output/fold_2/checkpoint-750/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_2/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_2/checkpoint-750/special_tokens_map.json\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from output/fold_2/checkpoint-700 (score: 0.8064814814814814).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1689\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='212' max='212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [212/212 00:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab09907aab049bf8bf5538a41340d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f5</td><td>▁▃▅▇▇█▇█████████</td></tr><tr><td>eval/f5-EMAIL</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/f5-ID_NUM</td><td>▁▁▆▂▄█▆█▆█▄█████</td></tr><tr><td>eval/f5-NAME_STUDENT</td><td>▁▂▅▇██▇█████████</td></tr><tr><td>eval/f5-PHONE_NUM</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/f5-STREET_ADDRESS</td><td>▁▃▇▇▇▇████▇█████</td></tr><tr><td>eval/f5-URL_PERSONAL</td><td>▁▆▂▅▆███████████</td></tr><tr><td>eval/f5-USERNAME</td><td>▁▁▁▁▅▅▅▄▅▅███▄▅▄</td></tr><tr><td>eval/loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/p-EMAIL</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/p-ID_NUM</td><td>▁▁██████████████</td></tr><tr><td>eval/p-NAME_STUDENT</td><td>▁▆▆▅▇███▇█▇██▇█▇</td></tr><tr><td>eval/p-PHONE_NUM</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/p-STREET_ADDRESS</td><td>▁▃█▇████████████</td></tr><tr><td>eval/p-URL_PERSONAL</td><td>▁▃▇▇▇▇▆▅▆▇▇▇▇▇█▇</td></tr><tr><td>eval/p-USERNAME</td><td>▁▁▁▁▅█▅▃█████▃█▃</td></tr><tr><td>eval/precision</td><td>█▁▂▁▃▅▅▅▄▅▄▅▅▄▅▄</td></tr><tr><td>eval/r-EMAIL</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/r-ID_NUM</td><td>▁▁▆▂▄▇▆█▆█▄▇████</td></tr><tr><td>eval/r-NAME_STUDENT</td><td>▁▂▅▇██▇█████████</td></tr><tr><td>eval/r-PHONE_NUM</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/r-STREET_ADDRESS</td><td>▁▃▇▇▇▇████▇█████</td></tr><tr><td>eval/r-URL_PERSONAL</td><td>▁▆▂▄▅▇██████████</td></tr><tr><td>eval/r-USERNAME</td><td>▁▁▁▁▅▅▅▅▅▅███▅▅▅</td></tr><tr><td>eval/recall</td><td>▁▃▅▇▇█▇█████████</td></tr><tr><td>eval/runtime</td><td>█▅▃▄▄▅▁▆▄▄▄▄▄▂▅▃</td></tr><tr><td>eval/samples_per_second</td><td>▁▄▆▅▅▄█▃▅▅▅▅▅▇▄▆</td></tr><tr><td>eval/steps_per_second</td><td>▁▄▆▅▅▄█▃▅▅▅▄▅▇▃▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▂▃▄▆███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f5</td><td>0.80648</td></tr><tr><td>eval/f5-EMAIL</td><td>0.99522</td></tr><tr><td>eval/f5-ID_NUM</td><td>1.0</td></tr><tr><td>eval/f5-NAME_STUDENT</td><td>0.79417</td></tr><tr><td>eval/f5-PHONE_NUM</td><td>1.0</td></tr><tr><td>eval/f5-STREET_ADDRESS</td><td>0.91228</td></tr><tr><td>eval/f5-URL_PERSONAL</td><td>0.93475</td></tr><tr><td>eval/f5-USERNAME</td><td>0.25243</td></tr><tr><td>eval/loss</td><td>0.00272</td></tr><tr><td>eval/p-EMAIL</td><td>0.88889</td></tr><tr><td>eval/p-ID_NUM</td><td>1.0</td></tr><tr><td>eval/p-NAME_STUDENT</td><td>0.59641</td></tr><tr><td>eval/p-PHONE_NUM</td><td>1.0</td></tr><tr><td>eval/p-STREET_ADDRESS</td><td>1.0</td></tr><tr><td>eval/p-URL_PERSONAL</td><td>0.52941</td></tr><tr><td>eval/p-USERNAME</td><td>0.33333</td></tr><tr><td>eval/precision</td><td>0.60909</td></tr><tr><td>eval/r-EMAIL</td><td>1.0</td></tr><tr><td>eval/r-ID_NUM</td><td>1.0</td></tr><tr><td>eval/r-NAME_STUDENT</td><td>0.80484</td></tr><tr><td>eval/r-PHONE_NUM</td><td>1.0</td></tr><tr><td>eval/r-STREET_ADDRESS</td><td>0.90909</td></tr><tr><td>eval/r-URL_PERSONAL</td><td>0.96429</td></tr><tr><td>eval/r-USERNAME</td><td>0.25</td></tr><tr><td>eval/recall</td><td>0.81707</td></tr><tr><td>eval/runtime</td><td>50.8015</td></tr><tr><td>eval/samples_per_second</td><td>33.247</td></tr><tr><td>eval/steps_per_second</td><td>4.173</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>756</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0033</td></tr><tr><td>train/total_flos</td><td>8908767673909248.0</td></tr><tr><td>train/train_loss</td><td>0.11571</td></tr><tr><td>train/train_runtime</td><td>2099.6473</td></tr><tr><td>train/train_samples_per_second</td><td>5.772</td></tr><tr><td>train/train_steps_per_second</td><td>0.36</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fold-2</strong>: <a href=\"https://wandb.ai/emiz6413/PII/runs/3tefsyka\" target=\"_blank\">https://wandb.ai/emiz6413/PII/runs/3tefsyka</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240228_022025-3tefsyka/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ae9f52368e4d4792bdc3d5deda45c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/504 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acf9aa1601e446692f1ed616811d678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/503 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1785241d6dfd4256bee88dff276990f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/503 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b02d985c7f45c390c7e06eb75423e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/503 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d221b7d3034f188b5aea5e3f2209ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/503 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840ba4397dde40a994f0a05fd8510198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/503 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5845ea249e4bcfa3031ecb833b08a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/503 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68593fc1877740efb2d70b3e1e411fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/503 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498dee4ee73a44f199799548d5ca7522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b83aab42014b2bbf58dfbcebc0ecc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074547198bb242159ddc8f25e1976137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/213 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed079e9f5ca4c6b8826eceb9f08482b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/213 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7379ede216804a50a9dbcc5f01903a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/213 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2daf5d270b4a069b46fdf5ad3b9d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/213 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acabd9e4981d478f87ad7beb21308848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/213 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822ffe12d34b4dea8f4ccf1e20625d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/213 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4025\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 756\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/notebooks/wandb/run-20240228_025716-143xrpj2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/emiz6413/PII/runs/143xrpj2\" target=\"_blank\">fold-3</a></strong> to <a href=\"https://wandb.ai/emiz6413/PII\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='648' max='756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [648/756 31:56 < 05:20, 0.34 it/s, Epoch 2.57/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F5</th>\n",
       "      <th>P-id Num</th>\n",
       "      <th>R-id Num</th>\n",
       "      <th>F5-id Num</th>\n",
       "      <th>P-street Address</th>\n",
       "      <th>R-street Address</th>\n",
       "      <th>F5-street Address</th>\n",
       "      <th>P-username</th>\n",
       "      <th>R-username</th>\n",
       "      <th>F5-username</th>\n",
       "      <th>P-email</th>\n",
       "      <th>R-email</th>\n",
       "      <th>F5-email</th>\n",
       "      <th>P-phone Num</th>\n",
       "      <th>R-phone Num</th>\n",
       "      <th>F5-phone Num</th>\n",
       "      <th>P-name Student</th>\n",
       "      <th>R-name Student</th>\n",
       "      <th>F5-name Student</th>\n",
       "      <th>P-url Personal</th>\n",
       "      <th>R-url Personal</th>\n",
       "      <th>F5-url Personal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.019536</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034146</td>\n",
       "      <td>0.035464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>0.268262</td>\n",
       "      <td>0.346341</td>\n",
       "      <td>0.342507</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.418006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.286624</td>\n",
       "      <td>0.323741</td>\n",
       "      <td>0.322137</td>\n",
       "      <td>0.060870</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.245946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.005489</td>\n",
       "      <td>0.361019</td>\n",
       "      <td>0.530081</td>\n",
       "      <td>0.520703</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.343902</td>\n",
       "      <td>0.507194</td>\n",
       "      <td>0.498098</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.517611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.004020</td>\n",
       "      <td>0.583673</td>\n",
       "      <td>0.697561</td>\n",
       "      <td>0.692365</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.172185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.572674</td>\n",
       "      <td>0.708633</td>\n",
       "      <td>0.702221</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.480740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.548712</td>\n",
       "      <td>0.796748</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571056</td>\n",
       "      <td>0.787770</td>\n",
       "      <td>0.776437</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.003986</td>\n",
       "      <td>0.504892</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.818198</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.496809</td>\n",
       "      <td>0.839928</td>\n",
       "      <td>0.818194</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.664662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.003050</td>\n",
       "      <td>0.653439</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.796231</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.910828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653959</td>\n",
       "      <td>0.802158</td>\n",
       "      <td>0.795227</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.637060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.003285</td>\n",
       "      <td>0.581315</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.806797</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.597113</td>\n",
       "      <td>0.818345</td>\n",
       "      <td>0.806848</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.656761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.003102</td>\n",
       "      <td>0.559742</td>\n",
       "      <td>0.845528</td>\n",
       "      <td>0.829244</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.910828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.556080</td>\n",
       "      <td>0.847122</td>\n",
       "      <td>0.830406</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.665663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.756672</td>\n",
       "      <td>0.783740</td>\n",
       "      <td>0.782663</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.913738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.753043</td>\n",
       "      <td>0.778777</td>\n",
       "      <td>0.777755</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.676876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.002650</td>\n",
       "      <td>0.686828</td>\n",
       "      <td>0.830894</td>\n",
       "      <td>0.824245</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.910828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.675912</td>\n",
       "      <td>0.832734</td>\n",
       "      <td>0.825369</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.641975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.548057</td>\n",
       "      <td>0.871545</td>\n",
       "      <td>0.852198</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.902208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.541713</td>\n",
       "      <td>0.875899</td>\n",
       "      <td>0.855598</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.671733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1706\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_3/checkpoint-50\n",
      "Configuration saved in output/fold_3/checkpoint-50/config.json\n",
      "Model weights saved in output/fold_3/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_3/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_3/checkpoint-50/special_tokens_map.json\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1706\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_3/checkpoint-100\n",
      "Configuration saved in output/fold_3/checkpoint-100/config.json\n",
      "Model weights saved in output/fold_3/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_3/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_3/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_3/checkpoint-50] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1706\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_3/checkpoint-150\n",
      "Configuration saved in output/fold_3/checkpoint-150/config.json\n",
      "Model weights saved in output/fold_3/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_3/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_3/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_3/checkpoint-100] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1706\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_3/checkpoint-200\n",
      "Configuration saved in output/fold_3/checkpoint-200/config.json\n",
      "Model weights saved in output/fold_3/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_3/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_3/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_3/checkpoint-150] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1706\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_3/checkpoint-250\n",
      "Configuration saved in output/fold_3/checkpoint-250/config.json\n",
      "Model weights saved in output/fold_3/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_3/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_3/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_3/checkpoint-200] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1706\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_3/checkpoint-300\n",
      "Configuration saved in output/fold_3/checkpoint-300/config.json\n",
      "Model weights saved in output/fold_3/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_3/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_3/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_3/checkpoint-250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1706\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_3/checkpoint-350\n",
      "Configuration saved in output/fold_3/checkpoint-350/config.json\n",
      "Model weights saved in output/fold_3/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_3/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_3/checkpoint-350/special_tokens_map.json\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1706\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_3/checkpoint-400\n",
      "Configuration saved in output/fold_3/checkpoint-400/config.json\n",
      "Model weights saved in output/fold_3/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_3/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_3/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_3/checkpoint-350] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1706\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_3/checkpoint-450\n",
      "Configuration saved in output/fold_3/checkpoint-450/config.json\n",
      "Model weights saved in output/fold_3/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_3/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_3/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_3/checkpoint-300] due to args.save_total_limit\n",
      "Deleting older checkpoint [output/fold_3/checkpoint-400] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1706\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_3/checkpoint-500\n",
      "Configuration saved in output/fold_3/checkpoint-500/config.json\n",
      "Model weights saved in output/fold_3/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_3/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_3/checkpoint-500/special_tokens_map.json\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1706\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_3/checkpoint-550\n",
      "Configuration saved in output/fold_3/checkpoint-550/config.json\n",
      "Model weights saved in output/fold_3/checkpoint-550/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_3/checkpoint-550/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_3/checkpoint-550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_3/checkpoint-500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BigBirdForTokenClassification.forward` and have been ignored: full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace. If full_text, token_map, length, document, tokens, provided_labels, offset_mapping, trailing_whitespace are not expected by `BigBirdForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1706\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/fold_3/checkpoint-600\n",
      "Configuration saved in output/fold_3/checkpoint-600/config.json\n",
      "Model weights saved in output/fold_3/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in output/fold_3/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in output/fold_3/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/fold_3/checkpoint-450] due to args.save_total_limit\n",
      "Deleting older checkpoint [output/fold_3/checkpoint-550] due to args.save_total_limit\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/models/big_bird/modeling_big_bird.py:981: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_idx, eval_idx) in enumerate(folds):\n",
    "    args.run_name = f\"fold-{fold_idx}\"\n",
    "    args.output_dir = os.path.join(OUTPUT_DIR, f\"fold_{fold_idx}\")\n",
    "    if Path(args.output_dir).joinpath(\"eval_result.json\").exists():\n",
    "        continue\n",
    "    original_ds = ds[\"original\"].select([i for i in train_idx if i not in exclude_indices])\n",
    "    train_ds = concatenate_datasets([original_ds, ds[\"extra\"]])\n",
    "    train_ds = train_ds.map(train_encoder, num_proc=os.cpu_count())\n",
    "    eval_ds = ds[\"original\"].select(eval_idx)\n",
    "    eval_ds = eval_ds.map(eval_encoder, num_proc=os.cpu_count())\n",
    "    trainer = Trainer(\n",
    "        args=args,\n",
    "        model_init=model_init,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=MetricsComputerV2(eval_ds=eval_ds, label2id=label2id),\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=512),\n",
    "    )\n",
    "    trainer.train()\n",
    "    eval_res = trainer.evaluate(eval_dataset=eval_ds)\n",
    "    with open(os.path.join(args.output_dir, \"eval_result.json\"), \"w\") as f:\n",
    "        json.dump(eval_res, f)\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(name=\"cv\")\n",
    "results = dict()\n",
    "for res_json_path in Path(OUTPUT_DIR).glob(\"fold*/eval_result.json\"):\n",
    "    fold = res_json_path.parent.name.split(\"_\")[-1]\n",
    "    with open(res_json_path, \"r\") as f:\n",
    "        res = json.load(f)\n",
    "        results[fold] = {k.replace(\"eval_\", \"\"): v for k, v in res.items()}\n",
    "results[\"cv\"] = {key: np.mean([r[key] for r in results.values()]) for key in results[\"0\"].keys()}\n",
    "table = wandb.Table(columns=[\"fold\"] + list(results[\"0\"].keys()))\n",
    "for f, res in results.items():\n",
    "    table.add_data(f, *[res[c] for c in table.columns if c != \"fold\"])\n",
    "wandb.log({\"eval_result\": table})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    },
    {
     "datasetId": 4379849,
     "sourceId": 7518925,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4459964,
     "sourceId": 7659420,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4466980,
     "sourceId": 7661007,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 163088908,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
